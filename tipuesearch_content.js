var tipuesearch = {"pages":[{"title":"À propos","text":"I'm Freeeeee ! C'est l'histoire d'un mec qu'est tombé dans la potion \"logiciel libre\" depuis la version GNU/Debian Slink , c'était en 1998. Il n'est pas passé sur le pont de l'Alma ni n'a perdu ses lunettes dans la Loire , juste passé par du Perl, PHP et enfin Python. Du Debian de l'époque, il retient une \"Fête de l'Internet\" à la mairie de Boulogne Billancourt avec la team DaLinuxFrenchPage d'alors, où on avait fini l'install party en parties de Quake3 :) Du Perl il retient IRSSI :P avec ce client je codais alors un gestionnaire de matches de jeux en reseau (comme ClanCalendar:P) Du PHP il retient : La création de PhpFr.Org site d'entraide au dev en PHP et permettant à tout à chacun de se mettre au developpement avec ce langage, site gracieusement hébergé 5ans par GCU et l'ami iMil . PunBB en PHP4 où j'avais croisé alors Vincent Garnier aka \"Vin100\" , auteur alors de Puntal, que j'avais rejoins sur le projet, puis nicosomb . CakePHP en PHP4. Avec lequel je retentais de faire Puntal avec le framework. Puis enfin l'excellentissime framework Jelix en PHP5 (dont le mec fut core-developper) et tous les membres qui composaient la team. Avec ce dernier je produisis le forum du site du projet, puis le repository des applications faites avec le framework nommé Booster Du Python il retient tout ce qu'on peut imaginer gravitant autour du web et du devops. Genre au pif, Django , l'exceptionnel DjangoRestFramework , et Ansible / Fabric Mon dernier joujou en date est un projet libre, un clone du service IFTTT (If This Then That) , nommé Trigger-Happy ouvert à tout à chacun et dispo sur github . A part ça je mets mon grin de sel à droite à gauche, je fais de la relecture chez Sam & Max ou parcours indexerror.net . Chess Parfois je pousse du buis tantôt blanc tantôt noir. Mon joueur préféré est un paranoïaque Aaron Nimzowitsch et son livre Mein System traduit très bien sa peur permanente, et du coup la prophylaxie qui fut son domaine aux échecs. Get in touch irc : sur Freenode @foxmask Mastodon @foxxmask Twitter foxmask Gitter.im keybase.io/foxmask","tags":"A Propos","url":"https://foxmask.net/pages/a-propos","loc":"https://foxmask.net/pages/a-propos"},{"title":"Trigger Happy archivé","text":"Trigger Happy Archivé - Yeoboseyo ? Voici le post de l'année ;) Bye Bye TriggerHappy Je viens de mettre en mode archivé le projet au 1300 etoiles et des poussieres https://github.com/push-things/django-th . En effet, je l'ai maintenu autant que j'ai pu, mais avec le temps : de moins en moins de gens s'y sont intéréssés j'ai élagué enormement de mes usages pour reduire mon emprunte avec tous ces services donc migré à django 3 serait trop long pour tout reprendre retester. j'ai conservé le projet pour tous les petits trucs qui jalonnent le projet, que ca soit pour des webhook, les form wizard, le load auto de classes génériques, etc. Quand je me repencherai sur le code plus tard je me dirai \"au la vache j'ai ecrit des trucs bien dégueux à l'époque\" (ou pas:) Du coup bye bye le projet Hello Yeobeseyo En lieu et place je me suis mis à starlette.io , avec lequel j'ai refait un simili TriggerHappy avec le peu de services dont j'ai besoin, le projet Yeoboseyo (\"Allo?' en Coréen). Permet de parcourir les flux RSS de son choix et créer des articles soit dans : son editeur markdown favori => Joplin Reddit Mastodon La batterie du portable est à 24%, je vais ecourter là ;) Bon courage pour la suite du confinement !","tags":"Techno","url":"https://foxmask.net/post/2020/05/01/trigger-happy-archive-yeoboseyo/","loc":"https://foxmask.net/post/2020/05/01/trigger-happy-archive-yeoboseyo/"},{"title":"SFR 8 mois de calvaire ...","text":"Je m'en va vous conter une chronologie quasi au jour le jour (surtout vers la fin) de tout ce cauchemar... Il était une fois un an et un mois chez SFR ca nous changera des déboires des transports en commun d'ile de france, meme si c'est aussi chiant à vivre ... allez jettons nous dans la story moisie. Et dire que je suis un cas isolé, ... j'ai des doutes :) 2016 En novembre 2016, arrive la saison de la valse des opérateurs internet/téléphone pour tirer les prix vers le bas et faire des econocrocs. Dans le coin, par chez nous, la fibre c'est Orange ou SFR. Comme j'étais chez Djosh et qu'il ne voulait pas s'aligner (avec une pauvre réduc de 5EUR sur 6 mois lolilol... ), je leur ai dit \"bon ben je pars, et à l'année prochaine !\" 28 Novembre 2016, souscription SFR Mobile 20 Décembre 2016, souscription SFR Fibre Pour une connerie de \"j'ai oublie mes papiers d'identiti\" j'ai pas pu souscrire les 2 offres en même temps, ce qui aura un impact ... non négligeable par la suite ... Jusqu'à l'été 2017, TOUT baigne ! 2017 Ete et début des emmerdes à la pèle ... Juillet 2017 : Blackout internet suite à la tempête \"estival\" qu'on a subit. Tout le quartier est coupé du net, SFR comme Orange. Août 2017 : Retour du service Je me réengage pour un an après avoir bien gueulé sur leur manque de comm... La dame etait trop forte et m'a convaincu ... Finalement girouette, et comme on subit des problèmes de réseau wifi à la maison, pression de la smala, j'appelle pour me rétracter dès le lendemain, pour obtenir un retour à la date d'engagement initiale du 28/11/2017 pour le mobile, mais pour le fixe, c'est plus compliqué, ca traine (ce n'est pas le même service qui gère, faut pas mélanger ...) parce que je me suis rétracté trop vite et que ce sont croisées la procédure de réengagement et celle de rétraction :P (ouais déjà que chez eux c'est le dawa, j'en rajoute une couche) Toujours en Aout, on recoit un mail de SFR qui n'a rien à voir avec la choucroute mais qui va en rajouter une triple couche ... pour nous informer du changement tarifaire. Du coup, bibi emet le souhait de résiliation, puisque je ne suis pas d'accord pour payer une chaine (altice:P) dont j'ai rien à foutre... Mais menaces au téléphone de mon interlocutrice de devoir payer des pénalités de résiliation... Comme ca m'a bien remonté j'ai laissé courir en mettant sur papier ce qui m'arrivait pour bien les coincer le moment venu Ensuite, pendant septembre 2017, SFR ne m'a jamais appelé pour me dire où ça en était ma rétractation sur la prologation de la fibre. Octbore 2017 : SFR fait le mort, et moi aussi, mais c'etait pour mieux nous retrouver :P Novembre 2017 : Ma rétraction de fin août enfin prise en compte le 23 11 2017, je suis tout joie ... (ca sera de courte durée ...) L'engagement \"fibre\" prend donc fin en décembre 2017 comme prévu initialement. YOU ... PI 28/11 : Changement d'opérateur mobile en passant par Sosh (c'est là que commence la merde) Décembre 2017 : Je m'aperçois 4 jours avant la date de fin prévu, de mon engagment initial à la fibre, le 20/12/2017, que SFR m'a réengagé pour le fixe pour un an... À mon insu, du coup j'appelle pour leur demander ce qu'ils leur à pris ... A partir du 16/12, je les relance tous les 2 jours pour avoir des nouvelles. Lors de mes relances, j'apprends que j'aurai appelé et demandé à me réengager. Ce qui est faux puisque je suis paché chez choche pour changer d'opérateur... \"ouais mais pour résilier il faut nous appeler\" qu'on me dit. Et puis quoi encore ? Appel le 17/12 au soir, on m'annonce l'annulation de la prolongation faite par SFR. (là, SFR admet son erreur) Rappel le 18/12 on me répond \"pour l'annulation, c'est fait, vous avez dû recevoir un mail\" mais jamais reçu ce mail même pas dans les spam (parce que j'ai pas de filtre anti spam :P) Rappel le 26/12 on me répond \"c'est fait, on rappelle sous 2 à 5 j\", SFR en mode péroquet Rappel le 28/12 on me répond \"vous n'êtes plus engagé depuis le 22/12 et la rétractation est en cours\" donc on a convenu que je pouvais résilier. Je suis enfin tombé sur une femme qui a entendu et compris ce que je disais... Aleylouya ! Pendant tout ce temps, à aucun moment j'ai à faire à des français (on reconnait un \"accent\") ce qui fait que eux traitent ma demande du mieux qu'ils peuvent, limite en se débarrassant fissa de mon problème... sauf cette femme. A qui je demande, pendant que je la tiens, la procédure de résiliation... Avec cette good news, le jour même, sans plus attendre, je passe chez Orange. Avec cette résiliation de fin novembre, les options et services qui étaient \"inclus dans le forfait\" me sont facturées au prix fort ... 62EUR pour décembre ... Le 29/12 : réception de la LiveBox. Envoi en recommandé avec AR à SFR de ma lettre de résiliation. Ah oui, au fait, chez SFR, le client communique avec du papier. Les mails, le site web, ca sert pas à résilier... t'oublies. Le 29/12 : Débranchement de la SFR box, branchement de la LiveBox Le 30/12 : Suspension de l'autorisation de prélèvement SFR. Pendant tout ce temps, avec tous ces problèmes, jamais SFR ne me rappelle pour résoudre mes problèmes, n'envoie de SMS ni de mail pour me tenir informé de quoi que ce soit. 2018 Janvier 2018 : 08/01 : J'ai reçu 3 mails ... accrochez vous parce que 3 MAILS KWA ; d'UN COUP alors que SFR m'en avait JAMAIS envoyé jusqu'alors :) 2 mails pour m'expliquer ce qu'il faut que j'fé pour retourner le matériel, 1 mail pour me dire \"vous pouvez conserver le matériel sans frais ....\" 10/01 : Je contacte SFR par téléphone. \"Qu'est-ce je faiiiiiiiis??????? avec vos mails contradictoires ?\" Finalement je renvoie les appareils SFR par la poste. avec un tout beau gros colis tout ficelé avec tout le matos :P Le tout à MES frais, l'étiquette que SFR m'envoie par mail pour que je la colle sur le colis, sert juste pour l'adresse, pas à affranchir le paquet dont elle connait le poids de ses appareils (faut pas déconner :P) 16/01 : mail de SFR (SI SI un autre MAIL !!!) pour remboursement dépôt de garantie de 49EUR.... 19/01 : suivi Colissimo, colis pas encore arrivé chez SFR, manquerait plus que LaPoste mette son grin de sel moisi quand je vois le tour de la terre que fait le colis ... 19/01 : Attente de la facture de clôture, de compte de la part de SFR. 21/01 : en consultant mes comptes je vois que SFR a remboursé 15EUR par virement (un trop perçu sur les 62EUR :P), reste le dépôt de garantie qui arriva sous 2j Finalement le colis est livre depuis le 15/01/2018 à DINAN (le lieu où doit retourner le matos), mais comme d'habitude, pas de mail pour accuser réception, ni SMS ni appel. 23/01 mail (ENCORE!) de SFR, mais celui là vaut le détour : \"Votre facture box de SFR n° xxx du 20/01/2018, d'un montant de 222,35 EUR\" ... Ben tiens donc, comme si je ne m'y étais pas attendu à celle là... Appel à 9h le jour même, pour la contester cette facture, on me répond \"on vous rappelle sous 3 ou 4 jours ouvrés\" Finalement un SMS de SFR (oui oui un SMS maintenant) le jour même pour confirmer leur erreur qui sera corrigé le 20/02. Je fais un SMS à ma femme pour lui annoncer mais je lui dit \"de toute façon c'est pas fini tant qu'on n'a pas cette facture de cloture de compte ...\" Et me demande de faire opposition au prélèvement. J'ai déjà fait sauter le prélèvement SEPA le mois dernier quand j'ai résilié.... Pendant tous ces échanges téléphoniques, j'ai dû répéter à chaque fois mon histoire \"je me suis abo en 2016 .... \" alors qu'ils ont eu traces de tous les échanges dans leur outil de suivi... Comme on peut s'attendre à tout de sfr, je n'ai pas pavoisé avant d'avoir cette fameuse facture de clôture de compte. Pendant tout cette période, on apprendra que le PDG de SFR perd \"juste\" 5 milliards https://www.universfreebox.com/article/41432/SFR-fait-perdre-5-milliards-a-Altice-en-bourse avec le cours de l'action qui plonge en flèche. De même qu'on lit la désastreuse gestion de ce dernier par ici https://www.capital.fr/entreprises-marches/sfr-comment-le-management-de-patrick-drahi-a-conduit-au-fiasco-1268741 Mais bon on n'est que poussiere :P De la poussiere d'or quand meme vu la tronche d'SFR ces temps ci :D Fevrier 2018 : Comme je m'y attendais en envoyant le SMS à ma chere et tendre ... 16/02 reception d'une mise en demeure de payer les 222EUR liés au pénalité d'engagement 17/02 appel de SFR qui me conseille de faire un courrier au Contentieux + Service Client avec copie du SMS (je vous épargne les aller-retour parmi les incompétents que j'ai eu au bout du fil qui m'invitait à appeler le service client SFR alors que c'etait leur numéro que j'avais fait ... et j'en passe et des meilleures) 19/02 donc envoye lettre avec AR aux services SFR Contentieux + Service Client 20/02 réception par mail des AR 21/02 réponse du service client par mail qui confirme que je n'ai pas à régler les 238EUR et font cesser les relances jusqu'à ce que la régularisation soit faite Mars 2018 : 06/03 Cette fois ci SFR me contacte pour me confirmer qu'ils me doivent encore 15 boules et que les 238 c'est fini 07/03 facture de regul' faite en ma faveur... FIN Conclusion: Il faut être tenace pour ne pas laisser filer son argent. je vous épargnerai le \"SFR plus jamais d'ma vie\", par contre un \"truc\" intéressant pour nous autres clients, quand on prend des abonnements mobiles chez X et une ligne fixe chez Y, c'est pour une somme Z pendant un an qui augmentera généralement dans 12mois. Mais c'est SANS ENGAGEMENT alors qu'avec une offre groupée Fixe+Mobile c'est engagement 12mois ... Donc c'est beaucoup plus souple ainsi. Ah vous le saviez déjà ? :) Et comme je l'avais annoncé à Djoche je suis reviendu chez eux.","tags":"Techno","url":"https://foxmask.net/post/2018/03/20/sfr-8-mois-calvaire/","loc":"https://foxmask.net/post/2018/03/20/sfr-8-mois-calvaire/"},{"title":"Migrer de Evernote vers Joplin","text":"Petit tuto pour migrer sans encombres ses affaires depuis Evernote vers Joplin. Exporter les notes Evernote Pour importer ses notes Evernote, d'abord exportez les depuis le client Evernote sous windows (en les selectionnant toutes d'un coup) dans une fichier sous le format .enex. Importer Puis, une fois fait, depuis le client Joplin, ouvrez \"Fichier > Import > Evernote Export File\" Mon bon gros paquet de 2500 notes (pour une poids de 650Mo) a été intégré sur mon vioc PC un Quad Core avé 6Go de RAM, de 8ans d'age en 25mnn alors que sur la babasse de compet @ work : 10mn... J'ai dû m'y reprendre à deux fois. Raison ? Hé bien, l'import ne gère pas la création des dossiers existants, et tout arrive dans une dossier Evernote. Du coup, pour préparer le terrain en amont, depuis Evernote, j'ai affecté à mes notes, le tag du nom de son dossier où elles se trouvaient. Preparer le terrain avant la synchronisation Une fois l'import effectué, on prendra le temps de retirer les tags créés \"expres\" pour la migration, mais pas tout de suite. Pourquoi ? Comme dit juste avant, l'import ne créé pas les dossiers. Donc : on va créer les dossiers manquants dans joplin à la mimine on va cliquer sur chaque tag et faire glisser toutes les notes dans le dossier de destination du même nom que son tag Du coup on se rend compte que si on supprime les tags avant, l'opération de dispatching va être empirque, voire impossible. Synchronisation Une fois fait tout ce mic mac, on va synchroniser les notes sur OneDrive, pour qu'ensuite ses notes soient dispo sur mobile. Pour mon cas 2500 notes produisent 8500 \" objets \" qu'il faut synchroniser. Donc j'ai laissé tel quel afin que la synchornisation se fasse sans encombre. Finalisation Au bout de quelques soirées, la synchro finie, on peut supprimer les tags ajoutés précédemment dans la foulée, et enfin profiter :) La suite ? On va attendre que Joplin s'enrichisse de fonctionnalités, par exemple : sur le filtrage de notes: recherche une chaine de caractères mais dans un dossier donné recherche une chaine de caractères mais dans un dossier donné sur une période donnée sur le suppport de la prise en charge de plus de documents puisque les pdf ou odt n'y ont pas droit de citer pour le moment etc... Pour le moment je cherche un moyen de créer des notes en me passant du moindre \"client\" desktop/mobile.","tags":"Techno","url":"https://foxmask.net/post/2018/03/17/migrer-evernote-joplin/","loc":"https://foxmask.net/post/2018/03/17/migrer-evernote-joplin/"},{"title":"Tableau comparatif d'outils de prise de notes partie 2 : Boostnote vs Joplin","text":"Dans la continuité du comparatif d'outils de prise de notes , voici 2 editeurs libres utilisant absolument les mêmes techno et archi. En effet tous deux : utilisent React Native fournissent un client \"lourd\" pour le desktop, un client mobile, mais 0 app web utilisent le format markdown (enfin le markdown de github pour Boostnote pour etre précis) exploitent le stockage \"sur le cloud\" pour synchroniser ses notes. Malgré cela ils se distinguent l'un de l'autre facilement. Boostnote Joplin Stockage Dropbox One drive (dropbox en cours de portage) Import Evernote ever2boost se nomme l'outil, mais plante Oui mais les pdf et docx ne sont pas gérés donc non synchronisés Cloud Pas pu tester le comportement de Dropbox puisque les imports Evernote plantent limitation des tailles de fichiers à 4Mo sur Onenote, rend les notes vide de sens, donc ne sont pas synchornisées Recherche Trouve immediatement les notes et surligne le terme rechercher Trouve les notes comportant le terme sans les surligner Boostnote hélas, sans matière, sans mes notes migrées de evernote, le tour du proprietaire a été rapide. Par exemple, il a un manque d'uniformisation entre la version desktop et mobile. Sur mobile on a l'impression qu'on a perdu ses notes qui sont pourtant synchronisées sur dropbox, la faute au menu qui liste les \"stockages\" au lieu d'afficher directement leur contenu. Par ailleurs, au lieu d'afficher le rendu d'une note dans la liste, pour l'heure, on nous affiche le code markdown.... sur une seule ligne... Joplin Autant l'extraction des notes sur mon desktop a été hyper rapide (~25min pour 650Mo de 2500notes), autant la synchronisation de Joplin vers Onedrive est TRES longue Mes notes ont été decortiquées par Joplin qui a comptabilisé 8500 objets à synchroniser (ce qui prend en compte, les dossiers, tags, tags dans les notes, images dans les notes etc) J'ai eu 8500 objets synchronisés en 4 soirées (de 19h à 1h du matin + une, ce soir, de 20h à 23h40 ;). Manques : Pour les 2 projets : Organiser les tag et dossiers en 'arbre' car la liste des dossiers et tags devient très longue et pollue visuellement Pas de widget sur mobile ni pour l'un ni pour l'autre. Limitation dû à un bug lié React Native il semblerait, donc bug qui impacterait aussi Boostnote. Sur Joplin les tags ne peuvent être créés depuis le mobile. Pour Boostnote : Pas de reminder Ce que sait faire l'un mais pas l'autre Boostnote on peut blogger depuis boostnote sur Wordpress :P Joplin les notes peuvent être chiffrées intégralement transformer une note en reminder et inversement Conlusion Ca serait sympa d'avoir un mix des deux, le desktop the Boostnote et la version mobile de Joplin Boostnote est attrayant visuellement mais Joplin gagne pour moi car j'ai pu recuperer toutes mes notes Evernote, que ca synchronise (même si l'intégralité aura pris beaucoup de temps la première fois) Il faudrait que je tarabiscote l'auteur de Joplin pour savoir comment je pourrai créer des notes en me passant du mobile et du destkop pour l'integrer à TriggerHappy :) Le contenu des fichiers markdown est special du coup si je ne mets pas le bon hash dedans, c'est comme si j'avais pissé dans un violon. PS : Un dernier détail qui ne gate rien, Joplin c'est français :P","tags":"Techno","url":"https://foxmask.net/post/2018/03/15/comparatif-editeurs-partie2-boostnote-vs-joplin/","loc":"https://foxmask.net/post/2018/03/15/comparatif-editeurs-partie2-boostnote-vs-joplin/"},{"title":"Tableau comparatif d'outils de prise de notes","text":"Voici un petit recapitulatif sans prétention, des outils de prise de notes que j'ai pu croiser, tester et tenter d'intégrer à Trigger-Happy . Il y a 2 ans je m'éutais déjà penché sur le sujet d'Evernote , et ce sujet continue encore et encore de me titiller. Tableau Comparatif des Editeurs Nom Evernote Simplenote Onenote Multi plateforme X X X Widget de liste des notes (android) X X X Recherche dans carnets offline X Recherche dans pdf + images X Gérer des rappels X X X E-mail pour recevoir les notes X API disponible X Jamais X Prix 29.90€/an (1Go de nouvelles notes) 0 tributaire de OneDrive Tableau Comparatif des Editeurs (2nde partie) Nom Google Keep Nimbus Dropbox Paper Multi plateforme X X X Widget de liste des notes (android) X widget H.S. aucun fonctionne Recherche dans carnets offline Recherche dans pdf + images Gérer des rappels X X X E-mail pour recevoir les notes X API disponible Jamais En cours X Prix tributaire de GoogleDrive 45$/an - 0 pour 100mb/mois tributaire de DropBox UI : Evernote: L'appli web \"ancienne version\" est devenue catastrophique pour sélectionner plusieurs notes. Quand on scrolle ses notes, elles disparaissent littérallement. La \"nouvelle version\" ne permet plus de sélectionner plusieurs notes d'un coup ... Sur windows, l'ouverture et la synchro avec le client natif, freezent l'appli Simplenote: Trop trop trop minimaliste. Pas de dossiers pour classer. Poiur ajouter des tags, sur mobile on met 3 heures à chercher où c'est caché... Il faut scroller 1km pour arriver en bas de sa page blanche, un champ tag. Le \"Term of service\" très olé olé, façon \"google reader\" ... genre \"je fais ce que je veux sans vous prévenir\" OneNote: Au bout de 10 minutes d'utilisation, conflit de synchro alors que je suis tout seul sur le compte depuis le mobile Google Keep: le widget est hideux. ne donne pas envie d'utiliser l'appli. Nimbus: Copie conforme de Evernote, on se retrouve vite avec ses habitudes. J'ai aimé les recherches sauvegardées dynamiquement pour refaire la même recherche ultérieurement sans avoir à la retaper Sur windows, l'ouverture et la synchro avec le client natif, freezent l'appli... comme Evernote :P Dropbox Paper: Sur l'appli web, on peut vraiment tout faire. Le rendu sur mobile est bien si on exclue les tableaux avec un scrolling horizontal pas génial mais quand on se créé un doc on sait pour quel usage on le fait et donc on évitera d'éditer un tableau trop long sur le mobile sauf si on est joueur :) | Un truc me gène quand même, compte tenu du fait que Dropbox paper s'appuie sur Dropbox, les reminder eux s'appuient sur le calendrier de Google Calendar... et ca j'aime pas du tout. Autant qu'un tel outil ne se \"décharge\" pas de la fonctionnalité en la déléguant. On peut comprendre le pourquoi du comment quand on sait/se rend compte que Paper est un outil collaboratif, et que Google Calendar offre la souplesse de la gestion des invitations pour des rdv, mais pour le quidam tout seul dans son coin, gérant ses rappels pépère, ca le fait pas je trouve. My XP :) Dropbox : limitation de l'API Comme Evernote devient de plus en plus compliqué à utiliser, je me suis tourné, en premier lieu, vers Dropbox Paper mais avec l'API qui limite la création de notes à 100 par jour, la migration de mes notes Evernote vers Dropbox Paper prendrait 25j ... Du coup je me suis fendu d'un ticket sur le github du SDK pour leur demander une possibilité de passer outre le Ratelimit juste pour l'opération d'import.... Attendons ... L'an dernier j'avais demandé la possibilité de créer des \"paper\" et ca avait fini par arriver en novembre dernier :) Nimbus : limitation de la bande passante consommée Ensuite j'ai tenté Nimbus, qui importe vos notes les doigts dans le nez. Mais comme j'ai 2500notes, l'opération a consommé les 100mb du mois ... Je songe refaire l'operation avec un autre compte en préparant mieux le terrain car : L'import des notes, ne recréé pas les dossiers dans lesquelles se trouvaient mes notes. Donc pour preparer les choses, je pense mettre en guise de nom de dossier dans Evernote, un tag qui correspondrait au nom du dossier. Chaque modification, resynchronise avec Nimbus ce qui ampute les 100mb mensuels Donc je conserverai pendant un temps, l'organisation des notes dans un tag (en lieu et place du dossier) et les déplacerait au compte-gouttes dans un dossier au fur et à mesure qu'il resterait du rab de bande passante en fin de mois. API : Il n'y en n'a pas pour le moment mais elle serait en cours de dev ... Autres J'ai aussi testé ya un bail Paperwork, mais je n'ai jamais daigné installer la stack LAMP + Node sur un serveur. Et pendant ce temps ... j'attends toujours la dernière version de Turtl ... Et j'ai démarré mon propre éditeur de prise de notes (en django & django rest framework & vuejs) mais c'est juste une version web, loin de couvrir le spectre des fonctionnalités des Nimbus et Evernote... Changement de mes habitudes Avant, avec Evernote, et TriggerHappy, je mettais absoluement TOUT dedans. Des idées de projets, des recettes de cuisines, des démarches administratives, mes CV, des rdv de doc, des tweets de conférences, des billets chopper chez \"sam et max\", tout y passait. Avec les problemes d'evernote, j'ai dû revoir et redecouper ma façon de m'organiser. A présent je colle dans pocket \"ma vie de technophile\", et dans l'outil que j'aurai adopté, \"ma vie personnelle\". Ca séparera les contenus et evitera la dispersion de ma concentration comme quand j'ouvrais Evernote pour lire un rappel et tombait sur un article de sam&max. Wikipedia encore et toujours update du 12/03/2018 un comparatif avec beaucoup beaucoup plus d'info et de softs Autres comparatifs update du 16/03/2018 une partie deux de nouveaux outils de prise de notes est disponible également ici","tags":"Techno","url":"https://foxmask.net/post/2018/03/07/comparatif-editeurs/","loc":"https://foxmask.net/post/2018/03/07/comparatif-editeurs/"},{"title":"Aparté avec Horizon Zero Dawn","text":"Intro Adepte du FPS depuis \"Counter-Strike 1.3\", et malgré une tentative de MMORPG, rien y faisait, c'était toujours le FPS qui avait mes faveurs ou rien. La découverte Mais là, au détour de surfing on the toile, à la recherche de \"hits\" pour le fils, à par BattleFront II et COD WWII, en décembre 2017, rien d'autre ne semblait se distinguer. Mais mais mais, l'univers de Horizon Zero Dawn a fini par bien me plaire. Un univers post apocalyptique, où l'homme redevient simple humain au sein de tribus. Armés d'arcs et de flèches, vivant dans une nature hostile, au dessus des vestiges de bâtiments et structures d'une époque proche de la nôtre... Chassés par des énormes animaux robotisés ayant résistés à cette fin du monde \"commune\", voire même qui l'aurait provoquée sous l'impulsion de quelques \"savants fous\". Le jeu Ce qui suit n'est qu'un petit tour d'horizon sans rien spoiler, pour cela il y a plein de sites bien faits fournissant détails en tout genre et des soluces quand on sèche sur une mission. Je ne vous ferai donc pas le tour de tout ce que fourni \" Guerilla Games \", ce afin de vous préserver, et vous laisser l'envie de vous mettre à ce jeu. L'histoire commence par la découverte d'une gamine élevée par un mentor, et initiée au rudiment de la chasse, gamine que l'on voit grandir, pour passer une épreuve de \"guerriers ado\": \"l'Eclosion\", qui sera le début de tout ce qui découlera. Jeu ouvert et totale liberté du choix du scénario À aucun moment vous ne vous retrouvez contraint de suivre un chemin, si vous vous retrouver dans une mission trop ardue pour le niveau de compétence du personnage, on peut partir du lieu où on se trouve pour se rendre ailleurs dans un lieu plus serein pour se refaire une santé quand ça se gâte :) Des quêtes, du très très facile (chasser du lapin/sanglier/renard/pêcher du poisson etc pour pouvoir construire des armes) au très très difficile (tuer une horde de Ravageurs/Dents de Scie au fin fond d'une caverne où l'on doit pirater un \"coeur\" de creuset) mais, fort heureusement, on nous répète à coeur et à cris, que tout dans le jeu n'est jamais rendu \"impossible\" (et vous allez lire pourquoi la dessous:) Se déplacer dans la pampa Bon évidemment, en jouant on peut faire le neuneu bourrin, et traverser toute la carte à dos de Brouteuse ou Galopeur pour se contenter des missions principales, mais on passera forcement à coté des petites qui, toutes aussi immersives, permettront de se refaire une santé, recharger ses munitions, gagner des points de compétences etc. les montures Cependant comme c'est \"ouvert\", vous pouvez passer d'une quête secondaire à une principale et inversement. Vous n'êtes pas obliger de finir ce que vous avez commencé, on le laisse tomber et on y revient quand on s'en sent l'envie. Par contre, dans les zones dites corrompues, si vous entamez la mission et en partez parce qu'à cours de flèches, quand vous y reviendrez, les ennemis éliminés seront de retour. De même, les zones corrompues sont délimitées par un cercle rouge, si vous en sortez trop longtemps, ca sera la même sentence. Pas glop quand on vient de ratatiner des gros zanimo pas boméchant. Donc en gros quand on se tape une zone corrompue, il faut y aller blinder de munitions sur tous ses arcs. A l'opposé du bourrin neuneu qui fonce de quête principale en quête principale, on peut prendre le temps de faire des \"tâches\", et quêtes secondaires (initiées par des personnages avec un point d'exclamation au dessus d'eux) pour acquérir des points de compétences en aidant les personnes croisées sur le chemin. Une tâche consiste par exemple à ratatiner des malheureux raton-laveur/sanglier/lapin pour récupérer leur viande/peau/os et se forger une carquois plus grand et passer d'une capacité de stockage de 10 à 40 flèches. Mais comme ne tombe pas tout cuit, même en écrabouillant l'une de ces bestioles vous n'allez systématiquement tomber sur, soit l'os, soit la peau, soit la viande à chaque coup meurtrier. Nan, les récompenses sont random. Selon la couleur de la récompense bleue ou verte, on sait qu'on aura mis la main sur l'os ou la peau, récompense en blanc, vous choppez la viande. Un quête secondaire , c'est \"un coup de main\" demander par un personnage que vous croisez dans les villes ou en chemin et qui vous dira \"ma soeur est en danger ; peux tu me l'a sauver des griffes de XXX ?\" et comme zetes bonne poire, qu'est-ce que vous ne feriez pas pour un peu plus de points d'XP :) Perso quand je me déplace, quand on est accroupis dans les hautes herbes, on n'est pas visible par les machines, du coup par réflexe, quand je me retrouve dans des lieux inconnus je me dis \"si hautes herbes pour se cacher, doit y avoir danger\" et je fais ma \"pause caca\" en scrutant \"au focus\" les alentours pour repérer un danger imminent, au cas où. Les compétences, les armes Avec les points de compétences acquis, vous les échangez pour de nouvelles aptitudes du genre, \"chasseuse\" pour avoir un carquois plus grand pour ses arcs et flèches, du genre \"discrétion\" (pouvoir courir à coté d'une machine sans se faire repérer au bruit). A coté des aptitudes de Aloy, on a la possibilité d'apporter des \"modifications\" à ses armes pour accentuer leur puissance, ou à ses armures (armures qui ne sont qu'en peau de bete hein:) pour accentuer leur résistance. Parmi les flèches de vos arcs, certaines sont de feu, de pierre, d'électricité, ou implosives (très très pratique pour faire sauter la carapace des carapateurs/crocodile par exemple, qui une fois sans défense, se finissent avec des flèches normales). Tout ce mélange de gestion de ses ressources, vous amène à avoir une stratégie accrue selon les lieux (montagne/déserts) et les animaux (tantôt faible au feu et résistant au froid tantôt faible à l'électricité et résistant au feu par exemple). Les zanimos machines Les machines portent des noms évocateurs de leur fonction plus que de leur forme d'animal. Certaines sont très connes, certaines très retors et ne tombent pas dans vos pièges \"trop gros\". Ah oui je ne vous ai pas dit, on peut poser des pièges pour se défaire de casse pieds :) Du coup pour les plus malins qui voient/sentent le piège, ils ne viennent pas à vous et c'est eux qui vous attendent de pattes fermes ! Les points de vie En cheminant de ci de là, vous croiserez des plantes qui retabliront la santé de votre personnage. Et quand cela ne suffira pas, vous aurez la possibilité d'acquerir des potions, quand vous n'en trouverez pas sur les corps des méchants :) Conclusion Le niveau le plus élevé est le 50, j'en suis à 42 et j'ai à peine fait 50% du jeu (selon la barre de progression du jeu). J'ai encore plein de petites tâches qui me pendent au nez comme des défis des chasseurs, assez ardus pour le manchot que je suis sur manettes (15ans de clavier/souris ca aide pas :). Avec le niveau du jeu choisi (qu'on peut accentuer pendant la partie ou adoucir) on prend le temps de faire le tour des mondes. Perso, le seul truc qui ne m'intéresse pas ; ce sont les dialogues entre Aloy et les personnes qu'elle croise. Je cherche l'action (des relents de FPS ca :) Par contre les enfants, qui au début ne s'intéressaient pas à ce jeu, scotchés sur StarWars BF2, s'y sont mis finalement et eux aiment justement les échangent entre les personnages, articuler comme un film... tout pareil. Au final tout peut se résumer à \"Graphisme superbe & Univers immersif avec très grande modularité du personnage et liberté de choix\". Et pour un jeu qui payait pas de mine, Sony semble faire un carton avec ce jeu inattendu.","tags":"Games","url":"https://foxmask.net/post/2018/02/05/horizon-zero-dawn-aparte/","loc":"https://foxmask.net/post/2018/02/05/horizon-zero-dawn-aparte/"},{"title":"Quand Fabric refuse de bosser","text":"Un comportement completement fou se produit pour moi avec Fabric, il n'execute absolument pas une command (run ici) en se faisant jetter par le serveur où il devrait executer celle ci... Je pose ça là vite fait pour qui rencontrerait le problème à son tour J'ai un script, le plus con du monde qui execute la commande id sur le serveur de mon choix, comme suit : # coding: utf-8 from fabric.api import env , local , run , settings from fabric.colors import blue , yellow env . user = 'root' env . password = 'root' def the_log ( f ): def new_f ( server ): msg = \"[ {0} ] {1} \" . format ( server , f . __doc__ ) print ( blue ( msg )) f ( server ) return new_f @the_log def main ( server ): \"\"\"This is my main function that will triggers anything\"\"\" with settings ( warn_only = True ): output = local ( \"id\" ) print ( yellow ( output )) output = run ( \"id\" , shell = True , combine_stderr = True ) print ( blue ( output )) Quand ce script tourne il fait : [ foxmask:~/DjangoVirtualEnv/monitoring/monitoring ] [ monitoring ] $ fab main:server = server1 -H server1 [ server1 ] Executing task 'main' [ server1 ] This is my main function that will triggers anything [ localhost ] local: id uid = 1000 ( foxmask ) gid = 1000 ( foxmask ) groupes = 1000 ( foxmask ) ,24 ( cdrom ) ,25 ( floppy ) ,29 ( audio ) ,30 ( dip ) ,44 ( video ) ,46 ( plugdev ) ,100 ( users ) ,103 ( scanner ) ,112 ( netdev ) [ server1 ] run: id on voit le resultat des 2 commandes... la seconde ne fonctionne pas et pourtant un ssh passe lui ssh root@server1 id uid = 0 ( root ) gid = 0 ( root ) groupes = 0 ( root ) après avoir écrèmé la toile à la recherche de config SSH que j'aurai raté, je testé le meme script sur un serveur. Et la entre serveurs tout passe. La difference avec ma station, OpenSSH 7.1 vs OpenSSH 5.3 Donc me dis que c'est quand même gonflé qu'une diff d'openssh me mette dedans Je mets à jour OpenSSH sur la Redhat mais j'arrive tout juste à la 5.3.13. Je mets du PDB partout pour voir qu'est-ce qui n'irait pas. Et je tombe sur cette page http://www.fabfile.org/troubleshooting.html?highlight=ssh Où je fais un bon copier coller du code sous le paragraphe \"Enable Paramiko-level debug logging\" Et là à l'execution ca donne : [ foxmask:~/DjangoVirtualEnv/monitoring/monitoring ] [ monitoring ] $ fab -f main main:server = server1 -H server1 -p pass1 [ server1 ] Executing task 'main' [ server1 ] This is my main function that will triggers anything [ localhost ] local: id uid = 1000 ( foxmask ) gid = 1000 ( foxmask ) groupes = 1000 ( foxmask ) ,24 ( cdrom ) ,25 ( floppy ) ,29 ( audio ) ,30 ( dip ) ,44 ( video ) ,46 ( plugdev ) ,100 ( users ) ,103 ( scanner ) ,112 ( netdev ) [ server1 ] run: id DEBUG:paramiko.transport:starting thread ( client mode ) : 0x66aed748 DEBUG:paramiko.transport:Local version/idstring: SSH-2.0-paramiko_2.4.0 DEBUG:paramiko.transport:Remote version/idstring: SSH-2.0-OpenSSH_5.3 INFO:paramiko.transport:Connected ( version 2 .0, client OpenSSH_5.3 ) DEBUG:paramiko.transport:kex algos: [ 'diffie-hellman-group-exchange-sha256' , 'diffie-hellman-group-exchange-sha1' , 'diffie-hellman-group14-sha1' , 'diffie-hellman-group1-sha1' ] server key: [ 'ssh-rsa' , 'ssh-dss' ] client encrypt: [ 'aes128-ctr' , 'aes192-ctr' , 'aes256-ctr' , 'arcfour256' , 'arcfour128' , 'aes128-cbc' , '3des-cbc' , 'blowfish-cbc' , 'cast128-cbc' , 'aes192-cbc' , 'aes256-cbc' , 'arcfour' , 'rijndael-cbc@lysator.liu.se' ] server encrypt: [ 'aes128-ctr' , 'aes192-ctr' , 'aes256-ctr' , 'arcfour256' , 'arcfour128' , 'aes128-cbc' , '3des-cbc' , 'blowfish-cbc' , 'cast128-cbc' , 'aes192-cbc' , 'aes256-cbc' , 'arcfour' , 'rijndael-cbc@lysator.liu.se' ] client mac: [ 'hmac-md5' , 'hmac-sha1' , 'umac-64@openssh.com' , 'hmac-sha2-256' , 'hmac-sha2-512' , 'hmac-ripemd160' , 'hmac-ripemd160@openssh.com' , 'hmac-sha1-96' , 'hmac-md5-96' ] server mac: [ 'hmac-md5' , 'hmac-sha1' , 'umac-64@openssh.com' , 'hmac-sha2-256' , 'hmac-sha2-512' , 'hmac-ripemd160' , 'hmac-ripemd160@openssh.com' , 'hmac-sha1-96' , 'hmac-md5-96' ] client compress: [ 'none' , 'zlib@openssh.com' ] server compress: [ 'none' , 'zlib@openssh.com' ] client lang: [ '' ] server lang: [ '' ] kex follows?False DEBUG:paramiko.transport:Kex agreed: diffie-hellman-group-exchange-sha256 DEBUG:paramiko.transport:HostKey agreed: ssh-rsa DEBUG:paramiko.transport:Cipher agreed: aes128-ctr DEBUG:paramiko.transport:MAC agreed: hmac-sha2-256 DEBUG:paramiko.transport:Compression agreed: none DEBUG:paramiko.transport:Got server p ( 2048 bits ) DEBUG:paramiko.transport:kex engine KexGexSHA256 specified hash_algo <built-in function openssl_sha256> DEBUG:paramiko.transport:Switch to new keys ... DEBUG:paramiko.transport:Trying discovered key b '8fbb1c6beb76bcc6767eccfcbef3730e' in /home/foxmask/.ssh/id_rsa DEBUG:paramiko.transport:userauth is OK INFO:paramiko.transport:Authentication ( publickey ) failed. du coup me v'la frais avec un paramiko qui met son caca en trouvant bien que l'userauth est ok mais pas la publickey. Ca te ferait mal de me laisser passer puisque tu as vu que le user/pass etait ok ? Etape suivante, trouver comment forcer SSH à utiliser le mot de passe et basta ... Je n'ai pas encore mis la main sur une solution , je mettrai à jour si j'en trouve une d'ici là :) @Tchao :)","tags":"Techno","url":"https://foxmask.net/post/2018/01/23/quand-fabric-refuse-bosser/","loc":"https://foxmask.net/post/2018/01/23/quand-fabric-refuse-bosser/"},{"title":"2017 at a glance","text":"2017 a special year this year was plenty of differents and rich activities from differents aspects OpenSource activities In 2015 my github activity concerned 90 contributions where in 2016 I increased by 400%, and finally by 50% more in 2017. During this year, I published 7 releases for the project TriggerHappy, from 1.3.0 to 1.5.2. New ideas, new needs I started a new project with VueJS and Django to produce an a11y editor, with OpenDyslexia font, speech2text and text2speech. I could quickly make something simple that works perfectly, but not for speech2text/text2speech where I still don't know how I could manage that part. That project allowed me to discover a new world between VueJS and DRF. That was a real pleasure. Now VueJS has progressed a lot, and other projects to bootstrap Django project with VueJS have been created to simplify everything, every steps, of the integration. Hacktoberfest (october 2017) An event that allows to meet dev arround the Trigger Happy project with the issues I opened and with other projects I submitted PR like Twython, Requests, Mastodon.py, Cozy-debian, Dropbox Paper, Django Channels and so on. At the end of the month I released a special \"Hacktoberfest version 1.5.0\" to thank every contributors. Meetup I went to just two meetups this year, the Django Paris meetup in september, and one about VueJS/NextJS early this year. I hope to be able to go and see more of them next year. Donation As there are projects where I can't contribute, I make donations instead. This year was for Django Rest Framework. This is a really great project that is so powerful in its simplicity, and managed by really nice guys. PSF Member I joined the PSF also, and i'm now a PSF member in the middle of the armada of all the talented developers. We feel to be part of something bigger than us, and at the same time, a little grain of dust in the Python galaxy ;) Journée des Femmes (march 2017) A special day, where I offered the place of my blog, for women I knew, to publish articles on any subjects. 2018, End and Conclusion Each year, when you are a lonly developer, there are a lot of moments where you don't have the mood to dive into anything, and moments where you are very motivated and ready to redo the world. Thus, I have many ideas about my main project for the next year, and i'd be glad to exchange with future contributors. Finally, 2018 will be the year of my 20 years of contributions in the OpenSource world. I will post about the path I followed since then, that cuold be fun to have a look at of thoses years :) I wish you the best for the celebretion of Christmas","tags":"Techno","url":"https://foxmask.net/post/2017/12/21/2017-at-a-glance/","loc":"https://foxmask.net/post/2017/12/21/2017-at-a-glance/"},{"title":"Coverage, install, upload results","text":"Installation of Coverage pip install coverage running on a django project : Running coverage coverage run -- source = '.' ./ manage . py test - v2 coverage html coverage report Publishing the results with covveralls.io On https://coveralls.io/github/foxmask/django-th get the token, then install pip install python - coveralls Now the command coveralls is available As I don't use Travis (but not in pro) and don't have private repo, I don't put the token in a .coveralls.yml at all, so to share the result we use the command line : COVERALLS_REPO_TOKEN = _the_long_string_of_the_token coveralls INFO : coveralls : 200 INFO : coveralls :{ \"message\" : \"Job ##1.1\" , \"url\" : \"https://coveralls.io/jobs/30251510\" } If you go back to coveralls.io, you will find the job and the result of the coverage Then add the following to your README.rst (for example) .. image :: https://coveralls.io/repos/github/foxmask/django-th/badge.svg :target: https://coveralls.io/github/foxmask/django-th End Ready to go !","tags":"Techno","url":"https://foxmask.net/post/2017/10/14/coverage-install-upload-results/","loc":"https://foxmask.net/post/2017/10/14/coverage-install-upload-results/"},{"title":"Retour sur des clients Mastodon","text":"Dans le genre \"j'ai testé pour vous\", voici un retour sur des clients mastodon pour android, que j'ai croisé depuis l'ouverture de ce reseau social 1 Tusky : pour: UI agréable, le haut de l'appli est épurée (comparer à Subway Tooter par exemple , cf plus bas) contre: mono compte on n'a pas le compteur du nombre d'étoiles, retoot, réponses pour une toot donné 2 Mastalab : pour: multi compte appli propre traduction des toots à la demande programmation de toots l'auteur de l'application est à l'écoute des demandes, par exemple C'est français (en plus d'être opensource) :P contre: la partie paramètres qui parait un peu trop chargée parfois le bouton de traduction apparait pour des mots \"non compris\" et qui ne veulent \"rien dire\", comme quand je toot \"bonjello\", du coup ca pollue un peu visuellement 3 Subway Tooter : pour: multi compte la personalisation des couleurs de l'interface la personalisation de l'affichage des onglets contre: on n'a pas le compteur du nombre d'étoiles, retoot, reponses pour une toot donné la grande quantitie de paramètres, comparé à mastalab, on ne sait meme pas à quoi serve la moitié :) on ne peut pas avoir une UI plus epuré, l'affichage constant du brandeau de l'onglet courant + celui du bas de l'écran est de trop. Un bouton flottant en bas suffirait et le haut pourrait disparaitre pour ressembler plus à ce que font mastalab ou tusky de ce coté là consomme trop de batterie 4 Twidere : pour: multi comptes on peut ecrire un texte qui est publié sur twitter et mastodon en même temps contre: la timeline mélange les toots et les tweets, comme j'aime bien le rangement, je prefere les separer, ce que ne permet pas l'application consomme trop de batterie beaucoup trop de paramètres plus que subway tooter 5 Client Web Je finirai pas un client mastodon web : tooty qui est super bien foutu mais dont le seul point \"contre\" que je souleverai sera que par defaut on ne peut pas définir la visibilité de ses toots. Il faut faire gaffe à chaque fois à qui on parle. 6 Conclusion Je recherchais dans ces clients, quelque chose de propre (et pas gourmand) et avec toutes les infos (les \"compteurs\") sur un toot donné, sans être contraint de l'ouvrir. j'en ai aussi testé d'autres qui étaient même en beta sur le playstore mais au final je conserve Mastalab , qui avec la mise à jour de ce mois, me va très bien à un truc ou deux près qui seront résolu dans les prochaines versions :) Merci donc à son auteur Thomas Schneider !","tags":"Techno","url":"https://foxmask.net/post/2017/08/31/clients-mastodon/","loc":"https://foxmask.net/post/2017/08/31/clients-mastodon/"},{"title":"Fabric, et la contrib django pour acceder à tous vos joujoux","text":"Dans mon billet précédent , j'évoquais l'existence d'une \"contrib\" Fabric pour intégrer Django au sein de ses tasks Fabric, mais qu'elle déconnait... Après maints tests supplementaires et digging ze toile, j'ai entrevu la lumière après la lecture de 3 issues chez Fabric #1509 , #1033 , #1207 . Tout compulsé ca donne ceci : # coding: utf-8 import sys import os # ajout du chemin où se trouve projetA path = os . path . join ( os . path . dirname ( __file__ ), '../../projeta' ) sys . path . insert ( 1 , path ) # appel de la librairie django de Fabric pour appeler les objets Django from fabric.contrib import django django . project ( 'projeta' ) # import de django import django # setup django django . setup () # appel du model from projeta.models import MonModel def main (): print ( MonModel . objects . get ( name = 'foobar' )) execution $ fab main foobar Done. Ceci fonctionne (pour un vhost) si vous avez collé votre appli \"fabric\" dans le même dossier que celui du projet Django, sinon adapter la ligne path = pour que ca match votre env","tags":"Techno","url":"https://foxmask.net/post/2017/07/10/fabric-contrib-django/","loc":"https://foxmask.net/post/2017/07/10/fabric-contrib-django/"},{"title":"Fabric, sa var 'env' et nos variables dynamiques !","text":"Fabric est un sérieux concurrent face à ansible, quoiqu'on en pense. Intro : J'ai eu à faire à ansible pendant plus d'un an pour automatiser des installations d'applications java sur tous les types de serveur d'applications du marché. Mais comme à mon habitude, je n'utilise pas des outils de DevOps pour ce qu'ils ont été conçus dès le départ, à savoir, répéter la même operation sur plusieurs hosts à la fois. Non, j'ai vu dans ansible, la possibilité d'automatiser toujours la même opération sur UN serveur et UN seul à la fois. En effet, les environnements jEE en entreprise sont du genre \"production\", \"test\", \"developpement\", et évidement, on ne va pas s'amuser à deployer la version de dev automatiquement en test et en prod. Donc on a 3 environnements par client, chacun vivant sa vie, comme chacun peut l'imaginer. Les corrections de bug de prod dans un coin, les évol en cours sur l'env de test etc... Du coup on voit bien que l'industrialisation \"classique\" des DevOps, consistant à installer 'n' fois un service sur 'n' serveurs, ne colle pas du tout au \"métier\". Ansible at first Avec ansible tout se passait pour le mieux, mais arriva un moment où, les serveurs unix devenaient retord et ansible n'accrochait plus ces derniers correctement. Plus de retour des tâches habituellement exécutées, tantôt stuck, tantôt plantées, mais trop souvent, restées dans le flou avec une question récurrente : \"mais où ca en est ?\" Alors sont en cause les serveurs eux mêmes puisque surchargés et RAM faible, mais à ce moment là je m'attendais à un minumum de reaction de ansible pour quand même \"revenir\" au serveur déclencheur du playbook et s'arrêter proprement. Donc las de cette situation, j'ai réécrit tous mon playbook et roles ansible en 5 modules python avec fabric en lieu et place. Fabric : avec ansible, il est possible de fournir un fichier JSON contenant des extra-vars. Très pratique pour moi, pour fournir à ansible, dynamiquement, le nom du serveur et les URL des applications java à recuperer pour les deplooyer ensuite. avec fabric , je fis un petit wrapper qui me permet de passer du JSON en un fichier settings à la django dynamiquement. Par contre, j'oubliais que comme 'n' users pouvaient utiliser mon appli Django pour déclencher les installations, tout allaient écrire dans le même fichier settings ... Donc en creusant comme avoir aussi avec Fabric un fichier d'extra-vars, je n'ai rien trouvé de meiux que de peupler la variable env de Fabric avec ma propre sauce. Vous pourriez objecter que c'est une grosse connerie parce que je vais me mélanger les crayons avec les variables de Fabric, mais que néni, mes variables sont toutes en MAJUSCULE, ce qui n'est pas la cas de Fabric. Donc env.user et env.password ne seront pas écrasées par mes env.ENV_USER et env.PASSWORD par exemple. Donc pour obtenir une fonctiionnalité équivalente de extra-var de Ansible avec Fabric, on spécifiera sur la ligne de commande l'option \"-c\" pour que Fabric aille chercher le fichier RC, qui lui, contiendra le même contenu que le fichier settings sus-mentionné. Ainsi 'n' users ne se marcheront plus sur les pieds. Dernière subtilité donnée dans la doc : les fichiers RC sont lus comme suit : CLE_A = valeur où valeur sera retournée sous la valeur : 'valeur' si valeur est un path ca donnera '/mon/path/' ce qui est la chianli quand on fait un os.path.join(env.CLE_A, 'sous_dossier') puisqu'on obtiendra comme valeur : '/mon/path/'/sous_dossier de même pour mon env.ENV_USER qui a pour valeur 'foxmask' ; quand je fais un sudo(cmd, user=env.ENV_USER) j'ai droit à un manignifique \"sudo user 'foxmask' n'existe pas\" de même pour un dict CLE_B = [{ 'truc_machin' : 'bidule' ] sera retourné sous une la forme d'une string du coup on le voit on se fait bien enfumer Pour régler son compte à ce comportement, reste à se faire une petite méthode pour épurer les valeurs de ses propres variables, avec un coup de env . MAVARIABLE = env . MAVARIABLE . replace ( \"'\" , '' ) là où c'est nécéssaire et appeler ladite méthode dans les modules où on utilise \"env\" :P Et pour la cas du dict, il faudra passer par le module \" ast \" pour revenir à un \"dict\" nomal, like this cle_b = ast . literal_eval ( env . CLE_B ) et on pourra retourner faire joujou avec son dict comme d'hab Fabric and Django integration Un moment j'ai cru voir une lueur d'espoir avec cette contribution qui permettrait d'accéder à \"ses affaires\" made in Django, depuis Fabric, genre au pif les données du modèle que j'injecte en JSON à la volée et récupère dans env.MES_VARIABLES_A_LA_NOIX mais, j'ai vite déchanté avec un erreur on ne peut plus bateau : ModuleNotFoundError : No module named 'monmodule' Conclusion : Voila voilo pour une billet fait rapidos sur des utilisations completements détournées du but premier :)","tags":"Techno","url":"https://foxmask.net/post/2017/07/06/fabric-sa-var-env-nos-variables-dynamiques/","loc":"https://foxmask.net/post/2017/07/06/fabric-sa-var-env-nos-variables-dynamiques/"},{"title":"Orotangi, pour se jouer de la dyslexie","text":"intro Je me suis lancé dans un nouveau projet, de prises de notes à la Evernote, qui sera loin de couvrir tout cette dernière offre mais pour l'heure ce n'est pas le but. Un sujet qui me tient à coeur ici, c'est qu'il me faut absolument la gestion de la police de caractères OpenDyslexic. En effet, \"n°2\" est concerné et c'est vraiement un plaie à gérer pour sa scolarité. J'ai le pot d'avoir un collège qui a un protocol pour les dyslexiques, et là c''est génial, mais il faut pousser plus loin. Par ailleurs il existe des associations pour dyslexiques dont Ordyslexie qui recycle des ordinateurs pour les reconditionner et fournir aux adhérents, moyennant un certain budget, \"un cartable numerique\" où l'élève dyslexique, fini par être plus rapide que son camarade non dyslexique. Seulement pour obtenir ce \"cartable\" il y a des démarches medicales et l'achat du \"cartable\", dont le coût s'envole grâce à la licence Microsoft OneNote... Du coup face, à ça, alors que Bill Gates (l'homme le plus riche du monde?) a monté une fondation (sans aucun rapport:), il aurait au moins pu laisser courir sur la licence de OnteNote..., je me suis lancé dans ce projet aux antipodes de ce que je fais habituellement. Le \"bidule\" (projet) inclura donc : Une prise de note facilité avec un éditeur \"tout pret\" avec l'utilisation de la police OpenDyslexic (ou non selon ce que chacun voudra) Du Text To Speech / Speech To Text, que le texte mis dans l'editeur soit lu au gamin (en surlignant mot à mot si possible), ou que le gamin parle et que ca enregistre ce qu'il dit dans une note. Les Dyslexiques n'étant pas tous identiques, chacun adaptera l'outil selon ce qui lui rendra service. Par exemple la police OpenDyslexic, pour certains d'entre eux n'est pas agréable, donc ils pourront retourner à une police \"classique\". Plus tard j'aimerai bien, comme ce que fait Ordyslexie, scanner des doc pour écrire directement dedans :) ou qu'après le scan, l'appli enregistre le texte dans une note pour que le TextToSpeech fasse la lecture du contenu. la Dyslexie et des \"génies\" Enfin à celles et ceux qui penseraient que les dyslexiques ne sont pas des lumières (vous en connaissez forcement 2, les plus célèbres qui soit) et que \"c'est rien, c'est passager\" . Il n'en est rien : Voici quelques témoignages sur Ted : Overcoming Dyslexia, Finding Passion The True Gifts of a Dyslexic Mind Free at last we are free at last Comme à mon habitude, ce projet est libre et je ne serai pas contre un peu d'aide si vous vous y connaissez un peu avec : Django Rest Framework VueJS l'UX adapté aux dyslexiques (même si vous n'y connaissez rien en dyslexie mais en UX je prends quand même ; de nombreux sites expliquent ce qu'on ne doit pas faire dans une page pour eux) Et comme \"qui peut le plus peu le moins\", ce projet devrait être utilisable pour tout le monde (par le biais de préférences / paramètrages) Checkpoint Les sources sont là En l'état actuel le projet permet d'ajouter des notes, d'importer des notes Evernote, de les afficher avec un éditeur Markdown (mais je pense le retirer parce que je ne m'imagine pas un enfant taper du markdown plus facilement que surligner le texte et sélectionner la mise en forme qui va bien, ou à défaut de le retirer, permettre par un paramètrage, d'utiliser l'editeur Markdown ou CKEditor/TinyMCE par exemple) Quand le projet sera mûr pour une première version, je ne pourrai que vous proposer de l'installer chez vous, je ne pourrai pas l'héberger comme ce que je fais pour un autre projet. Mais aucune crainte à avoir, la documentation sera là pour vous y aider :) End Cela sera mon 2nd projet qui vous permettra une nouvelle fois, de prendre le contrôle de vos données en les sortant des compagnies qui vous font payer pour ce service, Evernote s'étant illustrée en novembre dernier en s'occtroyant le prvilège de lire nos notes (avant de se raviser après que de nombreux clients aient manifesté leur mécontentement) ... ca sera la reponse du berger à la bergère du même coup :)","tags":"Techno","url":"https://foxmask.net/post/2017/04/25/orotangi-pour-se-jouer-dyslexie/","loc":"https://foxmask.net/post/2017/04/25/orotangi-pour-se-jouer-dyslexie/"},{"title":"Premières fois","text":"Premieres Fois Un petit tour de mes premières fois insolites Linux Installation Redhat 5 (la version libre du temps de debian 2 avant que Redhat ne devienne une boite), dans le manuel que je lisais page apres page je me suis farci la compilation du kernel de l'époque. Emacs au lancement je ne savais pas comment en sortir... j'ai reboot :P et plus jamais lancé ce truc Programmation Perl Pondre ce genre de truc : while ( <> ) print ; Adepte du \" Perlgolf \" avec une petite 21° place :) HTML 3 je découvris la premiere \"interface graphique\" avec laquelle j'ai produit en perl, des trucs QQ comme gérer les resultats du championnat de france de foot... fallait bien commencer par quelquechose :D PHP 3 ! arriver à gerer les sessions ! Oauth quand tu reussis à l'utiliser du premier coup Python découvrir Python, par le biais de Trac, et vite oublier (avant d'y venir 5ans +tard) Internet FAI - Hosting Passé du 3Ko/s avec son \"bippppppppp griiiii hiii ouiiichhhh\" au silencieux RNIS 7Ko messieurs dames à la raie manta pour du 128K Hébergé sa page perso \"gratuitement\" sur http://pagesperso.free.fr/ avec DotClear ! utiliser dyndns pour chopper son PC à la maison :) zero zocio twitter : \"mais à quoi ca sert ce truc\" ? Communauté \"Libre\" et premiers pas/contacts avec l'opensource Première Linux Expo, tenir le stand DaLinuxFrenchPage aux cotés de GCU, croiser R. Stallman en chausettes jouer au pipo la FSF Song dans les allées du salon :) puis Miguel de Icaza (le père de Gnome) IRC se faire kick ban en se pointant sur #linuxfr@ircnet en venant avec mIRC ! devoir se trouver un pseudo : vous voyez le résultat ) :) Jeux en ligne Découverte de Doom, lors d'une journée nationale de l'internet à Boulogne Billancourt avec DaLinuxFrenchPage et Fabien Penso Quand vint l'ADSL, alors HL1 puis CS1.3 devint mon violon dingue Vomir Steam à sa sortie","tags":"General","url":"https://foxmask.net/post/2017/04/07/premieres-fois/","loc":"https://foxmask.net/post/2017/04/07/premieres-fois/"},{"title":"Journée Internationale des Femmes - 8 mars 2017","text":"En ce 8 mars, journée internationale des Femmes , à la suite d'une idée de @kozlika qui me plu de suite, j'ai laissé la \"tribune\", l'entièreté du blog, à des femmes que j'ai eu l'occasion de croiser, et qui m'ont fait l'extrême plaisir et honneur de me permettre de reproduire quelques uns des articles qui leur tenaient à coeur. Parfois, la vie met sur votre chemin des femmes avec lesquelles vous échangez des idées sur votre métier, vos projets (libres), la Société ... et vous marque d'une manière ou d'une autre, que ca soit à ParisWeb , sur Irc, dans un pub, un resto... Voici donc ces \"Fées marquantes\" : Parmi elles, Coralie Mercier , Romy Duhem-Verdière , Sophie Drouvroy et Emilie Diab Coralie Mercier I work for a neutral intermediary I follow the International Committee of the Red Cross on Twitter and they twitted this earlier today: @ICRC: Our role as a \"neutral intermediary\" is at the heart of #humanitarian action. Our director of operations explains: [link] Original Message: Our role as a \"neutral intermediary\" is at the heart of #humanitarian action. Our director of operations explains: http://t.co/A2d49Gi6ER — ICRC (@ICRC) 30 mars 2013 The main part of their micro-post, \"neutral intermediary\" is at the heart of #humanitarian action , particularly resonated with me for several reasons, that I want to attempt to articulate in this post of what I did at one point as a hobby and how, in a way, some choices, people and events led me back to it. My years with the French Red Cross In my late teenage years I enrolled at the French Red Cross and during several years –until I started university– I participated in social, medical, training, fund-raising and first aid actions. It occupied my weekends, almost all my holiday time and several week evenings. I was very committed. I came to the Red Cross spurred by my twin brother who had recently become involved. It sounded useful and fun. It was indeed useful and fun. Even sorting clothes was fun. It was daunting; several piles of garments and shoes, tall as dunes, dumped in the vast depot next to the offices, that we had to plough through during hours. But at the end of the day (that is, late at night) we felt we had accomplished a useful action. Clothes and shoes, categorised and packed, were ready to be picked and handed off somewhere else. My friends and I would find a bar open till late for coffee and drinks, sometimes a game of cards, but mostly bonding. I met all kinds of people, from all walks of life, most of them interesting, some of them inspiring –students like me, nurses, police officers, business people, house wives, etc. I learned to give first aid, to operate a radio, to drive an ambulance (in particular to park it), to lead a team of first-aid workers, to cook for a crowd, to identify priorities, and to put things into perspective. I saw, heard, and experienced things that made me fully aware how lucky I was, and what a fine life mine was. I don't know if I was particularly gifted or actually good at it (I felt I was good), but my satisfaction was such that I wanted to make this my job. There even was a school I thought I might attend, Bioforce , which \"specialises in ‘support functions' (logistics, project coordination, administration and finance, human resources…) and in the field of water supply and sanitation.\" I didn't attend that school. I went to a local university instead, embarking on a different path. A few years later I looked for a job. I was a temp for a while. I worked as a clerk in a British law firm, although I tried very hard to wiggle out of this, as soon as I saw the place and realised the work conditions were going to be terrible. I passed the one interview I so wanted to fail. Thankfully it was a short mission. Discovering the world of a Research Lab I got my next job by luck. A friend of mine, whom I had met while studying in Edinburgh, let me know she knew someone whose mother worked with someone who needed a temp for a semester. Two actually. And my friend was on the market too, so it was perfect for the two of us. We both interviewed on the same day. We had been pre-assigned a position but after interviewing they changed their mind and swapped. She joined the administrative and legal department at INRIA Sophia Antipolis, I joined a research project as administrative aid. My years of volunteering and charity work were far behind me. The researchers I met were committed and inspiring people. Most wore shoes but many didn't. Most people appeared to not see the people around them, absorbed as they were. All had pens in the breast pocket of their shirt, or the pocket of their shorts. Most carried laptops. There were whiteboards everywhere and I had no idea what the colourful scribblings and equations meant. In that INRIA research project, I learned to type on a qwerty keyboard, to use e-mail on exmh, to print from a unix terminal, to get geek humour. I also learned LateX. Just for fun. When the end of my mission was near I wrote a fictitious humorous report, in LateX, featuring some of the people that crowded our floor. A thirty or so page report that I gave to the two project managers and a researcher I was particularly fond of. In exchange (not really), I was congratulated by the Director of the institute, and the project managers each gave me the bestest recommendation letters ever. I was on the dole for five months afterwards. My great letters, for all the power that I thought they wielded, didn't get me my next job. Joining the W3C Lucky again, someone who knew me was asked to tell me that the World Wide Web Consortium needed an administrative aid and that I should apply. The W3C was hosted at INRIA Sophia Antipolis and oddly enough people there seemed to remember me and speak highly of me! I interviewed and was hired. That was 14 years ago. What we do at W3C is basically convening the people who make the Web and the people who consume the Web, around a neutral table. The staff (there are between 60 and 70 of us, mostly technical, located throughout the world) is involved to help the Web stakeholders converge. From that collaboration, web standards are born, refined, and perfected. At W3C, I met the most incredible colleagues and co-workers, the most inspiring people, the most dedicated folks, bright, clever, helpful, friendly, reliable and supportive. Working with them, doing our job, doing this job, is fulfilling and gratifying. It's been a while so I have learned so much that it is difficult to synthesise. The one easy thing that comes to mind is NOT that I learned HTML or CSS (however, some of that I did learn), it is that I learnt to pack lightly, pragmatically and efficiently for trips abroad. We used to travel a lot. We still travel but not as much. I visited a big city for the first time during a W3C trip to a WWW conference. It was in Toronto. Then Boston, Tokyo, Hong Kong, Hawaii, Western Europe, Montreal, etc. I now pack in twenty minutes and travel with my purse, one carry-on and a laptop bag. For short trips, the carry-on is a small backpack. I can say that I have learned various jobs within our organisation. I started as administrative aid, and as such I organised meetings, ordered stationary, managed hirings and interns, I wrote internal policies, then managed the local office. I joined the Communications team and wore several hats. From secretary of the Advisory Board, translation community monitor, blog master, to Community Manager. I gather the press clippings, I send transition announcements to our Members when a technology progresses from one state to another, ultimately reaching that of Standard. I write to our Web site, which I occasionally break so I sweat a bit and eventually fix it. I do other internal comm things too. A couple years ago I had a skills assessment. I was at a point in time I wanted to focus on what I was good at, and what it was that I was skilled for. The exercise was interesting and useful. I was told my area of interest revolves around humanitarian activities and care giving. And that I have more than one string to my bow. No surprise, really, but it was reinforcement that I was in my field. My job is a passion. It may not be the humanitarian field action I dreamt of as a young woman, but many in our trade liken our job to humanitarian work. And indeed, we are a \"neutral intermediary\" at the heart of making free and open Web standards. billet à retrouver à sa source Romy Duhem-Verdière UX et logiciels libres : retour d'expérience (TAILS) Pas Sage En Seine, NUMA, jeudi 18 juin 2015 Retour d'expérience après un an de collaboration entre UX designers et développeurs libres, dans le but d'améliorer l'usage du logiciel Tails. Le but du logiciel libre Tails est de préserver votre vie privée et votre anonymat, de vous garantir un haut niveau de sécurité, c'est-à-dire de retrouver le même niveau de confidentialité que dans la vie hors ligne : lorsqu'on discute dans la rue, nos échanges ne sont pas systématiquement écoutés et enregistrés, contrairement aux échanges internautiques. Tails est né de la volonté de rendre cette protection accessible à tous et toutes, d'être facile à utiliser, là où il fallait auparavant installer plein de trucs compliqués. Ses utilisateurs ne sont pas des geeks, mais des lanceurs d'alerte, des opposants politiques, des ONG, mais aussi des victimes de violences conjugales, pour lesquelles utiliser Tails permet de contourner le contrôle abusif exercé par le conjoint. Problème : Tails reste encore trop difficile à utiliser. S'intéressant depuis toujours au design dans le logiciel libre, l'incubateur numérique NUMA a apporté son aide pour en améliorer l'expérience utilisateur. Plusieurs sessions de tests et de conception ont été organisées pour améliorer le projet Tails depuis son site web jusqu'au cœur de ses interfaces. Après une collaboration d'un an, un très intéressant retour d'expérience conjoint de Fiodor Tonti et Claudio Vandi, UX designers à NUMA, et de Tchou, développeur contributeur libre à Tails, a été partagé lors de Pas Sage En Seine 2015. En voici (enfin) ma prise de notes. D'abord observer les utilisateurs La méthode de recherche a été le « test utilisateur ». Ressources nécessaires : des utilisateurs et de la patience. Pas besoin d'un laboratoire, ni d'outils particuliers. Il faut juste avoir envie de s'y intéresser, précise le dev. Concrètement, il s'agit d'observer des utilisateurs à l'œuvre et tout noter. Avec méthode. Les utilisateurs observés sont pour la plupart des journalistes volontaires, qui ont déjà entendu parler de Tails et savent à quoi ça sert. Celleux-ci se voient confier des missions précises, représentatives des usages principaux de Tails : comment créer un document ? comment utiliser le mail une fois Tails lancé ? Il est important ici d'isoler des variables : identifier des buts clairs, élémentaires, afin d'identifier avec précision où l'utilisateur réussit et échoue. Ensuite, laisser les utilisateurs se démerder avec leur mission : l'équipe adopte une position d'observation, sans intervenir, même si l'envie démange de les dépatouiller. Quantifier les retours d'usages et problèmes observés Il faut tout prendre en note, mais attention à le faire de façon formelle, pour ne pas rester dans le subjectif et pouvoir analyser ensuite. Utiliser pour ça un rainbow spreadcheet permet de regrouper les notes par sources de problèmes, fréquence, et les hiérarchiser ensuite pour dégager leur impact sur le logiciel. Résultat : trop difficile à… installer ! De nombreuses difficultés se révèlent, qui sont liées, moins aux tâches confiées, que l'on souhaitait observer, qu'à l'usage basique de Tails… à son installation même. Carton rouge ! Ouvrir les yeux Du point de vue du développeur, il faut faire un travail sur soi, parce qu'il est vraiment naturel, quand on développe un logiciel, de vouloir répondre à l'appel à l'aide d'un utilisateur en difficulté comme il est tout « aussi naturel de ne pas vouloir voir les problèmes ». Par exemple, sur l'installation, on pensait que ça marchait parfaitement : puisque ça fonctionnait, il ne devait pas y avoir de difficulté d'usage, pas besoin de tester. Or les tests ont montré que c'était au contraire le cœur du problème. À l'observation des difficultés d'usage, quand on se dit « Ohlala, ça va être compliqué » (à corriger, à coder), c'est plutôt bon signe : c'est qu'on a identifié une vraie difficulté. Observer ainsi les utilisateurs permet de se décaler, sortir un peu de sa passion de développeur, pour voir autrement. En effet, en tant que développeur, l'idée de ce que vous être en train de construire n'est pas du tout alignée avec l'idée que les utilisateurs en ont. Ce n'est pas forcément que le logiciel est compliqué, ni mal fichu, mais que les utilisateurs ne le comprennent pas. Parce qu'on ne leur explique pas ! En fait, Tails n'est pas bon dans sa communication. Installation bloquante Tails est un système d'exploitation « live », c'est-à-dire installé sur un support amovible, à partir duquel on fait démarrer un ordinateur : c'est-à-dire que vous pouvez démarrer, sur quasiment n'importe quel ordinateur, depuis un DVD, une clé USB, ou une carte SD. Mais il est difficile de comprendre cela en visitant le site web de Tails. Pour nous les dev, c'est évident, mais en réalité les utilisateurs ne comprenaient pas du tout. C'est pourtant expliqué, mais avec des mots : trop peu expliqué, avec trop de mots. De fait, l'installation s'avère très difficile. Bloquante. Il faut d'abord télécharger des logiciels qui permettent d'installer Tails. Les users téléchargeaient plusieurs fois le truc sans comprendre. Un user est resté bloqué 30 min ! sur le reboot… Enfin, les users n'identifient pas quand Tails se lance, ni donc quand ils bénéficient de sa protection. En fait, tout le monde bloquait à l'installation. Pour les geeks, c'est facile, mais les users n'y arrivent pas. En réalité, qu'un utilisateur y arrive était exceptionnel. Et c'était parce qu'il était geek. La doc qu'on fournissait était trop laconique et les gens étaient perdus. Recherche de solutions Repenser en amont Suite à cela, il a fallu repenser jusqu'au user flow, le parcours utilisateur, dès l'installation. On a repris toutes les étapes pour les remettre à plat : lesquelles peut-on supprimer ? Quels scénarios d'usage veut-on privilégier ? On s'est rendu compte qu'il y avait 13 façons différentes d'installer Tails : quel gloubi-boulga ! Il a fallu repenser le parcours utilisateur pour l'install… Dessiner avant de coder La conception UX peut se faire très simplement, avec du papier et un crayon. Il faut d'abord réfléchir avant de se lancer dans le code. Si c'est clair sur papier, si d'autres réussissent à comprendre, c'est bon signe. Mais si un user ne comprend pas le process schématisé, ça ne sert à rien de le coder. Mieux vaut commencer sur papier, plutôt qu'en codant direct, parce que, si on se rend compte que ça ne marche pas, il est plus facile de jeter le papier que de jeter tout le code. C'est aussi moins douloureux. Pistes d'amélioration Plutôt qu'une documentation complète, opter pour un dévoilement progressif de l'info évite de noyer l'utilisateur dans une masse d'info qu'il ne comprend pas, qu'il ne sait pas par quel bout prendre. Deuxio : rapprocher l'info de l'endroit où en a besoin. Donner la bonne information dans le bon contexte. Souvent l'user bloquait parce qu'il n'avait pas l'info au moment où il en aurait eu besoin et il ne sait pas forcément que l'info existe et encore moins où la chercher. En réalité, il aurait fallu lire toute la doc avant, et l'avoir bien comprise, pour avoir une chance de réussir avec Tails. Enfin, faire une vidéo pour expliquer les étapes ? Trop long ! En testant, on s'est rendu compte qu'un gif animé suffisait. Faites des choses simples et efficaces, qui coûtent moins de temps et d'énergie. Pourquoi se préoccuper d'UX ? Pour être utilisé Se préoccuper d'UX est important si vous souhaitez que davantage de monde utilise ce que vous faites. Si vous n'avez pas besoin d'utilisateurs, alors pas besoin d'UX design. Découverte des UX designers dans cette collaboration : autant l'UX est naturelle est évidente pour les startups, autant ce n'est pas évident dans le monde libre. Dans le logiciel commercial, le besoin d'UX est implicite, car l'objectif étant de vendre, il est nécessaire de faciliter l'usage. Dans le monde libre et gratuit, c'est moins évident, certains logiciels libres n'ayant pas pour ambition d'être très utilisés. Mais que Tails soit simple d'usage est incontournable, pour éviter, par exemple, qu'un blogueur au Pakistan qui a compris que Tails est indispensable pour sa sécurité, soit incapable de l'utiliser, malgré toute la bonne volonté qu'il peut y mettre, et se retrouve donc exposé. C'est quand même la raison d'être de Tails : que ce soit plus simple que d'installer des tas de logiciels. Pour éviter de contre-performer Seconde raison : éviter que le manque d'utilisabilité produise l'effet contraire de celui voulu. Que les utilisateurs croient bénéficier de la confidentialité promise par Tails, ignorant qu'ils n'ont tout simplement pas réussi à l'installer, est un très sérieux problème pour un logiciel dont le but est de garantir la sécurité de l'utilisateur. Si l'objectif même du service est que les gens puissent agir en sécurité, mais que celui-ci ne permet pas d'identifier si on est dans un usage sécurisé ou pas, il faut savoir se remettre en question. Pour éviter cela, il ne faut pas perdre de vue l'utilisateur auquel votre logiciel prétend rendre service. Qu'avons-nous appris de cette collaboration ? Ce qu'il faut savoir, quand on fait de l'UX design, c'est qu'on remet en cause les fondamentaux. Il faut être prêt à se remettre en question , et accepter de questionner la raison d'être du logiciel. Et ça, c'est le dev qui le dit ! Ça va bousculer les choses. Et prendre du temps. Les développeurs ont tendance à mesurer le progrès en nombre de lignes de code, de features produites, de tickets fermés… Mais ça ne compte pas dans l'expérience utilisateur. Il faut adopter une autre modalité d'appréciation , passer de feature-driven à value-driven development. Il ne s'agit plus d'apprécier si le logiciel ou une fonctionnalité marche, mais si elle sert. C'est-à-dire évaluer si le blogueur pakistanais a réussi à transmettre de l'information sans se faire choper par la police. Bref, on n'utilise pas les mêmes metrics pour évaluer un bon code et un bon design. Qu'un logiciel soit fonctionnel et qu'il soit utile sont deux choses très différentes. Il y a un grand gap culturel entre UX et logiciel libre. Ce n'est pas un mariage évident. Autant on peut faire et distribuer des bouts de code assez facilement, autant l'UX est une démarche globale , avec une vision holistique du service. C'est donc compliqué à intégrer dans une équipe où chaque développeur bosse isolement sur un bout du projet. Attention, se soucier de l'utilisateur, ce n'est pas satisfaire le moindre de ses désirs. Les UX designers rappellent qu'il faut : avoir la patience de considérer les feedback utilisateurs. Si quelqu'un fait un retour critique, ce n'est pas parce qu'il est con. Ni méchant. Il faut prendre ça comme une occasion d'améliorer. D'un autre côté, avoir l'intelligence de ne pas implémenter aussitôt la moindre chose demandée ou critiquée par les utilisateurs, mais prendre le temps de réfléchir, de tester. Suivre les demandes des users peut faire complètement dérailler un projet. Bref, quand un utilisateur demande un feature, la première chose à faire n'est pas de la coder, mais de demander pourquoi, c'est-à-dire d'identifier la source de la difficulté rencontrée. Enfin, c'est plus facile si le souci de l'utilisateur est partagé par toute l'équipe et reste une vigilance constante : vaut mieux désherber régulièrement que de laisser une forêt de ronces s'installer. Les designers ne sont pas des magiciens. Dernière chose : travailler ensemble, UX et dev LL, ça marche ! Pour finir : « J'invite tous les dev à payer un coup à un UX designer, à parler avec… Vous avez plein à y gagner ! » Hips :) J'ai mis du temps à assimiler et retranscrire ce retour d'expérience passionnant. Ce qui est assez symptomatique, c'est que je ne comprenais rien lorsque le développeur parlait — trop de jargon technique pour moi — alors que son témoignage est particulièrement intéressant : il m'a fallu réécouter plusieurs fois l'enregistrement pour bien comprendre et pouvoir ici en rendre compte. Merci beaucoup à Tchou, Fiodor et Claudio de nous avoir partagé leur retour d'expérience après cette collaboration ! Au programme : UX et logiciels libres : retour d'expérience (TAILS) , Pas Sage En Seine, NUMA, 2015, avec archive vidéo , slides de la présentation , ainsi que le livetweet , avec questions-réponses de la salle. Lire aussi : Tails, l'outil détesté par la NSA, qui veut démocratiser l'anonymat en ligne , Le Monde, 2014 et Tails raconté par ceux qui le construisent , par Amaelle Guiton, Techn0polis, 2014. billet à retrouver à sa source Sophie Drouvroy Je ne pense pas trop vous faire découvrir Sophie , que j'ai rencontrée il y a des années, au hasard d'un resto un midi avec le sieur @notabene , puis revue lors d'un mémorable Ligthning Talk à ParisWeb2012 Emilie Diab Je ne pouvais pas finir sans lui faire un petit coucou, elle qui ne publie plus (DoooOOmage), et, que j'aprécie également beaucoup ;-)","tags":"General","url":"https://foxmask.net/post/2017/03/08/journee-des-femmes-8mars-2017/","loc":"https://foxmask.net/post/2017/03/08/journee-des-femmes-8mars-2017/"},{"title":"Orotangi, your thoughts everywhere","text":"Intro If you heard about Evernote and its little slap it got, at the end of november 2016, from the users community (because of the changes of the policy related to our own notebooks), you will then now understand what the purpose of that project. Orotangi is your own private notebook where you will store your thoughts and access them from anywhere. Goals Once again, as I already said, when something bother me, I build something else by myself. I already made or use the following open source a clone of : IFTTT with Trigger-Happy Pocket with Wallabag and now I start to make an open source clone of Evernote with Orotangi. Yes, that's the toolbox I use to organize my everyday life : settings reminder for rendez-vous grabbing news to be read later write a meeting report put thoughts of projects and so on Where is the project ? As I started at the end of january, it's absolutly not usable at all but I launched the project on github, in two pieces : the back with Django Rest Framework and the front with Vue.js . Why that one ? In my Freedom quest, as usual, I don't do anything to make project better than the original ones, I make them, because I need it first of all, and to feel well in my shoes :P By \"well in my shoes\", I mean disposing of my life with tools that are not the property of a company that could spy what I do with their services. By \"could spy\", I mean a company that could decide by itself to read our personal data or track the things we do with our data, or the Government of those companies which could bend each of them, to grab our own personnal data and control our life. Yes, I know, you could say, \"go on and make an opensource clone of gmail, facebook\". But in fact, I do things by myself when I don't have the choice, and for those \"services\", I can switch from gmail to my hosting provider tomorrow, it's not a big deal for myself as I'd already done that for the account I use for my \"developer life\", and that will be finished to be spied. And I don't care of facebook at all. When things go wrong with google and gmail I will warn the family to start thinking about changing their habits, but that... it's another story....","tags":"Techno","url":"https://foxmask.net/post/2017/02/09/orotangi/","loc":"https://foxmask.net/post/2017/02/09/orotangi/"},{"title":"Trigger Happy two weeks after a strong Storm...","text":"Two weeks ago, I told to myself, to try a last thing to get out the project of anonymity, before stopping publishing news about it. After all, I made the project for myself, If nobody needs it. It's Okkkkkkkkkay. But after some nice comments and exchanges on Linuxfr.org , a friend, that I had lost sight of, decided to post the project on HackNews . Then came the storm for the project. A lot of things suddenly awoke in the same times. First The stars on the github page reached the double in 2 days. But, as the link on hackernews pointed to the home of TriggerHappy website, they had to search on this one where to find the source of github. Yes the link is in the bottom, but how many people does not scroll down and leave before ? That signal told me hmm guys would seem to appreciate, as to reach that page, they come from the main page of the project, and the curiosity is awaken Second : The day after, a well known french website korben posted an article too. I thought \"God, heaven can wait\" :D the stats of the website exploded November the 14th: 30.000 hits in 2 days. Reminder, the project is unknown until then, so the visits are mine :) Third the day after, the project reachs the Python Github Trending : position 8. Behind were : Ansible, Django, Requests, not less ;) This, during a week. Consequences : Some nice mails (like one from an ex developper from ... IFTTT.com :) and github issues for nices requests like supporting new service, and encouragements on IRC and Twitter, essentially from people that do not come from the Python univers, or from non-French developpers fom the Python world. Here is a little part of the web, which spoke about Trigger Happy : 4 short links CronWeekly n0where with a rate of 50℅ Forum peerlyst (en) Forum Domadoo (fr) Forum Producthunt Today The madness is fallen, I can continue to sleep well without being overloaded by many requests ;) But of course, do not hesitate to ask for them, or drop a message to exchange ideas. for example like here . May be one day I will post you an article about Taiga.io, not necessary about the product/project itself but about the guy , bameda, who helped me a lot about how to interact with Taiga with great kindness... This is for this kindness and this exchanges that I always love to make things here and there, for the Open Source, my way.","tags":"Techno","url":"https://foxmask.net/post/2016/11/26/trigger-happy-two-weeks-after-strong-storm/","loc":"https://foxmask.net/post/2016/11/26/trigger-happy-two-weeks-after-strong-storm/"},{"title":"Say Thanks","text":"Thankfulness is the key to happiness Sometimes you never received a thanks for a project you made for a friend, sometimes you received a thanks from unknown people who just had fun with a little piece of code. Open Source is like that ;) And then, recently I crossed the road of a funny project \"Say thanks\" . Which permits to drop a little word to the author of the project you enjoyed. This project looks as clear and clean, as we usually see projects by Kenneth Reitz, \"Mister Requests\" . The strength is the simplicity. (I am asking myself if he does not play chess too, where strength in simplicity is often met:). I noticed a detail about the README of his project \"Random inspiration Links\" ;) Then, if you have enjoyed Trigger Happy , just drop me a word from","tags":"News","url":"https://foxmask.net/post/2016/11/26/say-thanks/","loc":"https://foxmask.net/post/2016/11/26/say-thanks/"},{"title":"UnitTest, Coverage and Mock","text":"Introduction When we write unittest, it comes the moment when we cross the road of some services that we can't test. So the moment comes when we have to use Mock In the following text, you should spot some evidence related to django ;) Function to mock here is the piece of code I will \"mock\" : def save_data ( self , trigger_id , ** data ): \"\"\" let's save the data :param trigger_id: trigger ID from which to save data :param data: the data to check to be used and save :type trigger_id: int :type data: dict :return: the status of the save statement :rtype: boolean \"\"\" status = False # set the title and content of the data title , content = super ( ServiceTwitter , self ) . save_data ( trigger_id , ** data ) if data . get ( 'link' ) and len ( data . get ( 'link' )) > 0 : content = str ( \" {title} {link} \" ) . format ( title = title , link = data . get ( 'link' )) content += self . get_tags ( trigger_id ) try : self . twitter_api . update_status ( status = content ) status = True except Exception as inst : logger . critical ( \"Twitter ERR {} \" . format ( inst )) update_result ( trigger_id , msg = inst ) status = False return status Mock a complet function the piece of unittest with the mock applied to save_data : from unittest.mock import patch def test_save_data ( self ): token = self . token trigger_id = self . trigger_id content = 'foobar #tag' self . data [ 'title' ] = 'a title' self . data [ 'link' ] = 'http://domain.ltd' self . assertTrue ( token ) self . assertTrue ( isinstance ( trigger_id , int )) self . assertIn ( 'text' , self . data ) self . assertNotEqual ( self . data [ 'text' ], '' ) with patch . object ( ServiceTwitter , 'save_data' ) as mock_save_data : se = ServiceTwitter ( self . token ) se . save_data ( trigger_id , ** self . data ) mock_save_data . assert_called_once_with ( trigger_id , ** self . data ) then the testing show us coverage run --source='.' manage.py test -v2 ... test_save_data (th_twitter.tests.ServiceTwitterTest) ... ok ... Fine ! But the coverage report (in html) shows us and the % [foxmask:~/DjangoVirtualEnv/django-trigger-happy/django-th] [django-trigger-happy] coverage report -m |grep twitter th_twitter/__init__.py 2 0 100% th_twitter/forms.py 12 0 100% th_twitter/models.py 21 0 100% th_twitter/my_twitter.py 117 56 52% 119-121, 138-173, 190-209, 219-231, 241-250, 256, 273-278 th_twitter/tests.py 80 0 100% Mock one FunctionB in a FunctionA It's fine, but in our quest of the perfect tests and to be sharper, we would like to test the content of save_data and only mock the function that makes the call to the Twitter API (named Twython). To do so we can use the manager like previously, or a decorator. Just have a look : from unittest.mock import patch @patch . object ( Twython , 'update_status' ) def test_save_data ( self , mock1 ): self . create_twitter () token = self . token trigger_id = self . trigger_id content = 'foobar #tag' self . data [ 'title' ] = 'a title' self . data [ 'link' ] = 'http://domain.ltd' self . assertTrue ( token ) self . assertTrue ( isinstance ( trigger_id , int )) self . assertIn ( 'text' , self . data ) self . assertNotEqual ( self . data [ 'text' ], '' ) se = ServiceTwitter ( self . token ) se . save_data ( trigger_id , ** self . data ) mock1 . assert_called_once_with ( status = content ) And this time the coverage report show us and the % [foxmask:~/DjangoVirtualEnv/django-trigger-happy/django-th] [django-trigger-happy] coverage report -m |grep twitter th_twitter/__init__.py 2 0 100% th_twitter/forms.py 12 0 100% th_twitter/models.py 21 0 100% th_twitter/my_twitter.py 117 40 66% 119-121, 138-173, 205-208, 241-250, 256, 273-278 th_twitter/tests.py 80 0 100% Mock (2 or more functions) FunctionB and FunctionC in a FunctionA Let's suppose you want to mock several functions in save_data , we will do something like this : from unittest.mock import patch # be careful with the order of the decorator @patch . object ( Twython , 'update_status' ) # will go to mock2 @patch . object ( AnotherService , 'other_method' ) # will go to mock1 def test_save_data ( self , mock1 , mock2 ): self . create_twitter () token = self . token trigger_id = self . trigger_id content = 'foobar #tag' self . data [ 'title' ] = 'a title' self . data [ 'link' ] = 'http://domain.ltd' self . assertTrue ( token ) self . assertTrue ( isinstance ( trigger_id , int )) self . assertIn ( 'text' , self . data ) self . assertNotEqual ( self . data [ 'text' ], '' ) se = ServiceTwitter ( self . token ) se . save_data ( trigger_id , ** self . data ) mock1 . assert_called_once_with () mock2 . assert_called_once_with ( status = content ) /!\\ Here, be really very carefull with the order of the decorator: if the parms for Twython.update_status and AnotherService.other_method are the same, we can write twice the same like mock1.assert_called_once(status=content) and mock2.assert_called_once(status=content) (for example) but if they don't, be sure to set the right parm to the right 'mock' End Hope this will be helpful like it was for me as I spent a lot of time to find and test that ;) If you want to dig that topic have a look at the doc","tags":"Techno","url":"https://foxmask.net/post/2016/10/21/test-coverage-mock/","loc":"https://foxmask.net/post/2016/10/21/test-coverage-mock/"},{"title":"Revue Technique de la semaine du 05/09/2016","text":"Cela faisait un bon moment que je ne m'etais pas penché sur une revue technique, depuis ma migration Wordpress => Pelican à vrai dire :) Donc voici un petit tour de la technosphere des sujets, projets (et autre bidules machins trucs) croisés sur le chemin Wallabag 2.0.8 Wallabag nous gratifie d'une version 2.0.8 avant une potentielle 2.1 Sam et Max 'ils' sont de retour avec de nouveaux billets dont le dernier aborde UUID : Vérifier qu'un UUID est valide en Python Django ... et une nouvelle release une version 1.10.1 contentant quelques bugfix ... et Docker Dockerizing a Python Django Web Application ... et le framework message Django Tips #14 Using the Messages Framework Django Tips #13 F() Expressions ... et sa config gunicorn Deploying Django with Gunicorn and Supervisor Billet que j'ai trouvé trop trop trop court comparé à un de mes vieux billets Python La couverture de ce livre Python 201 , m'a fait penser à un coaching culinaire par Ratatouille :) N'en demeure pas moins qu'il existe ... pour les dev intermediaires. 7 trucs pour ecrire du code plus meilleur ;) 7 Simple #Tricks to Write Better #Python Code https://t.co/uqEN50U7nI pic.twitter.com/dk2NDVVhZd — Nicolas RAMY (@darkelda) 30 août 2016 IndexError Comment pourquoi que ca marche-t-il (pas) mon code, S.O.S d'un codeur en détresse : voici les 5 derniers sujets traités ou en cours sur IndexError.net Facebook API Ajouter des données aux fields d'un form Architecture : faut-il limiter les methodes accessibles d'un objet ? Comment trier les logs de ses différentes applis ? decorateur async await vos types customisés utiles Meetup La rentrée scolaire annonce son lot de rencontres, dont une le 13/09 : Paris Devops Firefox On nous promettrait un Firefox 7x plus rapide que l'actuel grâce au multiprocessing, qui arriverait entre la version 49 et la 51. On pourrait déjà testé la fonction en l'activant depuis \"about:config\" en changeant le paramètre \"browser.tabs.remote.autostart\" à True. Un temps sur mobile, le multiprocessing a été retiré car produisant un bottleneck, mais avec cette amélioration sur la version desktop, le mutliprocessing sur la version sur mobile pourrait faire son retour. Smartphone Coté smartphones, pendant que Samsumg met le feu , Apple se mouille 30minutes 'max' et nous fait des écouteurs sans fils et Google rebaptise sa gamme Nexus en Pixel pour faire moins geek et plus grand publique. Ce n'est qu'un au revoir Un dinosaure qui disparait d'internet, Readability ferme ses portes le 30 de ce mois, à vos API, à vos migrations de service, de préférence vers Wallabag ;) C U A bientôt !","tags":"Techno","url":"https://foxmask.net/post/2016/09/08/revue-technique-de-la-semaine-du-05092016/","loc":"https://foxmask.net/post/2016/09/08/revue-technique-de-la-semaine-du-05092016/"},{"title":"PyCharm Pro for Free","text":"Hi, Here is a very short short short post to explain how to get the PyCharm Pro edition and feel like at home. And ... this is completely legal I suppose you use PyCharm Community edition to write your code in Python / Django, but now it is not enough for you. Pre requisites : you participate to a opensource project as core dev or are a leader of one Project must be at least 3 months old Project is in active development Project does not provide commercial service if it's your case : download the pro version then request your licence by selecting apply under 'For Regular Projects' Once you get confirmation by mail, create an account on jetbrains to activate the licence. At the cherry on the cake : this licence works for ALL THE IDE of jetBrains, even RubyMine and PhpStorm if you want. for example, you download RubyMine, and at the licence request, fill it with your account and go ;)","tags":"Techno","url":"https://foxmask.net/post/2016/09/01/pycharm-pro-for-free/","loc":"https://foxmask.net/post/2016/09/01/pycharm-pro-for-free/"},{"title":"Django et HTTPS","text":"Un rapide billet pour poser ici un retour de prise de tête :) Avec la venue de LetsEncrypt , il va fleurer bon les sites en HTTPS de ci de là. Par contre une fois mis en place en prod, coté dev il arrive qu'on fasse un hack vite fait pour pas passer 3 plombes sur sa conf nginx comme ça : export HTTPS = on ./manage.py runserver et roule ma poule ! Bon en prod par contre on ne va pas s'amuser à ca quand on a un NGINX en frontal du Gunicorn par exemple. Et si on laisse en etat, au final tout accès à request.scheme vous retournera dans les dents un casse-burette 'http'. Donc en suivant la doc on modifie son settings en rajoutant SECURE_PROXY_SSL_HEADER = ( 'HTTP_X_FORWARDED_PROTO' , 'https' ) mais ça ne suffit toujours pas. Nginx renverra encore et toujours du HTTP... donc à sa conf toute QQ location / { add_header Front-End-Https on; add_header Cache-Control \"public, must-revalidate\"; add_header Strict-Transport-Security \"max-age = 2592000; includeSubdomains\"; proxy_pass http://127.0.0.1:8080; # Pass to Gunicorn proxy_next_upstream error timeout invalid_header; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; } on rajoutera proxy_set_header X-Forwarded-Proto $scheme; pour obtenir : location / { add_header Front-End-Https on; add_header Cache-Control \"public, must-revalidate\"; add_header Strict-Transport-Security \"max-age = 2592000; includeSubdomains\"; proxy_pass http://127.0.0.1:8080; # Pass to Gunicorn proxy_next_upstream error timeout invalid_header; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Proto $scheme; } Et là au joie, on obtiendra bien un request.scheme vallant 'https'.","tags":"Techno","url":"https://foxmask.net/post/2016/07/03/django-et-https/","loc":"https://foxmask.net/post/2016/07/03/django-et-https/"},{"title":"Evernote de pire en pire","text":"It's not a bug it's a feature Jusqu'alors avec evenote on pouvait, via l'interface web supprimer plusieurs notes à la fois. Mais avec la nouvelle interface ce n'est plus possible.... c'est pas un bug c'est une feature ! Et sur l'application Android on ne pouvait pas supprimer plusieurs notes à la fois, juste une par une. C'était chiant à souhait mais avec le coup de main c'était supportable puisque un menu contextuel apparaissait tout mimi pour accomplir son office. À présent les temps changent : on peut faire une suppression de plusieurs notes d'un coup ! C'est pas beau ça ? Eh bien non c'est pas beau mais alors pas du tout ! Pourquoi ?! Parce qu'il faut s'y reprendre à plusieurs fois pour que la suppression soit effective ! c'est pas un bug c'est une feature ! Alors autant sur l'interface web on peut reprendre la vieille interface moins \"flat design\" que la nouvelle, mais qui fonctionne, autant sur Android on est cuit et là ça moi ça passe plus. Le comique de répétition ça le fait pas. Quand j'ai 200 notes à virer parce que lu ma veille techno du jour... Ils feraient bien de prendre des cours chez pocket qui fait ça les doigts dans le nez sur une quantité de notes illimitée, là où evenote n'en permet que 20 d'un coup. And so ? Et donc, du coup me revoilà à la recherche d'une application équivalente avec : - Client Android - Client Web - Le tout avec gestion des notes offline et optionnellement, possibilité de créer des \"rappels\" Et tant pis pour l'OCR. Et pas de self-hosted, les projets testés sont pas glop ,ou trop lent ou sans API exploitable ou mal finie :-/","tags":"Techno","url":"https://foxmask.net/post/2016/06/02/evernote-de-pire-en-pire/","loc":"https://foxmask.net/post/2016/06/02/evernote-de-pire-en-pire/"},{"title":"Apparition subliminiale de TriggerHappy au PHPTour 2016 avec Wallabag","text":"Le mois dernier s'est déroulé le PHPTour 2016 à Clermont Ferrand. Lors de cet évènement, TriggerHappy fit une apparation subliminale ;) lors de la présentation, par le project wallabag , de la migration version Symfony 3. La présentation de @nicosomb et de son binôme @j0k est disponible ici . Et \"l'apparition\" ici . Mais ... je vous vois venir faire votre \"Géronte\" et me sortir \"Mais que diable allait il faire dans cette galère\" (genre : \"mélanger des projets PHP et Python mais quelle hérésie\") mais en fait, ce n'est pas du tout le cas. Wallabag dans sa mouture 2.0, pourvue de son API, laisse toute latitude et on a franchement plaisir à y mettre ses découvertes au gré du buttinage sur la toie ou à faire sa veille techno en recupérant régulièrement les infos de ses sites favoris, spécialisés. En plus, ce qui ne gâte rien, c'est l'existence de plugin pour Firefox et d'une application Android. Ca en fait un bon projet bien complet à mon sens, qui surpasse le classique troll de PHP vs Python. Alors \"ayez confiaaaance\" et laissez vous tenter par les 2 :)","tags":"Techno","url":"https://foxmask.net/post/2016/06/01/trigger-happy-apparition-subliminale-phptour-2016-wallabag/","loc":"https://foxmask.net/post/2016/06/01/trigger-happy-apparition-subliminale-phptour-2016-wallabag/"},{"title":"Panda, Le gros nounours noir et blanc est passé","text":"En lisant ma pile d'articles de veille techno dans mon instance wallabag , je suis tombé sur un article mesurant le temps passé sur ses projets. Rien de nouveau au soleil puisque tout à chacun connait bien pandas :) Cependant, après un petit coup de pip install git-pandas et en exécutant ce script panda on peut donc mesurer le temps passé directos sur ses dépots, et donc pour bibi, il en ressort ceci $ python hours_estimate.py | grep -v 0 .00000 committer hours repository 0 foxmask 2 .603889 django-th 3 Olivier Demah 134 .288056 django-th 4 FoxMaSk 3 .017500 django-th 0 Olivier Demah 3 .000000 django-th-ansible 0 Olivier Demah 3 .877222 wallabag_api 0 Olivier Demah 8 .882778 dj-diabetes 1 foxmask 1 .000000 dj-diabetes En tout 140heures pour Trigger Happy. Si je pars sur des journées de 6heures ca fait 23jours ou 46jours en codant \"à temps perdu\", en gros un bon mois et demi non stop quoi. 4hres pour l'API wallabag 9hres pour le projet de \" gestion de son diabete au quotidien \" Pas de quoi fouetter un chat, mais ca donne une idée du temps que j'ai dépensé ;) Evidement la dedans on ne peut pas quantifier les durées pour le temps passé à tester moult techno pour arriver au resultat escompté. Mais ça donne un ordre de grandeur !","tags":"Techno","url":"https://foxmask.net/post/2016/05/14/le-gros-nounours-noir-et-blanc-est-passe/","loc":"https://foxmask.net/post/2016/05/14/le-gros-nounours-noir-et-blanc-est-passe/"},{"title":"A la recherche d'une alternative opensource d'Evernote","text":"En ce samedi (relativement) ensoleillé, j'ai demandé par ci par là si chacun connaissait une alternative opensource à Evernote fournissant un client Android, et cerise sur le gateau avec une API (pour évidemment passer les notes de Evernote à la futures solutions choisies) Le but étant d'avoir l'appli à portée de main facilement. Voici les réponses obtenues : solution Langage API Client Android Laverna JavaScript non non Paperwork PHP oui oui ownCloud PHP n/a oui Turtl JavaScript oui oui Simplenote Voici mon retour : Laverna semble fun (facile à installer, modulo le choix du \"storage\") mais pas de client android ni api ownCloud est \"too much\" pour juste un outil de prise de notes (entre autre) Turtl est sympa mais la doc de l'API ... se résume à une page qui me laisse sans voix https://turtl.it/docs/server/ simplenote n'est pas opensource Le choix ... mais : paperwork : J'ai facilement ( à partir de cette page ) pu installer, mais quand j'ai mis PostgreSQL en guise de RDBMS ... boom Du coup je suis le bec dans l'eau :P C'est beau l'opensource hein à se triturer les méninges pour se libérer de nos carcans ;) Au moins ça occupe :D","tags":"Techno","url":"https://foxmask.net/post/2016/05/07/evernote-opensource-alternative/","loc":"https://foxmask.net/post/2016/05/07/evernote-opensource-alternative/"},{"title":"Wallabag API 1.0.1","text":"Intro This API is like a story of old friends for me, destinies that cross and intersect. Something like 10 years ago, I met @nicosomb on http://punbb.fr, a french community arround PunBB, where I was administrator. After all that years, Nicolas made the PHP opensource project named \"poche\" at first, which became Wallabag , when on my side, after years, I stopped participating on Jelix, a PHP5 framework , for Python. Then, when I started TriggerHappy and could integrate Pocket successfully, I asked to Nicolas If he planed to make an API that I then could integrate too... That was 2 years ago ;) Today, Wallabag is now in version 2 and the API is ready. Thanks to him and to his wonderful team. So I finally could finish the Python API on my side too. And now there is no more barrier to each of us to host our own Wallabag and TriggerHappy instance for our own pleasure ;) How to create a post in wallabag ? Here is a snipset to create a entry in your wallabag account : from wallabag_api.wallabag import Wallabag # settings params = { 'username' : 'foxmask' , 'password' : 'mypass' , 'client_id' : 'myid' , 'client_secret' : 'mysecret' } my_host = 'http://localhost:8080' # get token token = Wallabag . get_token ( host = my_host , ** params ) # create a post wall = Wallabag ( host = my_host , client_secret = 'mysecret' , client_id = 'myid' , token = token ) my_url = 'https://blog.trigger-happy.eu' my_title = 'Trigger Happy blog' my_tags = [ 'python' , 'wallabag' ] wall . post_entries ( url = my_url , title = my_title , tags = my_tags ) this will give you something like this","tags":"Techno","url":"https://foxmask.net/post/2016/04/22/wallabag-api-1.0.1/","loc":"https://foxmask.net/post/2016/04/22/wallabag-api-1.0.1/"},{"title":"Quand Les \"job queue\" et les amis \"brokers de messages\" sont mis au rancart par la stdlib","text":"Intro Ce post n'est autre qu'un simple retour d'XP sur l'utilisation et tests de diverses solutions (almost)ready-to-use de job queue et brokers de messages. Présentation de l'archi du projet Comme vous pouvez l'imaginer avec IFTTT si vous connaissez ce service, j'ai des triggers qui permettent de récupérer des données pour être publiées n'importe où. Des mécaniques, il ne faut pas rouler Pour que cela ne soit pas fait par des traitements en série et bloquant, j'exploite le cache de Django à coup de cache.set() et cache.get() avec 2 tâches récurrentes, une qui récupère les données et fait donc le cache.set() et une qui publie les données à partir des éléments récupérés par cache.get() Les \"solutions\" utilisées pour le traitement des tâches Depuis quelques semaines pour Trigger Happy, j'ai migré de \"Job Queue\", passant de Celery à Python-rq . Celery cessant de fonctionner sans raison apparente (aucune info dans les logs ni dans la console), de guerrelasse, je suis passé à RQ , plus léger et facile à manipuler. Mais il y a 10jours j'en ai eu marre de voir RQ prendre des fins de non recevoir du serveur redis à coup de \"connections timeout\" pour des choses \"futiles\" (par exemple une pauvre tâche changeant version=2 à version=1 de 3 ou 4 données seulement). Donc je suis parti creuser côté RQ ce qui pouvait poser problème, et, au hasard d'un ticket, je découvre qu'en fait... RQ ne gère qu' un worker à la fois et pas toute une batterie comme avec celery. Ce ticket datant de 2012 (ouais 4ans hein) d'autres se sont lancés dans des projets parallèles ( https://github.com/Koed00/django-q , https://github.com/pricingassistant/mrq ) pour remédier au problème. Je ne me suis pas amusé à tous les tester sinon j'y serai encore. Et surtout pour me faire installer mongodb en plus (le cas du projet mrq), j'ai trouvé ça trop lourd pour moi et ceux qui s'installeraient mon projet pour eux. Je suis donc parti dans un (bad)road trip \"broker / job queue\" pour tenter de trouver mon bonheur. Road Trip Broker / Job Queue Ce n'est pas la route 66 mais vous allez voir que ya matière... 1iere escale : activeMQ Pourquoi celui là ? Parce-que la lib python gérant ce dernier, nommée stomp.py , était limpide à mes yeux, (et si je parvenais à faire fonctionner la machinerie, ca permettrait que le code soit utilisable également pour RabbitMQ et Appolo ), mais après avoir fini mon code et testé avec succès, comme pour mongodb , faire installer activeMQ et son giga de RAM à consommer, je ne me voyais pas mettre ce prérequis à installer pour le projet pour vous autres. 2ieme escale : beanstalk Pourquoi celui là ? Parce-que rapide, m'a-t-on rapporté, et faisant exactement ce pour quoi il a été écrit. Comme mon projet est en python 3, la batterie de libs existantes ne convenait pas. Je me suis orienté sur https://github.com/jonasvp/django-beanstalkd , que j'ai forké pour switcher de beanstalkc à pystalkd, histoire de faire ma sorcellerie dans mon coin sans gêner personne ;) Manque de bol chez moi après avoir arrangé le code, même si la rapidité était au rendez-vous, au premier lancement : JOB_TOO_BIG Alors peut-être que ça venait de la lib cliente pystalkd (en python 3 je rappelle), mais je n'ai plus eu l'envie de creuser celle-ci. Du coup, après tout ça, il ne me restait plus rien comme solution existante, que je maîtrisais/connaissais ... Donc 3ieme escale : la stdlib ! Il y a quelques mois j'évoquais sur s&m le multiprocessing . Donc c'est tout ce qu'il me restait, \"quand faut y aller faut y aller\" me dis-je. Et au final ça n'a pas mal réussi puisque les temps de traitement sont passés de un peu moins d'une minute les 40 triggers à... 7sec (en moyenne haute ;) Tout cela grâce à 9 lignes de code :) trigger = TriggerService . objects . filter ( status = True , user__is_active = True ) . select_related ( 'consumer__name' , 'provider__name' ) from multiprocessing import Pool , TimeoutError try : with Pool ( processes = settings . DJANGO_TH [ 'processes' ]) as pool : result = pool . map_async ( publishing , trigger ) result . get ( timeout = 360 ) except TimeoutError as e : logger . warn ( e ) chez moi settings.DJANGO_TH['processes'] vaut 5. Je vous fais grâce de la fonction publishing ;) Publication des données : j'ai voulu charger la mule (toute proportion gardée:) en rajoutant 21 triggers de plus (entre autre traquer #django sur twitter), hé bien je suis passé à 12 secondes pour publier les données de 61 consumers (quand ceux ci ont des données à fournir évidemment) (donc ca fait un coup de 305triggers/min) quand rien n'est à faire, la liste est passée au crible en 1/10° de seconde... c'est plutôt ... correct :D Récupération des données : 10secondes pour les 61 triggers (ca fait un coup de 360triggers/min) Tentative d'amélioration des perfs Pour éviter des accès à la base de données à tout prix, j'ai tenté de n'exploiter que le cache. Du coup j'ai dû m'orienter vers apply_async vs map_async , mais les temps de traitements étaient pire que si je n'avais pas utilisé multiprocessing.Pool.apply_async() . Ensuite j'ai rajouté plus d'info dans le cache pour arriver à ne passer que par map_async() mais encore une fois le résultat n'était pas terrible. Du coup j'en suis resté à mes 9 lignes de code ci dessus. Nota Ici je n'ai pas cherché à dézinguer un projet plus qu'un autre, chacun fonctionnant dans un univers, au final, très différent du mien, j'ai juste voulu souligner, qu'à aller chercher des solutions toutes faites, ce n'était pas forcément le plus bénéfique, si on prenait le temps de se pencher sur ce qu'offrait déjà le langage. Par ailleurs, un truc qui ne transparait pas ici, c'est le temps que tout cela m'a pris pour tester et appréhender chaque solution de job queue / messages broker, de même que se pencher sur apply_async et map_async . Ceci explique pourquoi, à un moment donné, je n'approfondis plus mes investigations dans la recherche de bug dans pystakld par exemple sur le JOB_TOO_BIG pour continuer sur la voie beanstalkd . Last but not... toussa En l'état, si on veut un rafraichissement des données assez correct (toutes les 15minutes) alors le serveur est capable de gérer un petit contingent de ~3600triggers repartis sur 60 utilisateurs chacun ayant 60 triggers, lesquels seront engloutis en 10min. Ni trop De plus il ne faut pas non plus \"publier\" trop souvent, les accès aux API des services tiers comme Twitter, finiraient par vous envoyer aux pelotes avec un truc genre User Limit Reached . Ni trop peu A l'inverse il ne faut pas non plus publier trop \"rarement\" car là vous amassez une quantité de données importante dans le cache, que vous ne pourrez pas publier correctement, avec le même genre d'erreur que précédemment. Dernière analyse Sur les 60triggers que j'ai défini, il y en a entre 3 et 11 qui sont utilisés de façon recurrente, par exemple ceux qui suivent des hashtags sur twitter ou suivent simplement un @compte_twitter, ou les gros sites de news. Du coup ca fait pas bézef sur les 60, mais 1) ca me suffit 2) ca marche parfaitement pour mon besoin. Voici tout de même quelques logs à se mettre sous la dent pour voir ce que donne les temps de réponses. toutes les 15min, la publication a lieu toutes les 13min, la récupération des données a lieu ne sont pas affichées, les logs qui n'ont pas de données à traiter, mais qui évidement, consomme du temps. Ce qui explique que de 14:00:00 à 14:00:06 il ne s'affiche rien. 2016 -04-13 13 :26:02,928 INFO tasks 3902 foxmask - ServiceRss - ServicePocket - Frandroid - pocket - 1 new data 2016 -04-13 13 :26:06,156 INFO tasks 3903 foxmask - ServiceRss - ServiceEvernote - LinuxFr - 1 new data 2016 -04-13 13 :26:06,802 INFO tasks 3902 foxmask - ServiceTwitter - ServicePocket - Django - 2 new data 2016 -04-13 13 :26:07,137 INFO tasks 3906 foxmask - ServiceTwitter - ServiceTrello - Django - 2 new data 2016 -04-13 13 :26:08,504 INFO tasks 3903 foxmask - ServiceTwitter - ServiceReadability - Django - 2 new data 2016 -04-13 13 :30:03,616 INFO tasks 3946 foxmask - ServiceRss - ServicePocket - Frandroid - pocket - 1 new data 2016 -04-13 13 :30:04,152 INFO tasks 3949 foxmask - ServiceTwitter - ServicePocket - Django - 2 new data 2016 -04-13 13 :30:04,931 INFO tasks 3947 foxmask - ServiceTwitter - ServiceReadability - Django - 2 new data 2016 -04-13 13 :30:07,977 INFO tasks 3948 foxmask - ServiceRss - ServiceEvernote - LinuxFr - 1 new data 2016 -04-13 13 :30:09,572 INFO tasks 3950 foxmask - ServiceTwitter - ServiceTrello - Django - 2 new data 2016 -04-13 13 :39:05,783 INFO tasks 4348 foxmask - ServiceRss - ServicePocket - TheVerge - 1 new data 2016 -04-13 13 :42:02,482 INFO tasks 4383 recycle of cache done ! 2016 -04-13 13 :52:03,933 INFO tasks 4462 foxmask - ServiceRss - ServicePocket - TheVerge - 1 new data 2016 -04-13 13 :52:06,910 INFO tasks 4462 foxmask - ServiceTwitter - ServiceTrello - Django - 2 new data 2016 -04-13 13 :52:07,632 INFO tasks 4461 foxmask - ServiceRss - ServiceReadability - Numerama - 1 new data 2016 -04-13 13 :52:07,993 INFO tasks 4463 foxmask - ServiceTwitter - ServicePocket - Django - 2 new data 2016 -04-13 13 :52:09,537 INFO tasks 4463 foxmask - ServiceTwitter - ServiceReadability - Django - 2 new data 2016 -04-13 14 :00:06,192 INFO tasks 4524 recycle of cache done ! 2016 -04-13 14 :00:06,309 INFO tasks 4539 foxmask - ServiceRss - ServicePocket - TheVerge - 1 new data 2016 -04-13 14 :00:06,788 INFO tasks 4540 foxmask - ServiceTwitter - ServicePocket - Django - 2 new data 2016 -04-13 14 :00:07,321 INFO tasks 4536 foxmask - ServiceRss - ServicePocket - TheVerge - 1 new data 2016 -04-13 14 :00:07,497 INFO tasks 4538 foxmask - ServiceRss - ServiceReadability - Numerama - 1 new data 2016 -04-13 14 :00:07,751 INFO tasks 4535 foxmask - ServiceTwitter - ServiceReadability - Django - 2 new data 2016 -04-13 14 :00:10,278 INFO tasks 4536 foxmask - ServiceTwitter - ServiceTrello - Django - 5 new data 2016 -04-13 14 :00:10,875 INFO tasks 4542 foxmask - ServiceRss - ServiceEvernote - Korben - 1 new data 2016 -04-13 14 :00:11,279 INFO tasks 4536 foxmask - ServiceRss - ServiceEvernote - BeGeek - 1 new data 2016 -04-13 14 :00:11,461 INFO tasks 4536 foxmask - ServiceRss - ServiceEvernote - Google high-tech - 2 new data 2016 -04-13 14 :00:11,717 INFO tasks 4543 foxmask - ServiceTwitter - ServiceTrello - Django - 2 new data 2016 -04-13 14 :00:11,793 INFO tasks 4542 foxmask - ServiceTwitter - ServicePocket - Django - 5 new data 2016 -04-13 14 :00:12,634 INFO tasks 4536 foxmask - ServiceTwitter - ServiceReadability - Django - 5 new data 2016 -04-13 14 :00:14,414 INFO tasks 4537 foxmask - ServiceRss - ServiceReadability - Korden - 1 new data 2016 -04-13 14 :00:15,533 INFO tasks 4534 foxmask - ServiceRss - ServiceReadability - Numerama - 1 new data 2016 -04-13 14 :00:16,498 INFO tasks 4543 foxmask - ServiceRss - ServiceEvernote - BeGeek - 1 new data 2016 -04-13 14 :00:17,206 INFO tasks 4542 foxmask - ServiceRss - ServiceReadability - BeGeek - 1 new data Conclusion Cette fois ci l'éternel insatisfait que je suis, se sent un peu mieux avec cette solution (... quoi que ya encore un truc à gratter pour que ca soit 'encore mieux';)","tags":"Techno","url":"https://foxmask.net/post/2016/04/14/job-queue-and-messages-broker-out/","loc":"https://foxmask.net/post/2016/04/14/job-queue-and-messages-broker-out/"},{"title":"ActionForm Django à la rescousse !","text":"Un besoin se fit ressentir today : Pouvoir, depuis la page d'administration de django, réaffecter des données se trouvant dans un service et les mettre dans un autre. Comme le montre la doc , on peut effectuer des actions à sa sauce comme ceci : from django.contrib import admin from myapp.models import Article def make_published ( modeladmin , request , queryset ): queryset . update ( status = 'p' ) make_published . short_description = \"Mark selected stories as published\" class ArticleAdmin ( admin . ModelAdmin ): list_display = [ 'title' , 'status' ] ordering = [ 'title' ] actions = [ make_published ] admin . site . register ( Article , ArticleAdmin ) Arrive le moment où c'est sympa, mais ceci ne suffit plus. Si j'ai à ventiler des données à partir de données se trouvant dans un autre modèle, je vais pas créer 'n' make_published_x,y,z . Et le DRY là dedans, hmmm? :P Pour arriver à ses fins, un helper existe tout de même, il s'agit d' ActionForm . Dans mon module admin.py j'y mettrai donc un truc du genre class ServicesActivatedActionForm ( ActionForm ): provider = forms . ChoiceField ( choices = ServicesActivated . objects . values_list ( 'id' , 'name' )) consumer = forms . ChoiceField ( choices = ServicesActivated . objects . values_list ( 'id' , 'name' )) on notera en passant l'astuce pour fournir à choices un tuple qui provient du modèle, grâce à l'utilisation de values_list() Puis dans le ModelAdmin je glisse : class TriggerServiceAdmin ( admin . ModelAdmin ): action_form = ServicesActivatedActionForm Ce qui aura pour effet, d'afficher à coté de la liste déroulante des actions, une liste déroulante des mes provider/consumer. En l'état ça ne suffit pas pour fonctionner complètement. Il faut évidement gérer la validation du choix du provider/consumer comme suit : def change_service ( self , request , queryset ): provider = request . POST [ 'provider' ] consumer = request . POST [ 'consumer' ] queryset . update ( provider = provider , consumer = consumer ) change_service . short_description = 'Change of Service' A présent, ma liste d'actions contient 2 actions, la suppression (action par defaut proposer par l'admin) et la mienne. Tout ça à gauche de ma liste déroulante des provider/consumer ! Le code complet à présent donne : from django.contrib import admin from django.contrib.admin.helpers import ActionForm from django import forms from django_th.models import TriggerService class ServicesActivatedActionForm ( ActionForm ): provider = forms . ChoiceField ( choices = ServicesActivated . objects . values_list ( 'id' , 'name' )) consumer = forms . ChoiceField ( choices = ServicesActivated . objects . values_list ( 'id' , 'name' )) class TriggerServiceAdmin ( admin . ModelAdmin ): list_display = ( 'user' , 'provider' , 'consumer' , 'description' , 'date_created' , 'date_triggered' , 'status' ) list_filter = [ 'user' , 'provider' , 'consumer' , 'status' ] action_form = ServicesActivatedActionForm def change_service ( self , request , queryset ): provider = request . POST [ 'provider' ] consumer = request . POST [ 'consumer' ] queryset . update ( provider = provider , consumer = consumer ) change_service . short_description = 'Change of Service' actions = [ change_service ] admin . site . register ( TriggerService , TriggerServiceAdmin ) Voilou pour le tips du jour ;) Edit du 22/09/2016 : Avec ce form, on peut avoir un soucis qui ne saute pas aux yeux, de prime abord, quand les données ne varient pas souvent class ServicesActivatedActionForm ( ActionForm ): provider = forms . ChoiceField ( choices = ServicesActivated . objects . values_list ( 'id' , 'name' )) consumer = forms . ChoiceField ( choices = ServicesActivated . objects . values_list ( 'id' , 'name' )) Mais si votre liste déroulante contient des catégories que vous mettez à jour, ajoutez, retirez regulierement, on \"remarque\" que les données ne sont \"rafraîchies\" correctement dans la liste déroulante. Par exemple, si j'ajoute une catégorie, elle ne s'affiche pas immédiatement dans la vue, et inversement si je retire une catégorie de la base, elle reste encore présente dans la liste déroulante de ma vue. Pour corriger cela on passe par un form simple : class ServicesActivatedActionForm ( ActionForm ): provider = forms . ModelChoiceField ( queryset = ServicesActivated . objects . all ()) consumer = forms . ModelChoiceField ( queryset = ServicesActivated . objects . all ())","tags":"Techno","url":"https://foxmask.net/post/2016/03/23/actionform-django-a-la-rescousse/","loc":"https://foxmask.net/post/2016/03/23/actionform-django-a-la-rescousse/"},{"title":"Le developpeur, cet artiste qui s'ignore !","text":"Intro : ce billet se résume bien en une réplique célèbre : \"N'y voyez pas le fantasme de l'homme, mais le délire de l'artiste !\" And so : Avec TriggerHappy , je passe par plein de phases et de découvertes techniques/technologiques. Dernièrement, je me suis dit \"bon les tests unitaires c'est sympa, mais si je blindais un poil plus tout ça ? Et fais-je cela ca correctement ?\". Et me voilà parti à utiliser coverage.py , pas évident au départ mais rudement éfficace à la longue. J'avoue que j'aurai pu me contenter de ce que je fais déjà, sans me décarcasser, mais je suis un peu comme ce 'dicton' : Vingt fois sur le métier, remettez votre ouvrage D'ailleurs l' auteur , poète de son état (au XVII°/XVIII° siècle), a dû inspirer le sieur Guido, si on reprend ce 'dicton' dans son poème originel : Avant donc que d'écrire, apprenez à penser. Selon que notre idée est plus ou moins obscure, L'expression la suit, ou moins nette, ou plus pure. Ce que l'on conçoit bien s'énonce clairement, Et les mots pour le dire arrivent aisément. tu le vois le \"The Zen Of Python\" là ? :) atta c'pas fini : Hâtez-vous lentement, et sans perdre courage, Vingt fois sur le métier remettez votre ouvrage, Polissez-le sans cesse, et le repolissez, Ajoutez quelquefois, et souvent effacez. Alors ça colle pas à ce qu'on fait tous les jours ? :) Bon sur ces drôles de loufoqueries, retournons au couvrage de mon ouvrage justement ! Bon weekend ;)","tags":"Techno","url":"https://foxmask.net/post/2016/03/18/le-dev-cet-artiste-qui-signore/","loc":"https://foxmask.net/post/2016/03/18/le-dev-cet-artiste-qui-signore/"},{"title":"Intégration continue de Github à Pypi via Travis-CI","text":"L'an passé, lors de la release 0.11.0 de TriggerHappy , je n'étais pas parvenu à faire fonctionner Travis-CI comme je l'escomptais, pour qu'il publie tout seul cette version sur PyPi . Du coup je m'étais bien pris la tête pour préparer le terrain pour la version suivante. Et cette nuit, lors de la sortie de la 0.12.0 j'étais zo zanges :) Donc \"Comment ça marche la fusée ariane ?\" dirait Michel Chevalet Tout simplement en 2 temps : 1) le fichier .travis.yml sur son repository github deploy : provider : pypi user : votre_login_pypi password : secure : le_mot_de_passe_crypte on : tags : true ici je ne vous ai mis que la tâche deploy chargée de s'occuper de l'installation de vos sources sur pypi sous la forme d'une archive, en respectant le nommage défini dans votre setup.py 2) sur Travis-CI, détection de l'application d'un tag sur le projet, via le déclencheur on: tags: true , et enchaînement de la tâche deploy C'est tout QQ et tout simple comme on aime et, évidemment très efficace !","tags":"Techno","url":"https://foxmask.net/post/2016/03/04/integration-continue-de-github-a-pypi-via-travis-ci/","loc":"https://foxmask.net/post/2016/03/04/integration-continue-de-github-a-pypi-via-travis-ci/"},{"title":"PyCharm running Flake8","text":"The Ugly As I like clean things, I like when python tells me that I made ugly things to be able to fix them. With this in mind, first of all, I used the service CodeClimate . This one permits to tell you where your code can be improved and how. But the service enters in the game, once the commits are done and pushed on the repository. As I try to keep the repository as clean as possible, I dont like that : test improvements, commit and push them, get the result from codeclimate, do anoither improvement and so on... The Beauty So I decide to stop that and ask python to tell me where the code need to be improved before any commit. To do so, as I use PyCharm 5.1 EAP , since a few days, I searched a way to make PyCharm detects and suggests improvements from the opened files. The code inspection works fine with PEP8, the result of the inspection is automatically displayed in the right margin of the opened file, but it seems there is nothing for Flake8 and McCabe. So here is what I did : in my virtualenv pip install flake8 flake -- version 2.5 . 4 ( pep8 : * 7.0 , pyflakes : * 0.0 , mccabe : 0.4 . 0 ) CPython 3.4 . 2 on Linux in PyCharm, go to File > Settings > External Tools > click on \"+\" and fill the fields as below : once it's done you now have an option in the menu External Tools \"Flake8\" then run it and see in the bottom of the PyCharm window with the \"console\" to see what's wrong (or not) with the module you've selected if you prefer to run flake8 on the entire folder, change : -- max - complexity 10 $ FileDir $ / $ FileName $ by -- max - complexity 10 $ FilePath $ Caveats : May be I went too far and something exists and is much better than this one. For example, now, if you switch of project, the previous settings remain with the path of the virtualenv we've setup. And if you change the path for the project B, and open project A, the modification from B will remain for A... As settings for Project do not include this parameter yet, but just \"interpreter\" and \"structure\", we are a little bit embarrassed. Conclusion : At least now I'm ready to (try to) make better things ;)","tags":"Techno","url":"https://foxmask.net/post/2016/02/17/pycharm-running-flake8/","loc":"https://foxmask.net/post/2016/02/17/pycharm-running-flake8/"},{"title":"Trigger Happy running softly with RQ","text":"TriggerHappy and Celery Actually, Trigger Happy works perlfectly with Celery , but.... there is a but. Sometimes, (once a week) celery stays stuck without any visible reason. No error logs occur, and even in DEBUG level, no log appear at all. The logfile remain as is (and yes the disk is not full:) As this is very annoying for me, I went on #celery@freenode to try to get some details about this behavior, before breaking everything ... Here, we told me that this behavior was also met with AMQ as broker (when I currently use REDIS). TriggerHappy and RQ So I decide to give a try to Python-RQ His (well known) author presents it as follow : Python RQ (Redis Queue) is a simple Python library for queueing jobs and processing them in the background with workers This project has been inspired by the good parts of Celery, So to integrate RQ with TriggerHappy here is what I should do : I installed django-rq that I added to INSTALLED_APPS in the settings.py file. to not break the existing way (with Celery) that (almost) works perfectly, I changed few details in the tasks.py module : before from celery import shared_task @shared_task def reading ( service ): [ ... ] @shared_task def publishing ( service ): [ ... ] after try : from django_rq import job except ImportError : from celery import shared_task [ ... ] @job ( 'default' ) @shared_task def reading ( service ): [ ... ] @job ( 'high' ) @shared_task def publishing ( service ): [ ... ] Those tasks are executed by celery with the decorator @shared_task (with its own scheduler) or by the RQ with the @job decorator, through some crontab tasks, with management commands. As RQ asole use my_tasks.delay(), I dont have to change anything else, other than just adding a @job decorator. I perfectly know that it's not satifying because if Celery or RQ is not installed, the decorator call with fail and the tasks wont happen. It's just for testing purpose :) That way, I'm able to compare performance between the two solutions, one weight and one very light. Surprisingly, the weight win by handling my reading tasks in less than a seconde for ~50triggers against 30sec for the light one. For the publishing tasks, they are neck and neck. But one thing hangs in the balance actually against celery, it is out of order once a week. The process are not dead, the log does not move, the query I made with celery inspect and so on, respond perfectly. As my project is not so overloaded I can say that the cursor between something a \"little bit slow\" and something \"out of order\" is quickly choosen :) So at least if you want to give a try to RQ with TriggerHappy, the stuff that will need to be handle, will continue to work. Here I provide the line I enter in my crontab : */12 * * * * . /my/env/bin/activate && cd /my/env/th/ && ./manage.py fire_read_data && ../bin/rqworker-default-burst.sh */15 * * * * . /my/env/bin/activate && cd /my/env/th/ && ./manage.py fire_publish_data && ../bin/rqworker-high-burst.sh */20 * * * * . /my/env/bin/activate && cd /my/env/th/ && ./manage.py fire_get_outside_data && ../bin/rqworker-low-burst.sh and the content of, for example, rqworker-default-burst.sh #!/bin/bash python manage.py rqworker default --burst & python manage.py rqworker default --burst & python manage.py rqworker default --burst & python manage.py rqworker default --burst & python manage.py rqworker default --burst & python manage.py rqworker default --burst & python manage.py rqworker default --burst & python manage.py rqworker default --burst & python manage.py rqworker default --burst & python manage.py rqworker default --burst & python manage.py rqworker default --burst & Yes I like to spawn a lot :) A Question to finish I take the opportunity of this post to ask you a question : If I want to support Celery and RQ for my tasks, one should I write a decorator that could handle each of them. Is there a way to write a generic decorator that will permit to be inherited to handle each of them ? Thus in my tasks.py I'll be able to just do from django_th.my_powerful_decorator import jobth [ ... ] @jobth def reading (): [ ... ] and jobth will check which of the two Queueing system is here and handled the right decorator","tags":"Techno","url":"https://foxmask.net/post/2016/02/15/trigger-happy-running-softly-with-rq/","loc":"https://foxmask.net/post/2016/02/15/trigger-happy-running-softly-with-rq/"},{"title":"History not to lose the thread : TrackingFields","text":"Introduction this post is an english version [of this post made on sam et max made last week. The goal of that post will be to show how, without changing anything in a form, we can track the modifications of the data made in the application the first part will set the scene by starting to show you how articulates an application with a form, additionally, composed by a sub form. I will explain when later. To do so, I takes you into the world of the 7th art, come, we will redo StarWars! One model, one form, a view, a template and that will be finished the models.py from django.db import models class Movie ( models . Model ): \"\"\" Movie \"\"\" name = models . CharField ( max_length = 200 , unique = True ) description = models . CharField ( max_length = 200 ) def __str__ ( self ): return \" %s \" % self . name class Episode ( models . Model ): \"\"\" Episode - for Trilogy and So on ;) \"\"\" name = models . CharField ( max_length = 200 ) scenario = models . TextField () movie = models . ForeignKey ( Movie ) def __str__ ( self ): return \" %s \" % self . name the forms.py, very mini mini from django import forms from django.forms.models import inlineformset_factory from starwars.models import Movie , Episode class MovieForm ( forms . ModelForm ): class Meta : \"\"\" As I have to use : \"exclude\" or \"fields\" As I'm very lazy, I dont want to fill the list in the \"fields\" so I say that I just want to exclude ... nothing :P \"\"\" model = Movie exclude = [] # a formeset based on the model of the Mother \"Movie\" and Child \"Episode\" + 1 new empty lines EpisodeFormSet = inlineformset_factory ( Movie , Episode , fields = ( 'name' , 'scenario' ), extra = 1 ) the views.py very very very DRY :) from django.http import HttpResponseRedirect from django.core.urlresolvers import reverse from django.views.generic import CreateView , UpdateView , ListView from starwars.models import Movie from starwars.forms import MovieForm , EpisodeFormSet class MovieMixin ( object ): model = Movie form_class = MovieForm def get_context_data ( self , ** kw ): context = super ( MovieMixin , self ) . get_context_data ( ** kw ) if self . request . POST : context [ 'episode_form' ] = EpisodeFormSet ( self . request . POST ) else : context [ 'episode_form' ] = EpisodeFormSet ( instance = self . object ) return context def get_success_url ( self ): return reverse ( \"home\" ) def form_valid ( self , form ): formset = EpisodeFormSet (( self . request . POST or None ), instance = self . object ) if formset . is_valid (): self . object = form . save () formset . instance = self . object formset . save () return HttpResponseRedirect ( reverse ( 'home' )) class Movies ( ListView ): model = Movie context_object_name = \"movies\" template_name = \"base.html\" class MovieCreate ( MovieMixin , CreateView ): \"\"\" MovieMixin manage everything for me ... \"\"\" pass class MovieUpdate ( MovieMixin , UpdateView ): \"\"\" ... and I'm DRY :D \"\"\" pass To finish to set the scene and the costumes : the templates base.html <!DOCTYPE html> < html lang = \"fr\" > < head > < title > Manage stories for StarWars </ title > </ head > < body > < h1 > Stories Manager for Starwars </ h1 > {% block content %} < a href = \"{% url 'movie_create' %}\" > Add a movie </ a >< br /> < h2 > Movie list </ h2 > < ul > {% for movie in movies %} < li >< a href = \"{% url 'movie_edit' movie.id %}\" > {{ movie.name }} </ a ></ li > {% endfor %} </ ul > {% endblock %} </ body > </ html > movie_form.htlm (the template used by UpdateView & CreateView) {% extends \"base.html\" %} {% block content %} < form method = \"post\" action = \"\" > {% csrf_token %} {{ formset.management_form }} < table > {{ form.as_table }} </ table > < table > {{ episode_form.as_table }} </ table > < button > Save </ button > </ form > {% endblock %} Update of the database this is necessary : ( starwars ) foxmask@foxmask:~/DjangoVirtualEnv/starwars/starwars $ ./manage.py migrate Operations to perform: Synchronize unmigrated apps: messages, starwars, staticfiles Apply all migrations: contenttypes, admin, sessions, auth Synchronizing apps without migrations: Creating tables... Creating table starwars_movie Creating table starwars_episode Running deferred SQL... Installing custom SQL... Here we are, ready, I can now create my double Trilogy like Georges Lucas Tracking the ungodly But a day comes when me, George Lucas, I sell StarWars to Walt Disney, but I want to miss what they will do my \"baby\", I add a \"tracker changes\" in my application, not to lose the \"field\" of history. Installation of Tracking Fields as a prerequisites, if you want to also track who change thing, and not what data have been whanged, you will need django-current-user, so the pip command to use will be this one ( starwars ) foxmask @foxmask : ~/ DjangoVirtualEnv / starwars / starwars $ pip install django - tracking - fields django - cuser Collecting django - tracking - fields Downloading django - tracking - fields - 1.0 . 6. tar . gz ( 58 kB ) 100 % | ████████████████████████████████ | 61 kB 104 kB / s Collecting django - cuser Downloading django - cuser - 2014.9 . 28. tar . gz Requirement already satisfied ( use -- upgrade to upgrade ): Django >= 1.5 in / home / foxmask / DjangoVirtualEnv / starwars / lib / python3 . 5 / site - packages ( from django - cuser ) Installing collected packages : django - tracking - fields , django - cuser Running setup . py install for django - tracking - fields ... done Running setup . py install for django - cuser ... done Successfully installed django - cuser - 2014.9 . 28 django - tracking - fields - 1.0 . 6 the necessary changes in settings.py : INSTALLED_APPS = ( ... 'cuser' , 'tracking_fields' , ... ) MIDDLEWARE_CLASSES = ( 'django.contrib.sessions.middleware.SessionMiddleware' , 'django.middleware.common.CommonMiddleware' , 'django.middleware.csrf.CsrfViewMiddleware' , 'django.contrib.auth.middleware.AuthenticationMiddleware' , 'django.contrib.auth.middleware.SessionAuthenticationMiddleware' , 'django.contrib.messages.middleware.MessageMiddleware' , 'django.middleware.clickjacking.XFrameOptionsMiddleware' , 'django.middleware.security.SecurityMiddleware' , 'cuser.middleware.CuserMiddleware' , ## <=== do not forget to catch the badass who make change on my movies;) ) the little migrate that fit our needs, to add the tables for our modeles of django-tracking-fields ( starwars ) foxmask @foxmask : ~/ DjangoVirtualEnv / starwars / starwars $ ./ manage . py migrate Operations to perform : Synchronize unmigrated apps : staticfiles , messages , cuser , starwars Apply all migrations : auth , sessions , contenttypes , tracking_fields , admin Synchronizing apps without migrations : Creating tables ... Running deferred SQL ... Installing custom SQL ... Running migrations : Rendering model states ... DONE Applying tracking_fields . 0001 _initial ... OK Applying tracking_fields . 0002 _auto_20160203_1048 ... OK and here we are ready to play with the trackers Usage We cant dream a more simpler way to do, this can be summarize to a decorator on the model which identifies which data are changed, and on field ̀ histo which will link the model TrackingEvent of the application TrackingFields, to my tacle to watch. And here, even if my model has been changed with this new field, it's unecessary to do a new \"python manage.py migrate\", nothing will happen, because histo will be a GenericRelation() . Effectivly, TrackingEvent is based of ContenType aka En effet, TrackingEvent repose sur ContenType aka \"The contenttypes framework\". If you already played with the permission management, you should have already meet it before;) To make it short, this will give : models.py from django.db import models from django.contrib.contenttypes.fields import GenericRelation from tracking_fields.decorators import track from tracking_fields.models import TrackingEvent @track ( 'name' , 'description' ) class Movie ( models . Model ): \"\"\" Movie \"\"\" name = models . CharField ( max_length = 200 , unique = True ) description = models . CharField ( max_length = 200 ) histo = GenericRelation ( TrackingEvent , content_type_field = 'object_content_type' ) def episodes ( self ): return Episode . objects . filter ( movie = self ) def __str__ ( self ): return \" %s \" % self . name @track ( 'name' , 'scenario' ) class Episode ( models . Model ): \"\"\" Episode - for Trilogy and So on ;) \"\"\" name = models . CharField ( max_length = 200 ) scenario = models . TextField () movie = models . ForeignKey ( Movie ) histo = GenericRelation ( TrackingEvent , content_type_field = 'object_content_type' ) def __str__ ( self ): return \" %s \" % self . name so, here, it is very simple like a pancake recipe: 3 imports, the decorator, the GenericRelation, we mix all of them and that give what follow. I have, in the meantime, added a function episodes to my Movie class, I will explain it too later ;) the template of the expected DetailView < table > < caption > History of the modification of {{ object }} </ caption > < thead > < tr >< th > Old Value </ th >< th > New Value </ th >< th > By </ th >< th > at </ th ></ tr > </ thead > < tbody > {% for h in object.histo.all %} {% for f in h.fields.all %} < tr >< td > {{ f.old_value }} </ td >< td > {{ f.new_value }} </ td >< td > {{ h.user }} </ td >< td > {{ h.date }} </ td ></ tr > {% endfor %} {% endfor %} </ tbody > </ table > Now if I go the my page to modify the story of one episode, my template below, wont display thoses modifications ! But Why god ? Because until here, I just display the \"histo\" of Movie and not of the Episode. We now understand here my interest for the sub form. The issue Let's fix it this is here, that enter if the game, the function episodes of my Movie class to permit to loop on it and display of the needed stuff the template of the expected DetailView (again :) < table > < caption > History of the modifications of {{ object }} </ caption > < thead > < tr >< th > Old Value </ th >< th > New Value </ th >< th > By </ th >< th > at </ th ></ tr > </ thead > < tbody > {% for h in object.histo.all %} {% for f in h.fields.all %} < tr >< td > {{ f.old_value }} </ td >< td > {{ f.new_value }} </ td >< td > {{ h.user }} </ td >< td > {{ h.date }} </ td ></ tr > {% endfor %} {% endfor %} </ tbody > </ table > {% for ep in object.episodes %} {% if ep.histo.all %} < table > < caption > history of the modifications of Episode </ caption > < thead > < tr >< th > Old Value </ th >< th > New Value </ th >< th > By </ th >< th > at </ th ></ tr > </ thead > < tbody > {% for h in ep.histo.all %} {% for f in h.fields.all %} {% if f.old_value == f.new_value %} {# they are the same when the new value is created to avoid to display \"null\" #} {% else %} < tr >< td > {{ f.old_value }} </ td >< td > {{ f.new_value }} </ td >< td > {{ h.user }} </ td >< td > {{ h.date }} </ td ></ tr > {% endif %} {% endfor %} {% endfor %} </ tbody > </ table > {% endif %} {% endfor %} And voilà ! Voili voilou ! As a bonus, if you're curious, of the admin side, you also have a list of all the changes if needed;) For the advanced users who would say: why have recoded the front side since this is already managed on the admin side without lifting a finger? Because George Lucas wants to show changes to his baby StarWars by Walt Disney, to the world of course Ho, and a last detail : in the admin, the view which displays the list of changes gives : \"Episode Object\" or \"Movie Object\". To avoid that, you shoud have notice that I have added the function str in my model which will return a more readable value of what have been changed Conclusion : IRL, I didnt see myself, create a History model linked physically by a FK on each model, I decide to search, arround the web, some ressources It's finally on #django-fr@freenode that I asked my question and got from Gagaro the grââl : one application named tracking-fields , his author. For once I do my lazy by not coding all by myself, it's nice to come across such an app ! If you want to play with the code of this Movie manager here is the tasty soup","tags":"Techno","url":"https://foxmask.net/post/2016/02/13/history-not-to-lose-the-thread-trackingFields/","loc":"https://foxmask.net/post/2016/02/13/history-not-to-lose-the-thread-trackingFields/"},{"title":"PyCharm : raccourcis pour obtenir les comportements \"habituels\" comme SublimeText et FireFox","text":"Avec PyCharm : j'ai besoin de fermer des onglets ; les restaurer ; et naviguer entre eux. Par défaut les raccourcis de l'éditeur qui m'interessent sont : ALT-DROITE = passer à l'onglet suivant ALT-GAUCHE = passer à l'onglet précédent CTRL-F4 = fermer un onglet n/a = réouvrir un onglet fermé Donc pour ce faire on se rend dans le menu qui gere les raccourcis comme suit : File > Settings > Keymap Faire une recherche par raccourci (en cliquant sur la loupe au dessus de la pyramide de carrés) et appuyer sur la combinaison de touches ALT-DROITE . Le nom de l'action associée s'affiche, on double clique sur l'action pour editer le raccourci et on tape CTRL-Page Down . On répète l'action pour ALT-GAUCHE pour l'onglet précédent, et CTRL-F4 par CTRL-W pour fermer l'onglet. Pour finir, pour réouvrir un onglet fermé précédemment avec Firefox on fait CTRL-SHIFT-T . Pour trouver l'action qui permet cela, on fait une recherche de close pour retrouver reopened closed tab on double clique sur l'action, on tape CTRL-SHIFT-T et hop Il se peut qu'en cherchant à affecter ces raccourcis, vous voyiez que vous allez écraser un raccourci déjà existant, à vous de voir ;) Un dernier truc, pour que CTRL-SHIFT-T , marche quand tous les onglets sont fermés, il faut au moins UN tab ouvert... Si vous etes sur la page vide d'où vous pouvez lire Search Everywhere ...Drop files here from file manager ; ca ne marchera pas.","tags":"Techno","url":"https://foxmask.net/post/2016/02/10/pycharm-raccourcis-pour-obtenir-les-comportements-sublimetext-firefox/","loc":"https://foxmask.net/post/2016/02/10/pycharm-raccourcis-pour-obtenir-les-comportements-sublimetext-firefox/"},{"title":"IFTTT se prend les pieds dans le SuperBowl","text":"Espionné la concurrence est un devoir quand on veut voir ce qu'on pourrait rater ;) Du coup je track le flux RSS du blog Engineering de IFTTT , et reçois ce matin dans mon carnet Evernote, une info \"IF Superbowl THEN\" Je ne suis pas fan du foot americain, je serai plus NBA ou NHL, mais depuis que le sieur Michael Jordan n'est plus sur les parquets, je ne suis plus du tout ;) Mais revenons à nos moutons. Dans l'article du blog de IFTTT j'apprends qu'on peut s'amuser à faire faire un feu d'artifice à ses lampes Philips HUE quand le score du superbowl évolue. IFTTT est tout content de nous dire le nombre de personnes utilisant ces \"triggers\", de part et d'autres des supporters des 2 equipes qui vont se rencontrer et surtout de nous dire que depuis les 2 dernières editions du superbowl, leur nombre était croissant. Du coup je me mets en quete de l'api de Philips HUE pour Python que je trouve en 2 secondes, puis je me dis, me reste plus qu'à chopper celle de ESPN qui est le \"trigger\" fournissant les infos sur les scores. Et là en 2secondes je découvre : ESPN Public retirement Alors je me dis, \"tiens tiens, comment IFTTT s'y prend ? allons voir leur page de leur article pour le leur demander\" et là ... sous nos yeux ébahis . Pour vous montrer que je n'ai pas la berlue voici l'article depuis mon carnet de notes \"According to our data\" comme ils disaient, je predis des lampes Philips Hue qui vont rester eteintes :D ESPN lutte pour l'economie d'energie je ne vois pas d'autres explications ;) Voilà vouuuuuuuuu aaaaaaaaaaaa laaaaaaaaa ;)","tags":"Techno","url":"https://foxmask.net/post/2016/02/03/ifttt-se-prend-les-pieds-dans-le-superbowl/","loc":"https://foxmask.net/post/2016/02/03/ifttt-se-prend-les-pieds-dans-le-superbowl/"},{"title":"Revue Technique de la semaine du 02/02/2016","text":"Ca vous manquait aussi un billet du renard masqué hein ? Bon qu'est-ce que je vais bien pouvoir vous racontrer ? Evernote de mieux en pire ? Je résumerai bien ce qui arrive a Evernote par \"Qui trop étreint, mal embrasse\" tant ce qui se passe le reflète. L'an passé on apprenait qu'Evernote se séparait d'une partie de ses equipes d'ingénierie, après que certains associés fondateurs aient tirés leur révérence, au point qu'on evoquait sa fin, ce qui m'a du coup fait prendre conscience, comme pour google, \"nom di diou si ca ferme demain que fais je de mon contenu?\", ou \"Le syndrôme des services gratos Google qui vous font perdre vos données à vous même rien à vous\". Pour ma part j'ai rapidement repris le dessus puisque je peux facilement renvoyer tout ce que j'ai chez, eux ailleurs, via mon projet TriggerHappy . Mais revenons à nos moutons, après ce premier évènement, suit un second depuis hier, Evernote annonce cesser son activité de vente de produits hype et quand même cher pour ce que c'est. Donc fini les carnets moleskine, portefeuille et scanner, qui ne seront plus vendus que par leur partenaire. Evernote se recentrant (enfin?) sur le software. Ce qui m'a toujours paru bizarre c'est leur blog où il tente de \"louer\", via des personnes exterieures à l'entreprise, la reussite de l'utilisation de l'outil, qui leur a changé la vie, qui les a rendu plus productifs etc. Comme s'ils avaient besoins de témoignages pour s'en convaincre eux mêmes. Ca me fait trop penser aux vieilles \"pub témoignages\" moisies \"Weight Watchers\" où pour vous convaincre, il faut vous le dire... Pour ma part j'adore l'outil qui, techniquement, est incontournable (le moteur de recherche offline + l'OCR notamment + le plugin firefox de capture d'article) mais la comm marketing autour est une misère. Donc pour l'heure, tant que le bateau ne chavire pas, je ne bougerai rien, sinon j'irai m'oriente vers une alternative libre déjà existante mais pour l'heure sans API permettant d'ajouter des données autrement que manuellement. Celle ci se nommera paperwork ou laverna Django 1.9.2 security fix and 1.8.9 bug fix A vos mises à jour si vous êtes sur la 1.9.x ! Pour la 1.8.9 rien à craindre, il s'agit que d'une release concernant peu de corrections de bugs. Extensions or not to be Il y a quelques jours j'évoquais un besoin sur les forms django (je ne sais plus lequel :P) avec SpoutBe qui me disait que django crispy répondait à mon besoin. Mais je lui avais répondu que j'étais très réticent à utiliser des \"projets satellites\" parce que le jour où plus maintenus, le code produit (n')évolue (plus que) difficilement. Donc tant que c'est pas \"core\" je n'y toucherai pas. Il n'y a que les cas où des projets satellites \"naissent\" du core que je les emploie, comme par exemple django-formwizard . Et vous, comment gérez vous l'ajout de fonctionnalités offertes par de tels projets pour Django ? Vous fiez vous à la \"vivacité\" du projet pour le choisir et vous dire que c'est un gage d'évolution ? Meetup - c'est reparti ! Des meetup (DevOps, Paris.py) ont repris mais j'ai pas l'envie là tout de suite, pourtant j'ai l'occasion d'y aller en bonne compagnie, mais non là non, j'ai encore un coup de \"mood\" là , ca attendra ! Paris DevOps, aujourd'hui Paris.Py, le 4 février Arround the world Quelques bons billets à se mettre sous la dent : Evolution de Python par Sam et Max Free Software activities par Anarc News de Crossbar par Sam et Max PyCharm 5.0.4 Le Pypi nouveau est sur les rails par la PSF A Tester / A Lire Ansible 2 , à tester avant de switcher de 1.9, car ca pète pas mal de choses qui focntionnaient avant ou passent en dépréciées comme les inclusions de playbook avec un tag ... ca ca me reste en travers ... ceci n'est plus possible : include: /path/to/foo/bar tag=foobar include: /path/to/bar/foo tag=barfoo du coup dans un main.yml, je ne peux plus conditionner l'inclusion (et donc l'execution de ce qui est inclus), merci ! Github Reviewer bien démarrer DRF - Partie 5 par Makina Corpus. ici j'ai pris le feuilleton en chemin, alors j'ai raccroché les vagons en m'envoyant les 4 premières parties ;) IE 8,9,10, de l'histoire ancienne ? j'aime quand c'est microsoft qui le dit :) Oracle met à mort le support de la java applet dans les browsers . Les boites qui sont encore avec du IE < 10 et avec des applet java (au pif Oracle Forms) ont du mouron à se faire ... J'en fout pas une Depuis quelques semaines je commit que dalle sur TriggerHappy, je \"papillone\", je teste des trucs, je regarde des nouveaux langages, je glande, je joue, je procrastine, je fais de la bouffe ( genre ), la vie quoi ...","tags":"Techno","url":"https://foxmask.net/post/2016/02/02/revue-technique-de-la-semaine-du-02022016/","loc":"https://foxmask.net/post/2016/02/02/revue-technique-de-la-semaine-du-02022016/"},{"title":"AIX fsck et demarrer des services","text":"Sur AIX, lors d'un reboot (malencontreux?) il peut arriver que des folders soient vides (aïe bobo partout) Bon du coup ca peut provenir du \"simple\" fait de \"dirty\" partitions qui ont besoin qu'on leur fasse faire un check fsck /opt/websphere on repondra oui s'il vous demander de \"FIX?\" et on pourra enfin faire le mount associé # mount /opt/websphere # ls /opt/websphere/ 5 .1 6 .1 IHS Dans la continuité du reboot, il peut arriver que des services ne se soient pas relancés donc : # startsrc ftp 0513 -124 The ftp subserver has been started. et on respire !","tags":"Techno","url":"https://foxmask.net/post/2016/01/13/aix-fsck-demarrer-des-services/","loc":"https://foxmask.net/post/2016/01/13/aix-fsck-demarrer-des-services/"},{"title":"Centos et la galère galère des installations recentes PostgresSQL 9.4 et du driver psycopg2","text":"Parfois il arrive qu'on croise des distributions qui nous rebutent... mais avec lesquelles il faut composer. Aussi voici un rapide retour d'XP sur l'installation de PostgreSQL 9.4 pour Centos et de son driver psycopg2 pour son virtualenv... Pour l'installation de PostgreSQL 9.4 je vous renvois à la doc super bien faite que je ne vais pas vous remettre en intégralité ici Ensuite pour l'installation du driver dans son virtualenv, c'est pas de la tarte, voici : ( foxmask ) foxmask@localhost:~/icibase $ pip install psycopg2 Collecting psycopg2 Using cached psycopg2-2.6.1.tar.gz Complete output from command python setup.py egg_info: running egg_info creating pip-egg-info/psycopg2.egg-info writing pip-egg-info/psycopg2.egg-info/PKG-INFO writing top-level names to pip-egg-info/psycopg2.egg-info/top_level.txt writing dependency_links to pip-egg-info/psycopg2.egg-info/dependency_links.txt writing manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt' warning: manifest_maker: standard file '-c' not found Error: pg_config executable not found. Please add the directory containing pg_config to the PATH or specify the full executable path with the option: python setup.py build_ext --pg-config /path/to/pg_config build ... or with the pg_config option in 'setup.cfg' . c'est pas de la tarte, parce qu'evidement le setup.py du dossier courant concerne l'installtion de mon app et pas celle du driver Alors on fera une recherche sur son moteur de recherche favori pour tomber sur un post de stackoverflow qui nous explique qu'il \"suffit\" d'installer le package python-devel ... Seulement quand le package est déjà là-bas, alors ne reste plus qu'une solution ... altéré le PATH ... Encore faut-il trouver où est planquée postgresql avec l'installation pas \"ordinaire\" resultante de la doc ci dessus Comme chuis une trume pour mémoriser les noms de packages à rallonge de centos, je fais un : ( foxmask ) foxmask@localhost:~/icibase $ ps aux | grep post postgres 2274 0 .0 0 .1 324636 14452 ? S 08 :39 0 :00 /usr/pgsql-9.4/bin/postmaster -D /var/lib/pgsql/9.4/data donc plus qu'à faire un petit export PATH ( foxmask ) foxmask@localhost:~/icibase $ export PATH = /usr/pgsql-9.4/bin/: $PATH puis ENFIN, de relancer l'installation du driver, ( foxmask ) foxmask@localhost:~/icibase $ pip install psycopg2 Collecting psycopg2 Using cached psycopg2-2.6.1.tar.gz Building wheels for collected packages: psycopg2 Running setup.py bdist_wheel for psycopg2 Stored in directory: /home/ekip/.cache/pip/wheels/e2/9a/5e/7b620848bbc7cfb9084aafea077be11618c2b5067bd532f329 Successfully built psycopg2 Installing collected packages: psycopg2 Successfully installed psycopg2-2.6.1 and Voilà ! :)","tags":"Techno","url":"https://foxmask.net/post/2016/01/12/centos-galere-galere-postgresql-pyscopg2/","loc":"https://foxmask.net/post/2016/01/12/centos-galere-galere-postgresql-pyscopg2/"},{"title":"Dans la série, j'ai migré vers Pélican","text":"Le Besoin Le besoin est le suivant, Pélican c'est cool parce que statique mais le travers c'est qu'on ne dispose d'aucun outil \"web\" pour produire un billet. Certes, si je suis joueur je peux, via mon smartphone, me connecter sur le \"blog\", lancer vi et produire mon billet.md :-) On voit le coté pratique de la chose :) Petite digression Gordon, sur irc://irc.freenode.net/sametmax m'a parlé de Lektor, un CMS à base de fichiers plats, sans aucune base de données. Le projet semble cool. Ce qui m'a fait sourire dans l'explication de la création de ce CMS c'est que l'auteur tire sur WordPress et balaye méchament de la main toutes les solutions de generation de pages statiques pour un blog en disant que c'est orienté pour les developpeurs, soit ! Mais quand on regarde attentivement la doc, ce n'est pas très orienté pour les \"non dev\" pour autant. Pourquoi ? Regardons le quickstart The best way to get started with Lektor is to use the quickstart command to let Lektor generate a basic project layout for you. Il faut taper une commande pour créer un projet. Ca suppose que l'utilisateur sait utiliser la ligne de commandes et ... ait accès à un shell pour héberger son site. Or avec Wordpress, on décompresse l'archive, on transfert via FTP chez son hébergeur (sans accès SSH) et on a déjà accès à son blog. Bref passons :) Billets depuis GitHub via des issues Bob m'a parlé d'une solution rigolote, à base d' \"issues\" github, qui permet de produire directement des billets en utilisant du coup l'interface \"markdown\" de GitHub et des tags pour gérer le statut du billet. via des nouveaux fichiers Du coup ca m'a donné l'idée d'également exploiter l'interface markdown de github, pour, ce coup-ci, créer directement des fichiers dans le dossier content . Puis une fois fait, sur le serveur hébergeant le blog, déclencher, via une crontab , un git pull , et lancer la génération du nouveau billet. La tâche dans ma crontab donne ceci : 0 0 * * * . /foxhome/.keychain/ ${ HOSTNAME } -sh && . /foxblog/bin/activate && cd /foxblog/website && git pull && make html (on passera sur le fait que j'ai un venv dans /foxblog:) L'astuce ici consiste à utiliser keychain , pour que le git pull ne reste pas stuck sur la demande de votre passphrase lors de l'exécution de la tâche En effet, après l'installation de keychain, on lanche la commande keychain $HOME /.ssh/id_dsa ce qui produit $ keychain ~/.ssh/id_rsa * keychain 2 .7.1 ~ http://www.funtoo.org * Starting ssh-agent... * Starting gpg-agent... * Adding 1 ssh key ( s ) : /foxhome/.ssh/id_rsa Enter passphrase for /foxhome/.ssh/id_rsa: * ssh-add: Identities added: /foxhome/.ssh/id_rsa Cette commande n'est à taper qu'une seule fois, par exemple après un reboot du serveur. Si vous la tapez une seconde fois il ne se passera rien :) il réaffichera la liste des clés déjà chargées Vous avez dit Offline ? Enfin, s'il vous arrive d'être déconnecté et ne pouvez exploiter github pour produire vos billets, ben un bête editeur texte fait l'affaire :) bon si vous avez PyCharm, vous avez un super editeur qui vous montre le rendu en live de votre texte;) Pour ma part je suis toujours fourré avec Evernote pour ce genre de chose, dans le bus et le train. Ca permet de relire plusieurs fois avant de commit 'n' fois pour rien le nouveau billet. Avec tout ça, on est paré :)","tags":"Techno","url":"https://foxmask.net/post/2016/01/05/dans-la-serie-jai-migre-vers-pelican/","loc":"https://foxmask.net/post/2016/01/05/dans-la-serie-jai-migre-vers-pelican/"},{"title":"De wordpress à pelican","text":"Ce billet est surement le beacoupdième abordant le sujet ;-) J'irai donc faire court dans la mesure du possible pour expliciter le pourquoi , le comment , et une idée qui a germé lors de l'utilisation de pelican Pourquoi ? La réponse est multiple, et un peu redondante avec le billet précédent. J'ai voulu mettre fin à mon blog en wordpress parce que je ne vais pas continuer à conserver le domaine et l'hebergement associé. Comme je n'ai tout de même pas voulu tout mettre aux oubliettes, j'ai tout de même conserver un peu de ces 11ans de blog. Le choix vers pelican s'est fait naturellement après l'avoir testé pour 2 autres blogs, celui pour \" la communauté sam et max \" et celui du projet trigger-happy Comment ? Comme le dit la doc, en premier lieu il faut exporter son blog wordpress dans un fichier XML. Après, pour l'importer, on choisit le format d'import : reStructuredText, Markdown. pelican-import --wpfile foxmaskinfo.xml ainsi lancée, la commande part pour me traduire 900 billets en .rst Mais là j'ai pris une sérieuse claque. A chaque lancement de la commande je me coltinais des erreurs de formatage, des retour à la ligne inopinée, des urls tronquées pour des carriage return line feed inopinées. Au bout de 2 jours, j'arrivais à finir ma migration d'import en ayant pris soin de remplacer toutes les urls http://foxmask.info et http://foxmask.bzh en '/' purement. Mais un malencontreux et tardif (vu l'heure) rm -f content/* m'a coupé les pattes , 2j partis en fumée j'en pouvais plus, j'étais sur le point de laisser tomber la migration. J'ai donc cherché une recette sur la toile, sur le best practice de cette migration et là je tombe sur ce post complet. Et surtout j'étais passé à coté du format Markdown. Donc nouveau plan de bataille : utiliser le format markdown gerer le remplacement de foxmask.(bzh|info) avec PyCharm afin qu'il gère le remplacement récursivement et tranquillement. pelican-import --wpfile -m markdown foxmaskinfo.xml Et là, miracle : PAS UNE SEULE erreur de formatage à l'issue de cette migration, j'étais comme deux ronds de flan. Ensuite, avec PyCharm, j'acheve la mise à jour des urls en 2min :P Le reste de la migration c'est : dans pelicanconf.py , gérer le chemin static de toutes les images que j'ai pu coller dans le blog Puis après quelques ajustements de themes et plugin pelican , voilà le blog prêt. Une idée Maintenant une idée toute QQ. Pelican c'est cool parce que statique. Ya pas plus simple qu'une page HTML à servir pour un serveur HTTP. Par contre niveau production de billet, pelican est très geek oriented. Je ne m'imagine pas un utilisateur lambda de wordpress, passer à Pelican. Déjà, les thèmes ne courrent pas les rues, et la saisie des billets n'est pas le plus fun. Du coup j'm'ai dizamoimeme : pourquoi ne pas faire une interface web qui permette de pondre ses billets ? Pas un truc aussi ambitieux que l'interface de Wordpress, mais au moins : une page pour créer un article avec un editeur wysiwyg à l'enregistrement de l'article, créer le fichier .md générer la page html via la commande make html avant de me lancer dans ce truc j'ai demandé sur #pelican@freenode si un tel projet existait, et j'ai eu droit à une réponse de normand... Donc bon je vois kiki va kor faire un truc con kwa.","tags":"General","url":"https://foxmask.net/post/2016/01/02/de-wordpress-a-pelican/","loc":"https://foxmask.net/post/2016/01/02/de-wordpress-a-pelican/"},{"title":"Une autre histoire","text":"Voici donc le début de la continuité de l'histoire de ce blog de 11 ans d'âge ;) Pour ne pas \"trop\" perdre en interactivité avec ceux qui me suivent tant bien que mal, j'ai collé disqus comme outil pour vous permettre de commenter mes billets. Comme je ne suis pas du genre à imposer quoique ce soit, vous n'êtes pas obligé d'être connecté à g+, twitter, facebook, ou disqus. Pour commenter en tant que visiteur, cliquer dans le champ nom, puis sur la case à cocher \"i'd rather post as guest\", ensuite mettez simplement votre email et le tour est joué. A noter que j'ai désactivé les trackers de disqus, permettant de vous coller de la pub \"personnalisée\". On va pas dévié du leitmotiv du site hein ;) Voilou ;) Ici, je ne vous parlerai plus que de dev et rien d'autre. print ( \"Bonnes fêtes de fin d'année 2015, attention à 2016 et à vos prochaines résolutions\" )","tags":"General","url":"https://foxmask.net/post/2015/12/29/une-autre-histoire/","loc":"https://foxmask.net/post/2015/12/29/une-autre-histoire/"},{"title":"Django TEST_RUNNER exploitant settings.INSTALLED_APPS","text":"Bon, à un moment, à force de faire des modifications sur son projet pour contenter \" son \" fan ;) Je ne pouvais pas rester à me limiter à faire tourner les tests sur 2 ou 3 applications django quand j'en fourni 10. Donc actuellement (pour faire court), Django lance les tests sur tout ce qui commence par test*.py à partir du dossier courrant. Problème : J'ai 10 applications, et seules 2 sont installées (présentes dans settings.INSTALLED_APPS ), et au lancement des tests on a droit à une erreur d'absence de table liée au model de son application X. Pour que ça marche quand même, on est obligé de faire ./ manage . py test - v2 app1 app2 app3 app4 ... Pas top du tout à mon goût. Je suis donc allé à la pèche et je n'ai rien trouvé que des besoins identiques Du coup ce soir je me suis fendu d'un test assez simple et qui rempli tout à fait sa fonction. La doc dit qu'il suffit de définir un TEST_RUNNER perso en fournissant la méthode run_tests() , et de l'indiquer dans le settings.py et ca donne ceci : settings.py : TEST_RUNNER = 'django_th.runner.DiscoverRunnerTriggerHappy' runner.py : from django.conf import settings from django.test.runner import DiscoverRunner class DiscoverRunnerTriggerHappy ( DiscoverRunner ): \"\"\" A Django test runner that uses unittest2 test discovery. \"\"\" def run_tests ( self , test_labels , extra_tests = None , ** kwargs ): \"\"\" Run the unit tests for all the test labels in the provided list. Test labels are taken from settings.INSTALLED_APPS A list of 'extra' tests may also be provided; these tests will be added to the test suite. Returns the number of tests that failed. \"\"\" self . setup_test_environment () for installed_app in settings . INSTALLED_APPS : test_labels = test_labels + ( installed_app ,) suite = self . build_suite ( test_labels , extra_tests ) old_config = self . setup_databases () result = self . run_suite ( suite ) self . teardown_databases ( old_config ) self . teardown_test_environment () return self . suite_result ( suite , result )","tags":"Techno","url":"https://foxmask.net/post/2015/11/30/django-test_runner-exploitant-settings-installed_apps/","loc":"https://foxmask.net/post/2015/11/30/django-test_runner-exploitant-settings-installed_apps/"},{"title":"Sam et Max Board Suivez l'actu de la communauté","text":"Hey ! Dans un billet d'il y a 2 semaines , j'exposais un nouveau projet que j'avais en tête et lancé (pas trop fort) et pas trop loin à priori parce que personne n'a rebondi sur l'occasion pour nous faire un retour, du coup ça végète ;) Donc je relance la question ici : Que pensez vous de \" Sam et Max board \" ? Le but du site, auto généré, à partir de flux RSS, des services \"Sam et Max\", est de fournir un point unique pour que plus personne ne rate une miette de ce qui se dit, entre autre, ci et là . Le site fourni à son tour un flux RSS de tout ce qui a été récupéré sur chacun de ces services. N'hésitez donc pas à glisser un commentaire pour nous dire si ca vous va ou si c'est pourri etc :) On verra où le mettre après ;)","tags":"Techno","url":"https://foxmask.net/post/2015/11/27/sam-max-board-suivez-lactu-de-la-communaute/","loc":"https://foxmask.net/post/2015/11/27/sam-max-board-suivez-lactu-de-la-communaute/"},{"title":"Trigger-Happy, stats, requête et proposition","text":"Avec wordpress on dispose de stats par billet, et le fait de m'être mis un coup de pompe au cul pour publier des billets in ze Shakespeare language a fait que dans le top 100 des billets les plus lus sur le blog, on trouve celui concernant la sortie de la v0.11.0 in English (en 87° position avec 1013 vues en 1mois et demi). Là où la version francophone , n'a pas atteint la moitié de l'autre sur la même période. L'heureuse conséquence tout de même, c'est que le nombre de personnes s'intéressant au projet (qu'on dénombre avec les petites \"stars\" sur github) fini par atteindre 99 :) A coté de ça le nombre de téléchargements de l'appli est de 333/mois sur pypi. Pour un projet ne cherchant ni gloire ni fortune, et au départ juste pour mes propres besoins, c'est toujours plaisant même si les chiffres sont ridicules comparativement à des projets similaires. Une requête Tout récemment, hier en fait, un utilisateur a envoyé un mail à la mailbox que j'avais ouverte pour contacter le projet. Ouais c'est à ça que ça sert les mails :) Le monsieur me demandait comment pouvoir utiliser TriggerHappy depuis le site. Bon voici ma réponse : \"bonjour, le service n'est pas encore rendu accessible à tout le monde, mais si vous n'avez pas les compétences (en langage de programmation python) ou les moyens d'installer l'application, je peux vous créer un compte pour que vous le testiez à votre guise\" Donc depuis hier, un utilisateur fait joujou avec le service. Il a activé 2 services et créé un trigger (le minimum vital pour faire quelque chose) et hop roule ma poule. Une proposition Si vous voulez également tester, faites péter un mail avec login souhaité, email à utiliser, le tout à \"service chez trigger-happy point eu\" et je vous fournirai le mot de passe et l'url de connexion en retour. Avant cela, vérifier que les services couverts par le projet sont à votre goût, histoire d'éviter que vous ne pestiez parce que ca ne fait pas le café. En plus c'est un projet libre, où j'ai documenté à mort \" comment contribuer \" et fourni des outils pour en faire le moins possible ;) Si la demande est plus conséquente, je me botterai le train pour rajouter un module d'inscription plus vite que prévu (j'attendais le retour du module django dédié mais il semble que ca prenne bien plus de temps) et chacun ira s'inscrire. Comme cette proposition est tout à fait informelle, je reviendrai en détails sur ce que je fais avec \"vos données perso\", mais pour résumer, c'est \"RIEN\". Vous êtes maître à bord et décider des recettes que vous voulez faire en faisant se parler les services de votre choix. Tout ce que je ferai, c'est avoir un oeil sur l'état du projet pour savoir où améliorer les choses mais aucunement me servir de vos infos pour mon usage perso (tel que collecte de mail pour spam ou autre joyeusete qu'on \"affectionne\" tous) Je reviendrai là dessus au moment de l'ouverture \"publique\" du service. Pour l'heure, ca sera du test avec votre concours, bien que je m'en serve au quotidien pour mes propres besoins depuis le début.","tags":"Techno","url":"https://foxmask.net/post/2015/10/02/trigger-happy-stats-requete-et-proposition/","loc":"https://foxmask.net/post/2015/10/02/trigger-happy-stats-requete-et-proposition/"},{"title":"Trigger Happy, de tout, de rien, la vie","text":"Depuis la sortie de la version 0.11.0, j'ai lancé quelques bidules périphériques : un blog dédié en pelican, que j'ai commencé à remplir. J'essaierai de lui trouver un thème plus responsive que du rendu à taille fixe. un compte @TriggerHappyEu dédié qui relaiera les news du projet une inscription sur Alternativeto.net , pour voir sur quel créneau se positionne le projet au milieu des plus \"lourds\" quelques posts sur r/selfhosted & r/django Quant à l'évolution du projet, j'ai pu ajouter à son arc, 2 services, Trello , Un gestionnaire de projets et GitHub . Ce qui permet par exemple de pouvoir créer des tickets sur github dès qu'on a créé une \"carte\" Trello dans un board de son choix. Dans le même temps j'ai pris contact avec le projet WeKan , un Trello like opensource, en tout point identique à l'UI originale, afin de leur proposer d'exploiter leur API, mais elle n'existe pas encore mais sera complétement calquée sur celle de Trello. Je vois bien d'ici là un yes.wekan.io poindre le bout de son nez tiens ;)","tags":"Techno","url":"https://foxmask.net/post/2015/09/08/trigger-happy-de-tout-de-rien-la-vie/","loc":"https://foxmask.net/post/2015/09/08/trigger-happy-de-tout-de-rien-la-vie/"},{"title":"Entrevue en toute simplicité - Sam et Max","text":"Ce billet aura une \"autre saveur\" puisque, alors que je m'étais refusé à le faire, me disant que tout le monde les connaissait, j'ai fini par franchir le pas. Donc voici une entrevue avec Sam ET Max Bonjour Sam et Max, Pouvez vous vous présenter en quelques mots ? Sam Ca va être dur sans tomber dans les généralités vu le côté anonyme du blog. Disons que je suis un homme de moins de 40 ans, de plus de 20 ans, informaticien freelance avec une forte affinité pour le voyage et la pédagogie. Enfin, ça dit pas grand chose, je pense que si les gens voyaient à quoi ressemblait ma vie, beaucoup serait surpris. Il y a énormément de choses qu'on se garde de dire sur le blog. Mais franchement, ce n'est pas très important. Y a assez de status facebook inutiles pour pas que je vous raconte en plus ma marque de pomme préférée. Max : tombé dans la marmite informatique tout petit, j'ai commencé à faire des petits progs en BASIC sur MO5, rien d'extraordinaire jusqu'aux années 2000, avec la fameuse bulle internet (iBazar, Aucland, etc) le net a commencé à prendre une place dans ma vie, je programmais déjà en PHP et je sortais mes premiers sites de divertissement pour adultes. Comme ça ne rapportait pas grand chose, en parallèle je bossais dans des boites comme webdesigner la plupart du temps. Ensuite j'ai monté ma boite (affiliation sur du contenu adulte payant) et c'est là que j'ai rencontré Sam (il faisait un stage dans les bureaux d'à côté). Le reste vous le saurez en parcourant le blog ;) Comment êtes vous venus à python et que faisiez vous avant de faire du python, depuis quand faites vous du python ? Sam Mon premier ordi avec une disquette 5 pouces et un bouton turbo pour booster le proc à 8Mhz. Pas Ghz. Mhz. Mais j'ai programmé très tard, ayant commencé ma formation en tant qu'ergonome puis rédacteur sur une webzine d'info connu. Du coup j'ai vraiment attaqué la programmation avec Basic et PHP3, que j'ai laissé tombé pour Python 2.4. Un jour je me suis juste dit \"PHP c'est vraiment pourri\" et j'ai cherché quelque chose de sympas. Je suis tombé sur un post de forum \"Ruby VS Python\" (déjà à l'époque...). Les gars de Ruby vendaient le côté 'stylé' du langage. Les gars de Python vendaient le coté 'lisibilité'. Je préfère le pratique à l'esthétique, du coup j'ai choisit Python et j'ai téléchargé le bouquin de Swinnen. Max c'est la faute de Sam, j'avais besoin de refaire des sites en PHP qui foiraient, il est arrivé avec sa science et voulait tout reprendre à zéro, comme il détestait PHP je lui ai fait confiance et on a tout passé en Django / python. Je ne le regrette pas. A quels projets opensource participez vous ? Sam On peut jeter à coup d'oeil aux repos pour ça ( https://github.com/sametmax?tab=repositories ). Mais ca dépend de la période de l'année. En ce moment surtout path.py qui a gagné l'attribut app_dirs et la commutativité du /. Il y a aussi tout ce qui ne se voit pas : rapport de bugs, aide sur les forums, etc. Max Moi très peu, j'étais sur les débuts de 0bin et je crois que c'est tout, pas trop le temps faut dire. Utilisez vous Crossbar / Autobahn en prod aujourd'hui ? Sur quel type de projet ? Petit/Gros ? Avec une stack qui déchire/à toute épreuve ? Sam Absolument pas. A mon avis j'aurais un truc en prod utilisant crossbar en 2016 à cause d'un projet dans les cartons, mais ça implique développer une surcouche autour. Crossbar mérite vraiment une abstraction. Je vends la techno clairement comme quelque chose en construction. Le jour où j'aurais un truc en prod, je ferai bien entendu un article dessus. Max Moi non j'y crois pas à ce truc :) Mais bon j'attends que Sam me fasse une super démo de son utilité en PROD !!!! Quelle(s) lib tierce(s) a/ont votre préférence ? Sam Au final on en revient toujours aux mêmes modules : codecs collections csv datetime functools hashlib itertools json pprint random re subprocess sys threading uuid Ce sont les essentiels de Python. Avant il y en avait plus, mais beaucoup sont remplacés par des libs tierces parties. La stack que j'utilise le plus est encore et toujours l'écosystème autour de Django. La raison est simple : il permet de faire le job pour 90% des missions. Je l'ai testé dans des conditions de merde en Asie et en Afrique, je l'ai utilisé par des trucs cleans et des trucs à l'arrache, et ça passe. On peut faire des petits sites, des gros sites, et ça va marcher. C'est quelque chose d'extraordinaire avec Python : on a un socle de base, mais après, on a cette polyvalence du langage qui fait que dès qu'on a un besoin, il y aura quelque chose pour le faire. Jusqu'ici, je ne me suis jamais retrouvé avec un truc où je me suis dis \"j'aurais pas du prendre python\". Par contre j'ai déjà identifié des projets où je me suis dit \"ce sera mieux de ne pas prendre Python\". Le blog par exemple. Est-ce que j'utiliserais toujours Django dans le futur. Probablement pas. La raison pour laquelle j'emmerde tout le monde avec crossbar en ce moment, c'est parce que je pense qu'on peut créer quelque chose de mieux avec cette techno. Mieux que ce qui existe en Python, mais surtout mieux que ce qui existe dans tous les autres langages. De toute façon si on ne le fait pas, Python disparaîtra du monde de la programmation Web et JS bouffera tout d'ici 10 ans du simple fait de son avantage compétitif injuste : le marché captif des browsers. Donc même si demain JS n'apporte rien sur la table, à effort égal, il gagnera. Et JS apporte quelque chose sur la table avec des projets innovant comme Meteor. Pour cette raison, il faut faire mieux que les outils JS actuels si on veut rester pertinent en Web backend dans la prochaine décennie. Max Requests, beautifullSoup, scrapy. Mais en général je me bats avec Sam pour utiliser le moins de libs possible, ça foire toujours à un moment donné (updates, rétrocompatibilité). Quand on se retrouve avec 50 libs et qu'on met son Django à jour par exemple, on prie énormément et on prend son Prozac avant, ça foire à 99%. Que faites vous aujourd'hui et quel avenir/quels projets envisagez vous ? Sam Aujourd'hui c'est beaucoup de formation et de dev, pas mal de taff autour du blog, et une avancée lente sur les projets en parallèle. Je crois qu'un de nos problèmes c'est qu'on a des vies trop confortables : on a réussi à dégager plus de revenus en travaillant moins que la moyenne, on vit dans des locaux souvent très chouettes, et on fait beaucoup d'activités de loisir. Du coup on rogne sur les projets. C'est la décadence :) Max Moi je gère toujours mes sites à fort trafic, avec Sam on aimerait bien finir un projet qu'on a commencé ensemble (parmis tant d'autres) qui semble prometteur mais j'en dit pas plus :p J'ai toujours 3 / 4 idées de projets sous le coude, cette année je vais essayer de voir avec Sam pour en réaliser plusieurs ensemble, car on se marre bien. Depuis le début de l'année vous avez ouverts des services tels indexerror.net et récemment un subreddit sam&max , vous avez d'autres idées sous le sabot ? Sam Oui, mais on ne va pas faire une annonce ici pour éviter le côté vaporware. On sait jamais, les journées sont courtes et la todo liste est longue. L'argent est aussi un problème. Aujourd'hui le blog et ses dérivés me prennent 15% de mon temps de travail. Mais me rapporte 0 euros depuis 3 ans. Du coup si on veut continuer il va falloir qu'un des projets rapporte des sous (on est pas consuméristes, mais on est très dépensiers), mais sans affecter les autres projets ou détruire la bonne ambiance qu'il y a dans la communauté. C'est très difficile à faire, la plupart des gens échouent et transforment leur succès en supermarché, et c'est pour ça qu'on ne se presse pas. Mais du coup le nombre d'articles à un peu diminué, avec en contrepartie une présence sur les autres plate-formes un peu plus forte. Je ne sais pas si c'est une bonne ou une mauvaise chose. Un truc qu'on peut dire, c'est que j'ai en tête un nouveau jeu avec lot sur le blog comme on avait fait les années précédentes. Comme d'hab se sera gratuit et le lot sera payé de notre poche. Mais la réa est vraiment beaucoup plus importante, donc ça va pas arriver demain. Il faut se souvenir que le S&M est une activité, qui bien quelle nous prenne beaucoup de temps, reste en marge de nos vies. Max Là on va freiner je pense de ce côté, car il faut penser à nous, il y a le Multiboards customisable qu'il faut sortir (on a bien foiré sa prod l'autre jour c'était marrant) et ça ira pour le moment. IndexError a été sorti en 1 jour (car c'est un script gratoche) et il tourne tout seul, mais il bénéficie de la communauté S&M qui est vraiment super alors c'est pas vraiment du taf mais il a son petit succès et ça motive quand même pas mal pour envisager un nouveau projet similaire, mais pas pour le moment. En plus de l'aspect très technique et didactique, le ton décalé du blog a séduit beaucoup de monde, \"votre\" communauté à grandi, et il me semble qu'une certaine interaction s'est créée avec celle ci via les services cités ci dessus. Sans déconner, Comment gérez vous cette \"notoriété\" où vous êtes cités tantôt dans un billet d'un blog tantôt dans une conf Python, tantôt interpelés sur twitter ou irc ? C'est le mojo baby ?! ;) Sam L'anonymat aide beaucoup. Notre entourage n'a aucune idée de notre notoriété (même ceux/celles qui savent se font une idée vague), et quand je vais à un event on parle souvent de nous à côté de l'un d'entre nous. Mais comme on a une espèce de légende créée autour de nous, les gens s'attendent à quelque chose de spécial. Du coup quand on arrive à l'arrache par la porte de derrière, personne ne nous remarque. Ca fait garder les pieds sur terre, mais ça aide à une chose : avoir confiance en son jugement. Notre profession souffre énormément du syndrome de l'imposteur, et la validation des pairs aide à l'outrepasser. Il y a un autre point à prendre en compte : Max et moi sommes prêts à voir tout disparaitre du jour au lendemain. Le blog ferme, plus personne n'entend parler de nous, tous nos business se pètent la gueule. Pas qu'on le souhaite, mais on y est préparé. Du coup on profite, mais pas avec autant d'attachement au résultat qu'on pourrait imaginer. On a déjà pas mal eu de \"hard reset\" avérés ou évités de justesse dans nos vies, et ça forge le caractère. Max On le doit beaucoup à Sam, il explique super bien les choses, c'est inné chez lui. Pour le côté cul je pense que c'est mon job principal qui a déteint dessus :) Et il me semble qu'au début on voulait se démarquer un peu alors on s'est dit pourquoi ne pas parler de cas concrets de foirages de code par rapport à nos sites. Et comme les sites c'était du cul ben les premiers tutos de code portaient sur du filtrage de commentaires de cul, de l'encodage de vidéos avec ffmpeg et python, etc. Dans le couple sam&max : Comment sont répartis vos rôles ? L'un gère l'Infra, l'autre le code par exemple ? Dans vos projets communs qui fait le choix de la stack ? Sam Max s'occupe de la maintenance et du design, moi du code et de l'archi. Le reste, c'est de la co-décision. Mais on ne travaille pas tout le temps ensemble. En fait, depuis que le blog a été créé, on a déjà vécu plusieurs fois ensemble, puis changé de pays plusieurs fois travaillant, à des milliers de bornes de distance, parfois sur des projets qui n'ont rien à voir. Parfois on ne se donne pas de nouvelles pendant des semaines. C'est pour ça que c'est amusant quand les gens croient savoir où on est. Le où change très souvent. Max Moi je suis l'admin système parce que je suis le seul à pouvoir rester plus de 48h sur un bug, Sam craque au bout d'une heure. Côté code, on code tous les deux mais moi c'est franchement dégueulasse alors il repasse souvent derrière pour lisser tout ça. Pour le choix de la Stack c'est Sam qui lance les propositions mais il veut toujours utiliser les dernières versions alpha.beta.planta, moi je suis très réfractaire aux nouvelles techno exotiques car la doc est toujours naze et ça plante sans qu'on sache pourquoi, ceci dit Sam a fini par s'en apercevoir sur nos projets et petit à petit j'arrive à le convaincre d'utiliser des technos plus vieilles mais plus robustes (je rappelle que je suis seul sur pas mal de projets en permanence et qu'on est que deux sur l'ensemble pour gérer du python, php, sql, postgres, ffmpeg, une 12aine de serveurs, celery, de l'upload, nginx, lua, jquery, redis, varnish, munin, bash, bottle, etc, etc ) alors les nouvelles technos avec 30 pages de doc incomplète qui bug de partout à mettre en prod et à rajouter à cette stack, bof bof. Pour l'ergonomie de nos projets on aime bien en discuter ensemble, on finit en général par tomber d'accord. En dehors du #NSFW, et du code, comment se passe une journée loin du clavier pour sam & max ? Une vie de patachon, de fetard ? Ou simplement des \"monsieur tout le monde\" ? Sam Croyez-le ou non on est très sport (sports en plein air, salle de muscu...) et jeu (LOL, Dota, JDR, plateau, console). Perso je suis vieux jeu et je lis beaucoup, mais regarde beaucoup de films/séries modernes, alors que Max est du genre à regarder en boucle des vieux classiques et à pouvoir citer chaque virgule de South Park. Max est le fêtard, et pendant ses beuveries et nuits blanches, moi je marathonne Got. Il soulève 90kg en développer-coucher, mais je lui mets la misère en footing. On ne ressemble pas du tout à l'idée que la plupart des lecteurs se font de nous dans les comments. Les plus gros points communs qu'on ait sont les voyages et la bouffe. On voyage tout le temps. D'où notre habitude à mélanger des sujets qui n'ont rien à voir. C'est le monde qui est comme ça : un bordel mouvant dans lequel il faut trouver un moyen de passer une belle journée. Et on se fait tous les restos sur la route. Après, le blog semble indiquer qu'on passe énormément de temps uniquement ensemble, mais nous avons chacun une vie entière complètement distincte, des réseaux sociaux très séparés. Max J'ai deux passions dans la vie, les putes et la bouffe, le reste c'est superflue :) Donc quand je code pas je suis avec les biatches ou sinon à faire des bouffes (ouais je cuisine). Une conclusion ? Sam La première fois que nous nous sommes rencontrés, on n'a pas pu se blairer. Comme quoi... Max A poil les putes! Merci pour votre participation à tous 2! (re)Lire une Entrevue précédente Debnet Ashgan Spout","tags":"Entrevues","url":"https://foxmask.net/post/2015/08/31/entrevue-en-toute-simplicite-sam-max/","loc":"https://foxmask.net/post/2015/08/31/entrevue-en-toute-simplicite-sam-max/"},{"title":"Shoot an Arrow with Ansible in the TriggerHappy target","text":"Create a module for participating to TriggerHappy is now so simple that I cant imagine to make a new one without this new little module named \"Trigger Happy Ansible\" What does it do ? Well, as anyone can imagine, when you start a django project you enter, python manage.py startproject when you start a new app you enter : python manage.py startapp thus you will have a new empty module with a lot a \"empty\" files. To speed up the creation of a Trigger Happy module, first I made a simple module django-th-dummy that provides a module ready to use , but that need to be customized to be used. So I went a little far away and now you can just enter ansible-playbook -i hosts site.yml and that's all ! a new TriggerHappy module is ready to be installed in the middle of all the others ones. Under the hood, what has been done ? You need to install ansible, then setup the site.yml file to change all the variables that fit your needs and once the playbook is played, a new folder will be created with everything needed by TriggerHappy Here is the site.yml file. The lines to be changed are those ones : vars : # to directory tree and class/module/name purpose module_name : johndoe service_name : johndoe class_name : Johndoe # for setup.py purpose author : John Doe author_email : john@doe.com description : this is a module that is fun details : when fun is higher than anything url : https://github.com/foxmask/django-th-johndoe download_url : https://github.com/foxmask/django-th-johndoe/archive/trigger-happy-johndoe- # for dependencies purpose external_api : foobar external_api_class : Foobar external_api_version : 1.2.3 as you can see, I separated variables by usage domain And here is the output of the running ansible playbook ansible-playbook -i hosts site.yml PLAY [ home-sweet-home ] ******************************************************** GATHERING FACTS *************************************************************** ok: [ localhost ] TASK: [ dummy | create folder of the module name ] ****************************** changed: [ localhost ] TASK: [ dummy | create tests folder of the module name ] ************************ changed: [ localhost ] TASK: [ dummy | travis.yml ] **************************************************** changed: [ localhost ] TASK: [ dummy | gitignore ] ***************************************************** changed: [ localhost ] TASK: [ dummy | copy of th_dummy/__init__.py ] ********************************** changed: [ localhost ] TASK: [ dummy | copy of th_dummy/tests/__init__.py ] **************************** changed: [ localhost ] TASK: [ dummy | copy of LICENSE ] *********************************************** changed: [ localhost ] TASK: [ dummy | copy of MANIFEST.in ] ******************************************* changed: [ localhost ] TASK: [ dummy | copy of setup.py ] ********************************************** changed: [ localhost ] TASK: [ dummy | copy of README.rst ] ******************************************** changed: [ localhost ] TASK: [ dummy | copy of requirements.txt ] ************************************** changed: [ localhost ] TASK: [ dummy | copy of my_dummy.py to my_ {{ module_name }} .py ] *************** changed: [ localhost ] TASK: [ dummy | copy of model.py ] ********************************************** changed: [ localhost ] TASK: [ dummy | copy of forms.py ] ********************************************** changed: [ localhost ] TASK: [ dummy | copy of test.py ] *********************************************** changed: [ localhost ] TASK: [ dummy | copy of the templates ] ***************************************** changed: [ localhost ] PLAY RECAP ******************************************************************** localhost : ok = 17 changed = 16 unreachable = 0 failed = 0 ( triggerhappy-bootstrap ) foxmask@zorro:~/Django-VirtualEnv/django-th-ansible$ ls -ltR django-th-johndoe/ django-th-johndoe/: total 24 drwxr-xr-x 4 foxmask foxmask 4096 août 23 16 :28 th_johndoe -rw-r--r-- 1 foxmask foxmask 14 août 23 16 :28 requirements.txt -rw-r--r-- 1 foxmask foxmask 1368 août 23 16 :28 README.rst -rw-r--r-- 1 foxmask foxmask 1186 août 23 16 :28 setup.py -rw-r--r-- 1 foxmask foxmask 194 août 23 16 :28 MANIFEST.in -rw-r--r-- 1 foxmask foxmask 1484 août 23 16 :28 LICENSE django-th-johndoe/th_johndoe: total 28 drwxr-xr-x 2 foxmask foxmask 4096 août 23 16 :28 tests -rw-r--r-- 1 foxmask foxmask 471 août 23 16 :28 forms.py -rw-r--r-- 1 foxmask foxmask 614 août 23 16 :28 models.py -rw-r--r-- 1 foxmask foxmask 6424 août 23 16 :28 my_johndoe.py -rw-r--r-- 1 foxmask foxmask 81 août 23 16 :28 __init__.py drwxr-xr-x 3 foxmask foxmask 4096 août 23 16 :28 templates django-th-johndoe/th_johndoe/tests: total 4 -rw-r--r-- 1 foxmask foxmask 3725 août 23 16 :28 test.py -rw-r--r-- 1 foxmask foxmask 0 août 23 16 :28 __init__.py django-th-johndoe/th_johndoe/templates: total 4 drwxr-xr-x 2 foxmask foxmask 4096 août 23 16 :28 th_johndoe django-th-johndoe/th_johndoe/templates/th_johndoe: total 20 -rw-r--r-- 1 foxmask foxmask 1277 août 23 16 :28 edit_provider.html -rw-r--r-- 1 foxmask foxmask 1277 août 23 16 :28 edit_consumer.html -rw-r--r-- 1 foxmask foxmask 1513 août 23 16 :28 wz-3-form.html -rw-r--r-- 1 foxmask foxmask 1513 août 23 16 :28 wz-1-form.html -rw-r--r-- 1 foxmask foxmask 382 août 23 16 :28 callback.html","tags":"Techno","url":"https://foxmask.net/post/2015/08/24/shoot-an-arrow-with-ansible-in-the-triggerhappy-target/","loc":"https://foxmask.net/post/2015/08/24/shoot-an-arrow-with-ansible-in-the-triggerhappy-target/"},{"title":"Django Trigger Happy 0.11.0 English Version","text":"Hi, Here is an English Version of my post I made Yesterday, that I couldnt forget to publish ;) So, Here comes a small update of my little project, micro ESB, allowing to orchestrate data retrieval and publishing, while exploiting your own Internet services (like Twitter to name one). Just to stay the master and crontrol your data without having to give your access permissions to anyone. In celebration program : New Functions Its now possible to produce RSS flux from the data, retrieved by another service we will installed. For example from the Twitter service. Typically we do a trigger for publishing on twitter, anything that comes from a site such as the week in chess, follow the news. Now you can do the opposite. Follow hashtag #chess, for example, and all that will be published on this subject, will eventually be generated in an RSS feed by TriggerHappy. I extend a hello to lonely friends of yahoo pipes to whom I did that, with a foot call from sam&max ;-) New service Integration : Trello . \"is the free, flexible, and visual way to organize anything with anyone.\" as it defines itself. You can then add card of things to do to organize your project. This adds a toy to the list of keys ring : Twitter, Evernote, RSS, Readability, Pocket Search engine (based on haystack & elasticsearch).It is not luxury when you ended up having a lot of triggers, existential questions arise \"well I had yet created a trigger that spoke of recipes Breton cuisine\" ;-) A function \" holidays \" which disables all the triggers, to enjoy his vacation for good ! Then, when you will come back from holidays, you disable the holidays mode which will reactivate the triggers that were enable before. Technical improvements No more Python2 anywhere. This force me to find solutions of other lib oauth2 authentication for the services like readability and Evernote. Blessing in disguise ! requests oauthlib is the solution like anyone can imagine :-) Django 1.8.x (naturally) Reorganisation of services modules in one application rather than having a repository for each module, I ended up all together in filing trigger-happy. Currently it's convenient for releasing and for unit testing, but my gut tells me that at one time I bite into fingers. Managing a limit on the number of publications to a service. Example : I publish on Twitter more than 30 websites that I follow. At a given moment, the news of each of them come too much, that I publish too quickly on Twitter, this has the side effect, to \"flood\" the timeline of my friends and followers, who, instead of having a heterogeneous timeline ending by hate me to plublish quicker than que Lucky Luke. Now, \"this\" is over. We define a limit and when it's reached, we publish the rest later. Performances As I'm never satisfied of what I produce, even when I finish a thing I tell myself I can do even better. From this perspective, therefore, I articulated the code based on the \" framework cache \" of django which permits to use the backend of your choice. Thus, all retrieving of data of all the service, is put in the cache. Then, at the publishing moment, TriggerHappy will pick the data in the cache. Before that, all was synchrone. Now Celery orchestrates this retrieving of data and their publishing. Documentation Updated everywhere. Do not hesitate a moment to read it To upgrade from the previoous release everything is here, a migration that took me time to finalize And tommorow ? some new service are planned, and some other ideas :) I also took the opportunity to rearrange tickets/labels/milestones on github, an easy way to find what one seeks to facilitate contributions . Thanks ! Thank to some interested , to some curious and finally to contributors who try to tiptoe ;) links : Trigger Happy home page Trigger Happy GitHub repository","tags":"Techno","url":"https://foxmask.net/post/2015/08/18/django-trigger-happy-0-11-0-english-version/","loc":"https://foxmask.net/post/2015/08/18/django-trigger-happy-0-11-0-english-version/"},{"title":"Django Trigger Happy 0.11.0","text":"Bonjour, Voici venue une petite mise à jour de mon petit projet, de micro ESB, permettant d'orchestrer la récupération de données et leur publication, le tout en exploitant vos propres services internet (comme Twitter pour n'en citer qu'un). Juste histoire d'être maître de vos données sans devoir donner vos autorisations d'accès à n'importe qui. ( un rappel complet dispo à partir d'un talk @ django Paris ) Au programme des réjouissances : **Nouveautés Fonctionnelles** - Il est à présent possible de **produire des flux RSS** à partir de données, récupérées par un autre service qu'on se sera installé. [![Services Installés](/static/2015/08/service_installe.png)](/static/2015/08/service_installe.png) Par exemple à partir du service twitter. Classiquement on fait un trigger publiant sur twitter, tout ce qui provient d'un site par exemple, europe-echecs, pour suivre l'actu échiquéenne. À présent on peut faire l'inverse. Suivre un hashtag \\#chess, par exemple, et tout ce qui sera publié sur ce sujet, finira par être généré dans un flux RSS par TriggerHappy. J'adresse un petit coucou aux amis esseulés de **[yahoo pipes](https://pipes.yahoo.com/pipes/)** pour qui j'ai fait ceci, avec un appel du pied de sam&max ;-) - **Intégration d'un nouveau service** : **[Trello](http://coreight.com/content/utiliser-trello-comme-un-pro)**. Ce service permet de gérer des projets en créant des tableaux de bord et pour chaque tableau de bord des listes de cartes. Cartes de \"trucs\" à faire pour organiser son projet. Pour la petite histoire, lors de mes tests, j'ai suivi le hashtag \\#django en créant un trigger qui devait publier dans Trello, mais comme mon code ne marchait mais immédiatement, le trigger a accumulé plus de 200 tweets qui sont arrivés dans un tableau de bord dédié et je pouvais voir en live dans trello arrivés les tweets un par un :-). Ce qui ajoute un jouet de plus au trousseau de clés : Twitter, Evernote, RSS, Readability, Pocket, Trello - **Un moteur de recherche** (basé sur haystack & elasticsearch). Ça n'est pas forcément du luxe quand on fini par avoir un grand nombre de triggers, des questions existentielles surgissent \"j'avais pourtant bien créé un trigger qui parlait de recettes de cuisine Bretonne\" ;-) - Une fonction \"**vacances**\" qui désactive tous les triggers, pour se mettre au vert pour de bon pendant ses vacances ! Ensuite quand vous revenez de vacantes vous désactivez le mode \"vacances\" ce qui réactivera que les triggers actifs avant le départ. **Améliorations Techniques** - Plus de python 2 nulle part. Ce qui m'a \"forcé\" à trouver des solutions d'authentification différentes de la librairie oauth2 pour les services tels readability et Evernote. Un mal pour un bien ! [requests oauthlib](https://requests-oauthlib.readthedocs.org/en/latest/) est la solution comme chacun s'en doutera :-) - Django 1.8.x (naturellement) - Réorganisation des modules par services dans une seule application Plutôt que d'avoir un dépôt par module, j'ai fini par tout regrouper dans le dépôt trigger-happy. Actuellement c'est pratique pour releaser et pour les tests unitaires, mais mon petit doigt me dit qu'à un moment je m'en mordrai les doigts. - Gestion d'une limite du nombre de publications à destination d'un service. Exemple : je publie sur twitter plus de 30 sites que je suis. À un moment donné les news de chacun d'eux affluent tellement, que je publie trop vite des news sur twitter, ce qui a pour effet de bord, de \"pourrir\" la timeline de mes amis followers, qui au lieu d'avoir une timeline hétérogène finisse par me haïr de publier plus vite que Lucky Luke. À présent \"ça\", c'est fini. On défini une limite et quand elle est atteinte on publie le reste plus tard, tranquille pépère. **Performances** - je suis un éternel insatisfait de ce que je produis, même quand je fini un truc je me dis que je peux largement encore mieux faire. Dans cette optique donc, j'ai articulé le code en le basant sur le \"[framework cache](https://docs.djangoproject.com/en/1.8/topics/cache/)\" de django qui permet d'utiliser le backend de son choix. Ainsi, toute récupération de données des services est déposé dans le cache. Puis au moment de publier les données, TriggerHappy va le chercher dans le cache. Avant cela, tout était synchrone. [A présent](/post/2015/06/19/supervisor-celery-django-orchestration/)[Celery](http://celery.readthedocs.org/) orchestre ces récupérations de données et leurs publications . **[Documentation](http://trigger-happy.readthedocs.org/)** - mise à jour en long en large et en travers. N'hésitez pas un instant à la parcourir - Pour mettre à jour depuis la version précédente [tout est ici, une migration qui m'a pris du temps à finaliser](http://trigger-happy.readthedocs.org/en/latest/migration.html) Et demain ? de nouveaux services sont prévus, et quelques idées :) J'en ai aussi profité pour réorganiser les tickets/labels/milestones sur github, histoire de facilement trouver ce qu'on cherche, afin de faciliter les contributions . Merci ! merci à quelques intéressés , à quelques curieux et enfin aux contributeurs qui s'essayent sur la pointe des pieds ;) liens : Trigger Happy page d'accueil Trigger Happy GitHub repository","tags":"Techno","url":"https://foxmask.net/post/2015/08/17/django-trigger-happy-0-11-0/","loc":"https://foxmask.net/post/2015/08/17/django-trigger-happy-0-11-0/"},{"title":"Ansible : les variables définies à la volée","text":"Ansible : comment définir des variables à la volée dans son playbook Imaginons que j'ai besoin de définir des variables au fur et à mesure que le processus avance Cas concret déployer des WAR issues d'une \"nightly build\" depuis des repo maven : je demarre par une tâche allant à la pêche aux SNAPSHOT - name : Find last snapshot command : ssh {{ server_repo }} ls -d {{ maven_repository }}/standard/{{ branch }}.*-SNAPSHOT | sort register : snapshot ici register va me permettre d'utiliser \"snapshot\" par la suite. Disons command m'a permit de trouver FOOBAR-1.2.3-SNAPSHOT - name : let's define \"war_path\" command : echo {{ path }}{{ snapshot.stdout | basename }}.war when : snapshot register : war_path ici 2 choses et une astuce : 1) j'utilise de nouveau register pour pouvoir télécharger le war plus tard 2) j'utilise when ce qui permet de ne faire la command que quand la tasks snapshot est register ed 3) l'astuce : utiliser command comme si on avait fait un simple export FOO=BAR, et sur la ligne de command on utilise snapshot.stdout parce qu'on a register snapshot précédement qui n'est qu'une chaine et pas une liste. Ce qui, avec les filtres \"basename\" et la concaténation avec \".war \", me donne grosso modo comme résultat un tout QQ : echo /un/jolie/path/FOOBAR-1.2.3-SNAPSHOT.war tâche suivante : - name : war_path download from server_repo command : scp -pr {{ server_repo }}:{{ war_path.stdout }} /temp when : war_path ici un simple scp, on remarquera war_path qui est la tasks register juste au dessus et le when qui utilise war_path - name : war_path check is here stat : path=/temp/{{ war_path.stdout | basename}} register : war_path_exists ici vérification que le transfert a eu lieu - name : war_path fail to download fail : file /temp/{{ war_path.stdout | basename}} does not exists when : war_path_exists.stat.exists == False ici on peut vérifier que ça a foiré, on se contente d'un message d'erreur mais on aurait pu faire péter un mail au service IT :P - name : war_path Extract command : chdir={{ home }}/{{ target }}-tomcat/webapps/{{ foobar }} {{ java_home }}/bin/jar xf /temp/{{ war_path.stdout | basename }} when : war_path et on acheve par la décompression du war je vous fais grâce du restart tomcat :) Pour finir, un petit gist qui fait joujou avec uniquement des variables à base de echo :)","tags":"Techno","url":"https://foxmask.net/post/2015/08/11/ansible-les-variables-definies-a-la-volee/","loc":"https://foxmask.net/post/2015/08/11/ansible-les-variables-definies-a-la-volee/"},{"title":"TODOIST vs Trello : rapidos prise en main","text":"TODOIST et TRELLO joue dans le même crédo: celui de constituer des listes de trucs à faire, avec un coté plus organistation de boulot en d'equipe du coté de TRELLO. Les 2 projets sont dispo aussi bien sur Android que Windows. Sur Windows: Bémoles : Trello ne s'installe qu'à partir de windows 8 voire 10. Todoist: interface très minimaliste, la même que celle du site web, une fenêtre, au pif pas plus grande qu'une reso 640*480... du coup ca limite l'utilisation. Mais ya pire (si c'est possible) Sur Android: Trello passe impeccablement, interface propre et agréable, ca donne limite envie de le remplir dans tous les sens ;) Todoist: interface toute aussi minimaliste, voire trop, là c'est l'inverse, j'ai du mal à m'y faire Point noire chez Todoist : on se mange quasiment constamment des popup \"abonnez vous à Todoist premium\" pour : - Ajouter des pièces jointes ... - utiliser des étiquettes - Ajouter une filtre Je ne sais plus quel ergonome disait qu'il valait mieux NE PAS MONTRER que ces features EXISTENT pour éviter la frustration de ne pas pouvoir s'en servir et qu'on rejette au final l'outil tout entier, et par exemple dédiée une page qui détaille \"version Free\" vs \"version Premium\" Quand un projet démarre, coller du \"vazy paye pour ca et ca et ca\" ; ca rebute. A côté Trello, c'est tout propre, je n'ai pas encore vu de limitations et du coup ca m'a donné envie d'utiliser leur API pour l'intégrer à TriggerHappy Pour conclure, voici les liens pour télécharger pour votre plateforme : les plateformes chez Trello - les plateformes chez Todoist","tags":"Techno","url":"https://foxmask.net/post/2015/08/10/todoist-vs-trello-rapidos-prise-en-main/","loc":"https://foxmask.net/post/2015/08/10/todoist-vs-trello-rapidos-prise-en-main/"},{"title":"Revue Technique semaine du 03/08/2015","text":"Tombé sur un truc qui m'a interpelationanoné : https://twitter.com/amontalenti/status/629006798922608640?s=09 Ca cause apache mais pas que puisqu'aussi de python. Il s'agit de deux projets apache nommés : storm qui fourni des \" tuples \" . Il a été utilisé (je ne sais pas s'il l'est toujours) par twitter. kafka qui pour faire court est un système de pub/sub, qu'on peut manipuler avec python via https://pykafka.readthedocs.org/en/latest/ je ne sais pas si vous voyez des similitudes avec CROSSBAR et Autobahn mais je trouve que c'est frappant ;) Autobahn 0.10.5 Pendant qu'on y est voilà quand même une nouvelle version :-) Dans le genre \"j'ai pas quatre bras ducon\" Samsung sortirait une petite tablette de 18.4 \" Elle devrait sortir avec avec une clé allen de chez Ikea pour se fixer aux hanches, les 2bras fournis en kit pour pouvoir porter le bidule. Si c'est vrai c'est apple qui m'l'a dit. Le changement c'est maintenant disait je sais plus qui... Microsoft fait la manche en vous demandant une petite pièce de monnaie, ou un ticket restaurant, ou un sourire pour le P-DG de windows 10. Si je débloque ? Hô si peu . Cette petite pièce me permettra de pouvoir utiliser un lecteur DVD ou jouer au solitaire puisque je n'ai pas d'ami . Pour finir par un truc pas tout neuf mais sOOoo Goude ! https://twitter.com/rit/status/549814266094551040","tags":"Techno","url":"https://foxmask.net/post/2015/08/07/revue-technique-semaine-du-03082015/","loc":"https://foxmask.net/post/2015/08/07/revue-technique-semaine-du-03082015/"},{"title":"Tuto linux : trouver le process qui a ouvert un port","text":"A qui ca n'est pas arrivé de voir un process se planter parce que \"port already bound\" ... Comment repérer le process qui a ouvert le port dont on a besoin ? imaginons les ports 8009 et 8080 on commence par un $ netstat -an | grep 8009 tcp 0 0 0 .0.0.0:8009 0 .0.0.0:* LISTEN tcp 0 0 127 .0.0.1:8009 127 .0.0.1:15561 ESTABLISHED tcp 0 0 127 .0.0.1:15477 127 .0.0.1:8009 ESTABLISHED $ netstat -an | grep 8080 tcp 0 0 0 .0.0.0:8080 0 .0.0.0:* LISTEN tcp 0 0 127 .0.0.1:8080 127 .0.0.1:60159 ESTABLISHED tcp 0 0 127 .0.0.1:58230 127 .0.0.1:8080 TIME_WAIT pour s'assurer que ces ports soient bien ouverts, histoire de voir qu'on a bien les yeux en face des trous Ensuite je vais pour utiliser lsof : $ lsof -i | grep 8009 httpd 20944 apache 14u IPv4 48859942 0t0 TCP localhost:15588->localhost:8009 ( ESTABLISHED ) httpd 20945 apache 14u IPv4 48858055 0t0 TCP localhost:15478->localhost:8009 ( ESTABLISHED ) java 23193 jboss 283u IPv4 48833112 0t0 TCP *:8009 ( LISTEN ) java 23193 jboss 830u IPv4 48858029 0t0 TCP localhost:8009->localhost:15473 ( ESTABLISHED ) java 23193 jboss 833u IPv4 48858251 0t0 TCP localhost:8009->localhost:15487 ( ESTABLISHED ) Parfait je vois qui m'enquiquine sur ce port j'ai le pid 23193 :P puis la meme pour le port 8080 $ lsof -i | grep 8080 $ mais là, rien ne s'affiche, wtf ? c'est parce que lsof -i affiche les process ayant ouvert des ports dont les services sont \"connus\" de /etc/services donc d'abord il faut faire un $ grep 8080 /etc/services webcache 8080 /tcp http-alt # WWW caching service webcache 8080 /udp http-alt # WWW caching service pour localiser le NOM dudit service lié au port 8008, puis on conclut par $ lsof -i | grep webcache java 23193 jboss 284u IPv4 48833113 0t0 TCP *:webcache ( LISTEN ) et voilà j'ai trouvé mes coupables ;)","tags":"Techno","url":"https://foxmask.net/post/2015/08/05/tuto-linux-trouver-le-process-qui-a-ouvert-un-port/","loc":"https://foxmask.net/post/2015/08/05/tuto-linux-trouver-le-process-qui-a-ouvert-un-port/"},{"title":"Entrevue en toute simplicité - Debnet","text":"Bonjour Debnet, Peux-tu te présenter en quelques mots ? Alors, je m'appelle Marc, j'ai environ 30 ans et je vis dans le Nord. Je suis expert technique, architecte logiciel, libriste convaincu et activiste agile, je baigne dans les technologies depuis mon plus jeune âge et j'adore ça ! Comment es-tu venu à Python et/ou Django et que faisais-tu avant de faire du Python ? Depuis quand fais-tu du Python et/ou Django ? Quand j'ai fini mes études, j'avais un bon bagage de technologies \"grand public\" tels que Java ou .NET. La première chose que j'ai fait était de pousser les portes d'une SSII locale et j'ai pû affiner encore plus mes compétences. Jusqu'au jour où l'université cherchait des intervenants extérieurs pour enseigner des concepts techniques ou des langages de programmation aux universitaires, j'ai proposé alors d'enseigner le Ruby (on Rails) et le Python (avec Django), que j'ai alors dû apprendre pour l'occasion parce que j'étais curieux et qu'ils commençaient à faire parler d'eux. Résultat : je n'en suis plus jamais revenu et je continue aujourd'hui de les enseigner à l'université. J'ai également réalisé à l'époque un outil de gestion de projet agile pour le compte de ma SSII en Django. Du coup je fais du Python depuis plus de 5 ans maintenant. A quels projets open-source participes-tu ? C'est délicat, je n'ai pas de \"projet communautaire\" à proprement parler, mais je participe beaucoup au suivi et à l'amélioration des produits que j'utilise au quotidien. L'écosystème dans le monde de l'open-source est très bien fichu quand on prend le temps de s'y intéresser, et on se laisse vite emporter lorsqu'on est un minimum curieux. Les projets où ma participation est la plus active sont certainement \"django-rest-framework\" (surcouche REST par dessus Django, la pierre angulaire de beaucoup de nos projets professionnels) et \"celery\" (qui permet d'exécuter des tâches asynchrones dans un environnement Python/Django). J'ai forké quelques projets au seuil de l'abandon également, mais principalement pour les besoins de nos projets. Et bien sûr, avec Sam & Max , on est vite amené à participer aux communautés francophones autour de Python. Je suis également très présent sur Twitter où je collecte les trucs techniques utiles et j'en profite pour troller JavaScript. Quelles parties de Python et/ou Django et/ou stack utilises-tu le plus ? Loin devant les autres \"django-rest-framework\", ce truc a révolutionné notre façon de travailler avec Django, surtout que dans le web \"moderne\" (avec beaucoup de guillemets) la tendance est à la séparation et l'abstraction des couches (avec AngularJS côté front notamment). Ca fait finalement longtemps que j'ai pas codé une vraie vue avec des forms en Django, ces outils sont très bien mais ils ne s'inscrivent plus à mon sens à la vision actuelle du web. Ma curiosité me pousse également de plus en plus vers \"asyncio\" et \"crossbar\", mais là c'est le manque de véritable plus-value business qui freine mon investissement personnel, sans compter que ces technologies sont encore un peu jeunes et mal exploitées. Quelle(s) librairie(s) tierce(s) a/ont ta préférence ? Alors j'ai dû faire un petit tour dans les requirements de mes projets pour répondre à celle-là. En vérité je ne me suis jamais vraiment posé la question, j'ai souvent des automatismes et des habitudes concernant les dépendances, je ne m'en rends même plus compte. Hormis les projets cités précédemment qui sont excellentissimes (\" django-rest-framework \" et \" celery \" pour rappel), il y a quelques noms qui reviennent : Déjà \" numpy \"/\" scipy \", on ne s'en sert vraiment pas tous les jours, mais quand c'est le cas c'est bougrement impressionnant. Python s'est fait une place de choix dans les laboratoires d'informatiques grâce à ça, par contre je le maîtrise encore assez mal car j'ai rarement de cas concrets qui nécessitent leur utilisation. Ensuite \" requests \", je ne peux plus faire sans et je ne me pose même plus la question de l'utiliser systématiquement. Ensuite pour Django uniquement, on a l'embarras du choix et la plupart sont de qualité : - Je ne citerai pas la fameuse \" django-debug-toolbar \" qui reste indispensable même quand on fait du REST. - Le combo \" pytest \"/\" mommy \" pour les tests unitaires, ça fait gagner un temps dingue. - \" ipython \" dont je me sers tous les jours et qui se marie très bien avec Django. - \" raven \" avec \" sentry \" pour la surveillance et le monitoring des applications, un must-have. Que fais-tu aujourd'hui et quel avenir envisages-tu ? Aujourd'hui, je travaille chez un éditeur qui s'est longtemps reposé sur son unique progiciel et qui n'a pas su prendre le virage de la modernité à temps. Je suis arrivé là bas pour insuffler une nouvelle dynamique et apporte des propositions techniques pour concrétiser leurs nouvelles ambitions. J'y bénéficie d'une confiance absolue m'ayant permis de pousser des technologies comme Python et Django, travailler avec des méthodes agiles, apporter la notion d'intégration continue et les tests unitaires ainsi que l'open-source. En somme une vraie révolution, avec ses hauts et ses bas au quotidien mais surtout un vrai défi ! Si l'avenir le permet, je voudrais continuer à faire du coaching et du consulting dans les nouvelles technologies et accompagner comme je le fais les équipes sur les problématiques techniques et sur l'architecture des projets. Question délirium : demain tu trouves une idée de génie qui grâce à Python sauve la planète, qu'est-ce que ça serait et comment ? Alors j'ai un peu adapté la question, tu m'en voudras pas, dans la version d'origine j'avais je crois le choix entre : - la sauver de la désertification en transformant les déserts du Sahara et de Gobi, - la sauver de la connerie humaine en réinitialisant le cerveau des humains, - tourner une suite à l'âge de glace nommé \"le python des neiges\" (ça peut sauver la planète ça ?) - t'envoyer chez le psy parce que t'as fumé et que je comprends rien à tes questions ! Je t'avoue que je suis bien tenté par la dernière proposition. Sinon, si l'humanité comptait sur moi pour la sauver, elle serait bien mal tombée parce que des idées de génie j'en ai, mais de là à les appliquer ça serait plutôt un truc du genre \"on verra demain\". :D A bientôt ! o/ Restons en contact si vous souhaitez entrer en contact avec le sieur Debnet, faites moi le savoir via la page de contact , et je lui ferai suivre ;) (re)Lire une Entrevue précédente Ashgan Spout","tags":"Entrevues","url":"https://foxmask.net/post/2015/08/03/entrevue-en-tout-simplicite-debnet/","loc":"https://foxmask.net/post/2015/08/03/entrevue-en-tout-simplicite-debnet/"},{"title":"Revue Technique de la semaine du 27/07/2015","text":"OnePlus 2 Le nouvel appareil de OnePlus montre les crocs, mais ce dernier fait petite mine avec un proc moins bien lotis que ses concurrents, sinon une bonne grosse batterie comme le LG 4 qui va permettre de profiter de l'objet dans se dire \"oh p#@%& vite un câble USB !\" Pour rappel, cet appareil a produit sa propre ROM depuis janvier après s'être copieusement pris la tete avec CyanogenMod pour non respect de l'exclusivité de l'expoitation de CM ... Voici donc les nouveautés de OxygenOS très très très proche d'un Android Stock. Retrouver aussi le comparatif avec les autres \"gros\" du marché. les prix : \\~340€ pour la version 16Go contre \\~400€ la version 64Go Google nous lâche la grappe avec Google+ ! Après nous avoir bien fait chier à devoir absolument nous coltiner son réseau social de merde dès l'ouverture d'un compte sur Gmail ou à l'ouverture d'un smartphone sur Android... Google fait son mea culpa et lâche l'affaire en commençant par YouTube où on pourra commenter sans avoir un compte g+ Postgresql à toutes les sauces et sur debian :) https://twitter.com/rodoq/status/609364038695407616?s=09 La couv' de la semaine (voire du mois ou de l'année) ! Allez voir Humanoide n°5 , vous devriez comprendre comment ENFIN on nous écoute depuis le temps que chaque parti prend des tartes électorales et que chacun de ses dirigeants nous sort à qui mieux mieux \"on a compris le message des Français\". Ben là on va se faire une joie de leur brouiller la ligne :P IndexError Comment pourquoi que ca marche-t-il (pas) mon code, S.O.S d'un codeur en détresse : voici les 10 derniers sujets traités ou en cours sur IndexError.net Intégrer Python Social Auth à une app Django Ajouter de manière récursive des clés service sms avec django un python dans le jardin Sanitize HTML, comment retirer des attributs via une wildcard ? Accéder à une Foreign Key dans la classe d'un modèle marshall python Bottle + AutobahnPython + pcduino Pourquoi Python n'intègre pas un module natif pour faire de la crypto (AES, DES etc.) ? Requetes Django en parallèle","tags":"Techno","url":"https://foxmask.net/post/2015/07/29/revue-technique-de-la-semaine-du-27072015/","loc":"https://foxmask.net/post/2015/07/29/revue-technique-de-la-semaine-du-27072015/"},{"title":"Entrevue en toute simplicité - Ashgan","text":"Bonjour Ashgan, Peux tu te présenter en quelques mots ? Ashgan Je suis expat' à Dakar, au Sénégal, depuis bientôt 10 ans. Actuellement sysadmin / dev indépendant pendant le peu de temps libre qu'il me reste, je suis un autodidacte complet. Pour ceux qui sont assez vieux pour connaitre, j'ai joué comme tout le monde avec des MO5 pendant les cours, quelques fois le CPC464 noir/vert à cassette piqué en douce au grand frère ou les fameuses TI-8x au lycée. Bref, essentiellement du Basic et dérivés au siècle dernier, puis plutôt du rien depuis, en fait. Traumatisé par les goto? va savoir.... Ceci dit, étant sysadmin dans une petite structure, à part commettre un script bash ou un truc web en HTML voire PHP de temps en temps, le dev ressemble plutôt à du debug de formules excel chez Aminata (la Brigitte locale) à la compta. Comment es tu venu à python et que faisais tu avant de faire du python, depuis quand tu fais du python ? Ashgan Pour diverses raisons, ça fait maintenant 4-5 ans que mon infra tourne à peu près ET que j'ai du temps pour me consacrer à d'autres choses. A l'époque, j'étais tombé sur \"Learn Python The Hard Way\" ou un autre bouquin tout aussi recommandable. Je me suis sorti les doigts, consommé beaucoup de cafés+aspirines et j'ai commencé petit, script après script, histoire de s'approprier le truc sur des problèmes d'admin-sys, qui me prennent toujours une bonne partie de mon temps. Puis un jour, la boulette : j'en ai eu marre de voir mes users recopier à la main 3-4 données d'un écran vert façon année 80 vers excel et aujourd'hui j'ai un intranet en Flask sur les bras. Mon côté mère Theresa . J'arrête pas de rajouter brique apres brique. Un problème ? \"y'a python pour ça.\". Depuis, mes users ont le poil vif et l'oeil humide. mon côté marabout . J'ai pas vraiment l'envie ou la prétention de remplacer l'ERP maison codé en RPG-LE mais ça donne un petit côté 21eme siècle à une infra des années 90, c'est rigolo. Mon côté Denisot . Quelles parties de Django|Flask / python utilises tu le plus ? (et / ou ta stack utilisée) Ashgan Rien de très méchant. Je tourne essentiellement avec sur Flask (et 2-3 plugins, genre flask-login, flask-kvsession), pypyodbc, pygal. Ca couvre 99% de mes besoins. Pour le reste, classique: Debian stable, nginx, uwsgi. Quelle(s) lib tierce(s) a/ont ta préférence ? Ashgan j'aime beaucoup Flask pour son côté tout terrain: même si ton environnement est bizarre (pour pas dire pourri), tu arrives toujours à faire un truc. Va faire pareil avec Django aussi facilement :) Il y a aussi requests et beautifulsoup sont très pratiques pour se sortir de la m\\&#94;panade dès que t'as un truc à récupérer online. j'ai une note très recente dans mes trucs à voir rapidos : mailthon. ça a l'air très sympa pour envoyer du mail, couplé à requests/bs, j'ai quelques scripts qui vont applaudir des 2 mains! Que fais tu aujourd'hui et quel avenir envisages tu ? Ashgan aujourd'hui, je suis partagé entre l'admin-sys et le dev. je fais aussi quelques extras de temps en temps (en python, forcement) quand le problème m'intéresse. J'aime beaucoup réfléchir sur des problèmes plus ou moins bizarres, sur toute la stack: comment, sur quoi, quel chewing-gum coller sur quel trombone, etc... Mon côté Mc Guyver . A ce propos, pardon à ceux qui m'ont pas encore mute sur IRC, c'est mon côté Calimero :) Si je peux trouver un poste de développeur python, que demander de plus? Question délirium : Demain t'as des super-pouvoirs en Python : à quelles fins les mets tu à profit ? Tu changes la version de ton python au boulot ? (faut pas déconner le cadre de travail ca compte :P) Tu transformes python en un autre langage ? Tu ponds un outil connecté inutile & indispensable ? (lequel ? :) Ou comme dirait une certaine \"Miss\" de concours de beauté \"... Pour un monde en Paix !\" Ashgan t'es en panne de plan pour dominer le monde, c'est ça ? j'en ai un, je le garde ! Une conclusion ? Ashgan Je ne sais pas quel exemple mon interview va donner aux p'tits jeunes ou aux autres, mais l'exercice était marrant. sinon...oubliez Django, Flask c'est le bien! :D Merci pour ta participation et à bientôt sur #sametmax@freenode.net ;) Restons en contact si vous souhaitez entrer en contact avec le sieur Ashgan, faites moi le savoir via la page de contact , et je lui ferai suivre ;) (re)Lire une Entrevue précédente Spout","tags":"Entrevues","url":"https://foxmask.net/post/2015/07/27/entrevue-en-toute-simplicite-ashgan/","loc":"https://foxmask.net/post/2015/07/27/entrevue-en-toute-simplicite-ashgan/"},{"title":"Entrevue en toute simplicité - Spout","text":"Voici une nouvelle série de billets qui démarre aujourd'hui : de petites entrevues en toute simplicité avec des dev de touzorizon. Le premier d'entre eux @Spout Bonjour Spout, Peux tu te présenter en quelques mots ? Spout Geek passionné du Web et des nouvelles technologies, je joue souvent aux jeux vidéos. Quand je suis pas sur le PC je vais à la pêche et fait du Revolution Kite quand le vent est au rendez-vous. Comment es tu venu à python/django et que faisais tu avant de faire du python, depuis quand tu fais du python/Django ? Spout Quand j'ai commencé le développement Web, c'est évidemment vers le PHP que je me suis tourné (j'avais appris le C/C++ pendant mes études). Comme beaucoup de développeurs j'en suis venu à faire mon framework/CMS PHP. Ruby on Rails est sorti un peu plus tard, présenté comme LE framework révolutionnaire, je m'y suis un peu intéressé. Puis en réponse à RoR, il y a eu une émergence phénoménale des frameworks PHP. Mon choix s'est vite porté sur CakePHP qui reprenait les concepts de RoR, notamment le principe que j'apprécie \"Convention over configuration\". A mon premier emploi en tant que développeur Web (avant c'était juste en indépendant complémentaire où j'ai toujours eu le choix du tech stack), j'ai dû utiliser Zend Framework 1. Au fil du temps j'ai trouvé ce framework assez contre-productif et je cherchais autre chose pour développer en après journée. C'est à ce moment là que Foxmask m'a parlé de Django sur l'IRC #cakephp-fr. J'avais déjà fait le tuto officiel (polls) 2 ans auparavant, et ne connaissant pas Python je ne m'y étais guère attardé. Début 2013 j'ai refait le tuto polls, sans jamais avoir tapé la moindre ligne de Python auparavant et là j'ai beaucoup apprécié Django et Python. Le langage cohérent (needle/haystack ?), les conventions PEP8 (PSR est arrivé tard en PHP) , la doc super complète, batteries included, DRY, ... Et depuis je ne jure plus que par Python/Django, j'ai refait la plupart de mes projets perso avec et j'ai constaté un gain de productivité. A Quels projets open source participes tu ? Spout J'ai un projet (non terminé) de package pour Django qui reprend ce principe: https://djangosnippets.org/snippets/2309/ mais modifié pour utiliser les CBV. Quelles parties de Django / python utilises tu le plus ? (Et / Ou juste Quelle stack utilises tu le plus) Spout J'utilise tout le MVT de Django + Admin + Auth + Forms + I18n + Feeds + Sitemaps. En guise de Stack: Debian + Django + MySQL (migration vers PostgreSQL en cours) + Supervisor + Gunicorn + Nginx + Redis (cache backend) + Elasticsearch (Haystack) Quelle(s) lib tierce(s) a/ont ta préférence ? Spout Il y en a plusieurs que j'utilise régulièrement: django-braces: https://github.com/brack3t/django-braces django-crispy-forms: https://github.com/maraujop/django-crispy-forms django-haystack: http://haystacksearch.org django-allauth: https://github.com/pennersr/django-allauth easy-thumbnails: http://easy-thumbnails.readthedocs.org/en/2.1/ django-sekizai: https://github.com/ojii/django-sekizai django-compressor: https://github.com/django-compressor/django-compressor django-libsass: https://github.com/torchbox/django-libsass django-extra-views: https://github.com/AndrewIngram/django-extra-views django-modeltranslation: https://github.com/deschler/django-modeltranslation django-rosetta: https://github.com/mbi/django-rosetta micawber: http://micawber.readthedocs.org/en/latest/ django-countries: https://github.com/SmileyChris/django-countries Que fais tu aujourd'hui et quel avenir envisages tu avec toutes ces compétences ? Spout Je recherche un emploi en tant que développeur Web, en Python ce serait le rêve. Question déliruim : Demain tu croises un génie, tu as 1 voeu à exaucer, rencontrer l'une des ces 3 personnes : qui et pourquoi ? Guido Sam et Max Moi :-) Spout Foxmask pour aller boire une bière (belge) et partager notre passion. Merci pour ta participation ! Restons en contact si vous souhaitez entrer en contact avec le sieur Spout, faites moi le savoir via la page de contact , et je lui ferai suivre ;) sinon vous avez aussi son @compte twitter","tags":"Entrevues","url":"https://foxmask.net/post/2015/07/20/entrevue-en-toute-simplicite-spout/","loc":"https://foxmask.net/post/2015/07/20/entrevue-en-toute-simplicite-spout/"},{"title":"Revue Technique, Django Inside, semaine du 13/07/2015","text":"Pour ceux ce qui ne sont pas 'core zalaplage voici un petit bouquet fleuri estival ramassé mon fidèle destrier Tornado : Il y eu d'abord une teuf pour fêter les 10 piges de Django , si tu l'as pas vu, tu l'as ratée (hop la lapalissade:) Zensuite mon chemin a croisé ces 3 billets du même auteur Comment profiler ses middleware django : Comprendre comment produire les middleware django Debuter avec Celery et Redis Un petit The User is Dead par Sam et Max La prochaine DjangoCon sera aux USA le 6 septembre 2015 j'ai cherche un jeu de mots entre la ville où ça se passe et l'homme qui valait 3 milliard, ou, la ville et la bagnole, mais au final je me contenterai de vous dire que ça se passe à Austin, Texas More infos ici Last but not least : des fixes en veux tu en voilà Trigger Happy : Wazaaaaaa ??? un petit topo sur Trigger Happy vite fait pour dire que je suis sur le regroupement de tous les services existants dans une seule appli, qui avec tox/travis-ci sont en parfait accord ;) Ensuite je suis aussi sur une amélioration pour permettre de produire des flux rss à partir de n'importe quel service, l'idée c'est de palier aux orphelins de Yahoo Pipelines qui ferme ses portes le mois prochain. Enfin j'ai croisé de plus près Trello , ca avance . Si vous avez d'autres idées de services à plugger au projet ; du genre je voudrai avoir un mail quand la météo annonce une tempête de neige ; let me know, et je produirai un service \"Mail\" et un service \"Meteo\" et hop .","tags":"Techno","url":"https://foxmask.net/post/2015/07/17/revue-technique-django-inside-semaine-du-13-juillet-2015/","loc":"https://foxmask.net/post/2015/07/17/revue-technique-django-inside-semaine-du-13-juillet-2015/"},{"title":"Supervisor Celery Django : Orchestration","text":"Dans ce billet je vais aborder un triplet que chacun connait surement et a déjà dû le mettre en place, mais j'irai posé là, comme à mon habitude, un retour d'xp qui m'est propre et pourrait servir à de nouveaux venus sur django et l'administration de nos instances avec ce framework. une intro vite fait sur ces outils tout de même au cas où je parlerai chinois ;) Celery est un système distribué simple, flexible et fiable pour traiter de grandes quantités de messages, tout en fournissant des opérations avec les outils nécessaires pour maintenir un tel système. Supervisor est un système client / serveur qui permet à ses utilisateurs de surveiller et de contrôler un certain nombre de processus sur les systèmes d'exploitation de type UNIX. Comme tout à chacun (d'entre nous) on cherche à maintenir nos applications \"en vie\" en les faisant démarrer comme services et les animées par des tâches (récursives ou non). C'est là le but de ces outils ! Celery Donc pour démarrer on aura besoin dans son projet django de créer le fichier suivant permettant tout simplement d'aller à la pêche aux tâches des applications django indiquées dans le settings.INSTALLED_APPS. celery.py from __future__ import absolute_import import os from celery import Celery from celery.schedules import crontab from django.conf import settings os . environ . setdefault ( 'DJANGO_SETTINGS_MODULE' , 'mon_projet.settings' ) app = Celery ( 'mon_projet' ) app . config_from_object ( 'django.conf:settings' ) app . autodiscover_tasks ( lambda : settings . INSTALLED_APPS ) Ensuite on pourrait avoir besoin de tâches récurrentes qui doivent être définies dans le settings de son projet (et pas dans celery.py comme je l'ai cru en parcourant la doc celery) mon_projet/settings.py CELERYBEAT_SCHEDULE = { 'add-every-hour-cache' : { 'task' : 'mon_app.tasks.read_data' , 'schedule' : crontab ( minute = '27,54' ), }, 'add-every-hour-publish' : { 'task' : 'mon_app.tasks.publish_data' , 'schedule' : crontab ( minute = '59' ), }, } Bizarreries : Ici j'étais parti pour écrire, telle une vraie crontab, CELERYBEAT_SCHEDULE = { 'every-hour-put-in-cache' : { 'task' : 'mon_app.tasks.read_data' , 'schedule' : crontab ( minute = '*/27' ), }, 'add-every-hour-publish' : { 'task' : 'mon_app.tasks.publish_data' , 'schedule' : crontab ( minute = '*/60' ), }, } Seulement au lancement du \"beat\" je me suis mangé une erreur sur le contenu de minute qui doit être compris en 1 et 59... Or dans une crontab je peux tout aussi bien écrire */360 sur le range \" minutes \" si ca me chante :P Ensuite le */27 foire tout aussi bien. Le beat démarre à 27 minutes, puis 54 minutes .... et 00 ... Même comportement avec */59 (à la place de */60), il démarre bien à 59 minutes .... puis à 00 ... Enfin, ensuite voici un exemple de tâches qui pourrait être déclenchées à intervalle régulier mon_app/tasks.py from __future__ import unicode_literals from __future__ import absolute_import from celery import shared_task @shared_task def read_data (): .... @shared_task def publish_data (): ... au lancement du beat (qui gère la crontab) on obtient donc : $ celery - A th beat - l debug celery beat v3 . 1.18 ( Cipater ) is starting . __ - ... __ - _ Configuration -> . broker -> redis : // localhost : 6379 / 10 . loader -> celery . loaders . app . AppLoader . scheduler -> celery . beat . PersistentScheduler . db -> celerybeat - schedule . logfile -> [ stderr ] @% DEBUG . maxinterval -> now ( 0 s ) [ 2015 - 06 - 18 21 : 32 : 50 , 117 : DEBUG / MainProcess ] Setting default socket timeout to 30 [ 2015 - 06 - 18 21 : 32 : 50 , 117 : INFO / MainProcess ] beat : Starting ... [ 2015 - 06 - 18 21 : 32 : 50 , 134 : DEBUG / MainProcess ] Current schedule : [ 2015 - 06 - 18 21 : 32 : 50 , 134 : DEBUG / MainProcess ] beat : Ticking with max interval -> 5.00 minutes [ 2015 - 06 - 18 21 : 32 : 50 , 169 : DEBUG / MainProcess ] beat : Waking up in 5.00 minutes . le lancement du worker donne ceci à son tour $ celery - A th worker -- autoscale = 10 , 3 - l debug [ 2015 - 06 - 18 21 : 36 : 40 , 087 : DEBUG / MainProcess ] | Worker : Preparing bootsteps . [ 2015 - 06 - 18 21 : 36 : 40 , 094 : DEBUG / MainProcess ] | Worker : Building graph ... [ 2015 - 06 - 18 21 : 36 : 40 , 095 : DEBUG / MainProcess ] | Worker : New boot order : { Beat , Timer , Hub , Queues ( intra ), Pool , Autoscaler , StateDB , Autoreloader , Consumer } [ 2015 - 06 - 18 21 : 36 : 40 , 116 : DEBUG / MainProcess ] | Consumer : Preparing bootsteps . [ 2015 - 06 - 18 21 : 36 : 40 , 117 : DEBUG / MainProcess ] | Consumer : Building graph ... [ 2015 - 06 - 18 21 : 36 : 40 , 136 : DEBUG / MainProcess ] | Consumer : New boot order : { Connection , Agent , Events , Mingle , Gossip , Tasks , Control , Heart , event loop } [ 2015 - 06 - 18 21 : 36 : 40 , 140 : WARNING / MainProcess ] / home / sites / trigger - happy . eu / local / lib / python2 . 7 / site - packages / celery / apps / worker . py : 161 : CDeprecationWarning : Starting from version 3.2 Celery will refuse to accept pickle by default . -------------- celery @monserver v3 . 1.18 ( Cipater ) ---- **** ----- --- * *** * -- Linux - 2.6 . 32 - 042 stab106 . 4 - x86_64 - with - debian - 8.1 -- * - **** --- - ** ---------- [ config ] - ** ---------- .> app : th : 0x7f85f6883b50 - ** ---------- .> transport : redis : // localhost : 6379 / 10 - ** ---------- .> results : disabled - *** --- * --- .> concurrency : 2 ( prefork ) -- ******* ---- --- ***** ----- [ queues ] -------------- .> celery exchange = celery ( direct ) key = celery [ tasks ] . celery . backend_cleanup . celery . chain . celery . chord . celery . chord_unlock . celery . chunks . celery . group . celery . map . celery . starmap . mon_app . tasks . publish_data . mon_app . tasks . put_in_cache . mon_app . tasks . read_data [ 2015 - 06 - 18 21 : 36 : 40 , 165 : DEBUG / MainProcess ] | Worker : Starting Hub [ 2015 - 06 - 18 21 : 36 : 40 , 165 : DEBUG / MainProcess ] &#94;-- substep ok [ 2015 - 06 - 18 21 : 36 : 40 , 165 : DEBUG / MainProcess ] | Worker : Starting Pool [ 2015 - 06 - 18 21 : 36 : 40 , 177 : DEBUG / MainProcess ] &#94;-- substep ok [ 2015 - 06 - 18 21 : 36 : 40 , 178 : DEBUG / MainProcess ] | Worker : Starting Consumer [ 2015 - 06 - 18 21 : 36 : 40 , 179 : DEBUG / MainProcess ] | Consumer : Starting Connection [ 2015 - 06 - 18 21 : 36 : 40 , 206 : INFO / MainProcess ] Connected to redis : // localhost : 6379 / 10 [ 2015 - 06 - 18 21 : 36 : 40 , 206 : DEBUG / MainProcess ] &#94;-- substep ok [ 2015 - 06 - 18 21 : 36 : 40 , 207 : DEBUG / MainProcess ] | Consumer : Starting Events [ 2015 - 06 - 18 21 : 36 : 40 , 238 : DEBUG / MainProcess ] &#94;-- substep ok [ 2015 - 06 - 18 21 : 36 : 40 , 238 : DEBUG / MainProcess ] | Consumer : Starting Mingle [ 2015 - 06 - 18 21 : 36 : 40 , 238 : INFO / MainProcess ] mingle : searching for neighbors [ 2015 - 06 - 18 21 : 36 : 41 , 246 : INFO / MainProcess ] mingle : all alone [ 2015 - 06 - 18 21 : 36 : 41 , 247 : DEBUG / MainProcess ] &#94;-- substep ok [ 2015 - 06 - 18 21 : 36 : 41 , 247 : DEBUG / MainProcess ] | Consumer : Starting Gossip [ 2015 - 06 - 18 21 : 36 : 41 , 251 : DEBUG / MainProcess ] &#94;-- substep ok [ 2015 - 06 - 18 21 : 36 : 41 , 251 : DEBUG / MainProcess ] | Consumer : Starting Tasks [ 2015 - 06 - 18 21 : 36 : 41 , 258 : DEBUG / MainProcess ] &#94;-- substep ok [ 2015 - 06 - 18 21 : 36 : 41 , 258 : DEBUG / MainProcess ] | Consumer : Starting Control [ 2015 - 06 - 18 21 : 36 : 41 , 261 : DEBUG / MainProcess ] &#94;-- substep ok [ 2015 - 06 - 18 21 : 36 : 41 , 261 : DEBUG / MainProcess ] | Consumer : Starting Heart [ 2015 - 06 - 18 21 : 36 : 41 , 262 : DEBUG / MainProcess ] &#94;-- substep ok [ 2015 - 06 - 18 21 : 36 : 41 , 262 : DEBUG / MainProcess ] | Consumer : Starting event loop [ 2015 - 06 - 18 21 : 36 : 41 , 263 : WARNING / MainProcess ] celery @monserver ready . [ 2015 - 06 - 18 21 : 36 : 41 , 263 : DEBUG / MainProcess ] | Worker : Hub . register Pool ... [ 2015 - 06 - 18 21 : 36 : 41 , 264 : DEBUG / MainProcess ] basic . qos : prefetch_count -> 8 si on est joueur comme moi, on peut rajouter au lancement du worker un petit coup de --autoscale=10,3 $ celery - A th worker -- autoscale = 10 , 3 - l info ce qui fera qu'au démarrage du beat, il demarrera 3 workers d'un coup et si les ressources systemes le permettent, pourra monter à 10 workers d'un coup :) J'ai testé, c'est très efficace. Dans la liste des tasks de \"mon_app\" on voit que j'ai pas, deux tasks comme je l'ai montré, mais trois. En effet read_data lance put_in_cache , et c'est là que les 10 workers font la teuf parce que j'ai ecrit ce qui suit : @shared_task def read_data (): data = UnModele . objects . all () for stuff in data : put_in_cache . delay ( stuff ) C'est une task ( read_data ) qui en enchaine d'autres Ici, grosso modo ici je récupère tous les flux RSS de UnModele et pour chaque flux j'enclenche la mise en cache en rafale puisque 10 workers bossent pour moi. Pour finir l'histoire, du coup par la suite, publish_data recupere les données dans le cache (un Redis bien évidement) et les envoie à travers les services Twitter, Pocket, Evernote etc... Je vous laisse imaginer le gain de temps de traitement par rapport à un traitement sans cache et en série. Supervisor je n'aborderai pas ici toutes les possibilités de la bête mais juste comment intégrer Supervisor à des projets Django voici à quoi ca ressemble dans le fichier /etc/supervisor/conf.d/mon_projet.conf [ program : mon_projet_gunicorn ] user = foxmask ; User to run as directory = / home / sites / mon - projet / mon_projet command = / home / sites / mon - projet / bin / gunicorn_start ; Command to start app autostart = true autorestart = true redirect_stderr = true stdout_logfile = / home / sites / mon - projet / logs / gunicorn - start . log ; Where to write log messages stderr_logfile = / home / sites / mon - projet / logs / gunicorn - start - err . log ; Where to write log messages [ program : mon_projet_worker ] user = foxmask ; User to run as directory =/ home / sites / mon - projet / mon_projet command =/ home / sites / mon - projet / bin / celery - A th worker -- autoscale = 10 , 3 - l info autostart = true autorestart = true redirect_stderr = true stdout_logfile =/ home / sites / mon - projet / logs / mon_projet - worker . log stderr_logfile =/ home / sites / mon - projet / logs / mon_projet - worker - err . log [ program : mon_projet_beat ] user = foxmask ; User to run as directory =/ home / sites / mon - projet / mon_projet command =/ home / sites / mon - projet / bin / celery - A th beat - l info autostart = true autorestart = true redirect_stderr = true stdout_logfile =/ home / sites / mon - projet / logs / trigger - happy - beat . log stderr_logfile =/ home / sites / mon - projet / logs / trigger - happy - beat - err . log command gere le lancement du process cible directory est le dossier où on a mis le projet django autostart et autorestart permettent de lancer automatiquement la command quand supervisor démarre comme par exemple au demarrage du serveur, quant à autorestart, il lance aussi command après un plantage ou un kill du process pour Celery j'ai defini 2 program (un pour les workers un pour le beat) afin de séparer les logs et les process plutot que de lancer celery comme ceci : celery worker - B Ce qui n'est pas recommandé par la doc de Celery. un autre exemple de conf où j'ai utilisé supervisor : sentry : [ program : sentry - web ] user = foxmask directory =/ home / sites / sentry / command =/ home / sites / sentry / bin / sentry -- config =/ home / sites / sentry / conf / sentry . conf . py start autostart = true autorestart = true redirect_stderr = true stdout_logfile =/ home / sites / sentry / sentry . log stderr_logfile =/ home / sites / sentry / sentry - err . log [ program : sentry - worker ] user = foxmask directory =/ home / sites / sentry / command =/ home / sites / sentry / bin / sentry -- config =/ home / sites / sentry / conf / sentry . conf . py celery worker - B autostart = true autorestart = true redirect_stderr = true stdout_logfile =/ home / sites / sentry / sentry . log stderr_logfile =/ home / sites / sentry / sentry - err . log la façon de lancer le worker n'est pas recommandée mais utilise une version de celery plus ancienne. Alternatives à Celery Il existe bien d'autres alternatives à celery On peut aussi s'orienter vers des systèmes de queueing alternatifs listés ici Dont RQ: Simple job queues for Python par l' auteur du fameux git worklow Voire carrément exploiter redis directement Alternative à Supervisor On trouve Circus mais je n'ai pas réussi à le faire fonctionner (ca remonte à des mois)","tags":"Techno","url":"https://foxmask.net/post/2015/06/19/supervisor-celery-django-orchestration/","loc":"https://foxmask.net/post/2015/06/19/supervisor-celery-django-orchestration/"},{"title":"Revue Technique du 15/06/2015 : Django (again)","text":"Django arround ze world \" J'ai dix ans Si tu m'crois pas , tartagueule à la récré \" Donc Django a(ura) 10 ans (cet été) !?! L'occasion d'une teuf ! Django asynchrone ? Un sujet lancé par ici . Cela dit avec Wamp/Crossbar on a déjà matière pour s'éclater avec l'asynchrone et le temps réel ;) Mais sujet à surveiller de près From twitterLand Python inspire des tigres blancs et d'un tas de monstres tout gentils https://twitter.com/beaucouplus/status/611477044740915200 SnakeLand Une interview : Le dev de la semaine \"cocorico\" : Tarek Ziadé qui fut aussi le président de l'AFUP en passant ;) Dans la série \"pydev of the week\" \" dans les épisodes précédents \" vous trouverez tous les autres interviews avec les liens dans le pied de l'article. Un projet : LFU implémenté en python Least frequently used cache eviction scheme with complexity O(1) in Python ou 2 ;) Des closures pour un python + rapide ou tout du moins plus optimisé : Optimise Python with closures | Wrong Side of Memphis Snake Tips Pourquoi utilisé des namedtuple plutot que des tuple Monitoring Moniiiiiitooooooooooring ! Tant qu'à rester sur le monitoring, un article dans la lignée de Supervisor Celery Django : Orchestration , donc un article très intéressant sur le service Check my website par Nicolargo (l'article pas le service ;)","tags":"Techno","url":"https://foxmask.net/post/2015/06/19/revue-technique-du-15062015-django-again/","loc":"https://foxmask.net/post/2015/06/19/revue-technique-du-15062015-django-again/"},{"title":"Revue Technique de la semaine du 25/05/2015","text":"Sécu https://twitter.com/newsycombinator/status/603712716814057473 Dev World TDD Django Tuto PyPyJs un ovni ? Asynchronous Python and Databases Django Interview ... Questions ... and some answers gerer des fichiers de configuration en python sortie d'angularJS 1.4 Et toutes les erreurs commises à ce sujet : https://twitter.com/morlhon/status/603801656145371136 En est-ce une de ne m'y être toujours pas mis ? Je vous le dirai quand j'aurai commencé :-) WordPress avec un plugin Microsoft oneNote c'est possible ! TriggerHappy en python 3 only on ze road, J'en reparlerai plus en détails dans un billet dédié.","tags":"Techno","url":"https://foxmask.net/post/2015/05/29/revue-technique-de-la-semaine-du-25052015/","loc":"https://foxmask.net/post/2015/05/29/revue-technique-de-la-semaine-du-25052015/"},{"title":"Revue Technique de la semaine du 18/05/2015","text":"Au menu ici, un tuto sur Python, Twisted, MAT, Django Python Le tuto de Sam et Max https://twitter.com/sam_et_max/status/599840755734175744 Si le portage de Twisted sur Python 3 avance, il faut garder à l'esprit que \"derrière\" ces avancées, se cache des dev de Autobahn/WAMP/Crossbar qui use et abuse de Twisted pour ce projet ;) https://twitter.com/sam_et_max/status/600568610768162816 Et n'oublions pas une correction de sécurité de Django qui n'affecte que les versions 1.8 et supérieure https://twitter.com/djangoproject/status/601100599569027072 MAT \"Metadata Anonymisation Toolkit\", un projet, libre & écrit en python, qui se charge de rendre vos données anonymes, histoire de faire la nique aux dispositifs sensés filtrer et repérer vos données avec \" les fameuses sondes \" poser chez nos FAI et autres hébergeurs de contenus","tags":"Techno","url":"https://foxmask.net/post/2015/05/21/revue-technique-de-la-semaine-du-1805/","loc":"https://foxmask.net/post/2015/05/21/revue-technique-de-la-semaine-du-1805/"},{"title":"Debian 8 sans systemD mais avec le système D !","text":"Debian 8 installé après un upgrade foiré , me voici sur XFCE4 ;) après avoir shooté systemd , on redecouvre les joies de la ligne de commandes pour certaines choses aussi QQ que se connecter en VPN :) aptitude install openvpn suivi d'un coup de sudo openvpn foobar.ovpn Sat May 16 14 :06:56 2015 OpenVPN 2 .3.4 x86_64-pc-linux-gnu [ SSL ( OpenSSL )] [ LZO ] [ EPOLL ] [ PKCS11 ] [ MH ] [ IPv6 ] built on Dec 1 2014 Sat May 16 14 :06:56 2015 library versions: OpenSSL 1 .0.1k 8 Jan 2015 , LZO 2 .08 Enter Auth Username: Enter Auth Password: Sat May 16 14 :07:05 2015 UDPv4 link local: [ undef ] Sat May 16 14 :07:05 2015 UDPv4 link remote: [ AF_INET ] aaa.bbb.ccc.ddd:eee Sat May 16 14 :07:05 2015 WARNING: this configuration may cache passwords in memory -- use the auth-nocache option to prevent this Sat May 16 14 :07:06 2015 [ VPN ] Peer Connection Initiated with [ AF_INET ] aaa.bbb.ccc.ddd:eee Sat May 16 14 :07:09 2015 TUN/TAP device tun0 opened Sat May 16 14 :07:09 2015 do_ifconfig, tt->ipv6 = 0 , tt->did_ifconfig_ipv6_setup = 0 Sat May 16 14 :07:09 2015 /sbin/ip link set dev tun0 up mtu 1500 Sat May 16 14 :07:09 2015 /sbin/ip addr add dev tun0 local fff.ggg.hhh.iii peer fff.ggg.hhh.iii Sat May 16 14 :07:09 2015 Initialization Sequence Completed Un coup sur http://whatismyipaddress.com/ et on peut vérifier qu'on est bien ailleurs :P Un truc sympa avec XFCE4, les applications par défaut \"Navigateur Web\" et \"Client de messagerie\" sont liées à vos propres outils tel FireFox ou Thunderbird même s'ils sont dans votre \\$HOME/thunderbird \\$HOME/firefox. En quoi c'est sympa ? ben pas besoin de définir des shortcuts à la con ... comme avec Gnome 3 :P Par ailleurs quand on défini un nouveau lanceur, taper le début du nom du programme et il vous trouvera tout ce qui est dans \\$HOME, genre SublimeText pour ma part. XCFE4 c'est ZE système D(ébrouille|émerde) je reviendrai sur ce billet vous livrer le reste de quelques trucs sympas","tags":"Techno","url":"https://foxmask.net/post/2015/05/16/debian-8-sans-systemd-mais-avec-le-systeme-d/","loc":"https://foxmask.net/post/2015/05/16/debian-8-sans-systemd-mais-avec-le-systeme-d/"},{"title":"Debian Update Wheezy à Jessie : quand votre grub fait boom","text":"La mise à jour de Debian Wheezy à Jessie s'est passé comme le titre vous le laisse à penser, explosif. Au reboot, grub était vide... Le pied ! Etat des lieux avant avec Wheezy : PC en dualboot Win7/Debian, chacun son HDD le MBR est pris par le bootloader de Windows, là avant :P Debian démarre sur le boot de son propre disque Après l'installation, grub affichait simplement le prompt \"grub>\", et plus rien. Lors de l'installation j'ai refusé que GRUB se place sur le MBR sur 1° disque parce que je ne voulais absolument pas que Windows disparaisse. Actions entreprises pour tenter de retrouver le grub comme avant : Résultat KO : J'ai donc mis à jour (quand meme) win32-loader qui permet de booter linux depuis windows Résultat KO : J'ai réinstallé Debian uniquement sur la partition root sans toucher à /home Résultat KO : depuis le menu \"grub>\", la commande ls m'affiche ... le dossiers systeme de WINDOWS ! Trop beau ! Résultat KO : boot en mode rescue pour réparer grub en tapant update-grub Action qui a empiré les choses : boot en mode rescue pour réparer, et taper la commande : shell grub-install /dev/sda qui a flingue le MBR de mon windows ... au lieu de le faire pointer sur /dev/sdb J'ai rebooté et bien evidement, plus d'accès à windows dans le menu Donc retour au mode rescue j'ai créé un fichier /etc/grub.d/42_win contenant : menuentry \"Windows 7\" { insmod ntfs search --set = root --label WINDOWS_7 --hint hd0,msdos2 ntldr /bootmgr } ce qui m'a permit de cette fois ci rajouter windows au menu de grub apres un update-grub ! Création du fichier de configuration GRUB… Found background image: /usr/share/images/desktop-base/desktop-grub.png Image Linux trouvée : /boot/vmlinuz-3.16.0-4-amd64 Image mémoire initiale trouvée : /boot/initrd.img-3.16.0-4-amd64 Image Linux trouvée : /boot/vmlinuz-3.2.0-4-amd64 Image mémoire initiale trouvée : /boot/initrd.img-3.2.0-4-amd64 Windows 7 (loader) trouvé sur /dev/sda1 Windows Recovery Environment (loader) trouvé sur /dev/sda4 fait La doc que j'ai pu suivre Grub Multiboot , ainsi que GrubRecover Encore une mise à jour comme on les aime... edit @ 23h : HHAHAHAHAAAAAAAAAAAAAAAAAAaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa firefox et sublimtext qui petent .... l'installation avec un kernel 32bits sur un proc 64bits Ahaaaaaaaaaaaaaaaaaaaaaa le mode rescue ahaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa","tags":"Techno","url":"https://foxmask.net/post/2015/05/15/debian-update-wheezy-a-jessie-quand-votre-grub-fait-boom/","loc":"https://foxmask.net/post/2015/05/15/debian-update-wheezy-a-jessie-quand-votre-grub-fait-boom/"},{"title":"Quête WampWS et Autobahn - Episode 3 - Crossbar config","text":"Dans les épisodes précédants je vous ai montré qu'il était possible de débuter un projet from scratch rapidement, et comment 2 composants se parlaient. Donc dans la continuité, de ces 2 épisodes, ici nous allons aborder comment on configure crossbar, pour qu'il ait connaissance de nos components, dès son démarrage. Jusqu'à maintenant ( épisode 1 ) on a pu voir que les composants pouvaient être démarrés indépendemment et manuellement. A présent on va voir comment on les démarre au boot de crossbar. En intro à la \"configuration\" voici d'abord une version de l'épisode 1 \"amélioré\" qui va vous introduire des notions que l'on reportera dans la config de crossbar ensuite. dans chacun des 2 composants on avait : def onJoin ( self , details ): pool = txpostgres . ConnectionPool ( None , port = 5432 , database = 'th' , user = 'th' , password = 'th' ) yield pool . start () print ( 'DB Connection pool started' ) self . db = pool Or, mettre en dur des info de config dans le code, c'est sale. La version proprette donne plutôt ceci : def onJoin ( self , details ): dbconfig = self . config . extra [ 'database' ] self . db = txpostgres . Connection () try : yield self . db . connect ( ** dbconfig ) except Exception as e : print ( \"could not connect to database: {0} \" . format ( e )) self . leave () return else : print ( \"PostgreSQL adapter connected to database\" ) [ ... ] if __name__ == '__main__' : config = { 'database' : { 'port' : 5432 , 'database' : 'foobar' , 'user' : 'foobar' , 'password' : 'foobar' }, 'TIME_ZONE' : 'America/Chicago' , } log . startLogging ( sys . stdout ) from autobahn.twisted.wamp import ApplicationRunner runner = ApplicationRunner ( url = \"ws://127.0.0.1:8080/ws\" , realm = \"realm1\" , extra = config ) # app-level debugging runner . run ( RssComponent ) Et voilou. Dans le détails cela donne : dans le onJoin , on récupère self.config.extra qui provient du parm \"extra\" qu'on voit à la fin du code, qui est disponible grâce à crossbar. def onJoin ( self , details ): ... dbconfig = self . config . extra [ 'database' ] ... ensuite on passe dbconfig à la connexion yield self . db . connect ( ** dbconfig ) ce qui donne pour le compoent Rss de l'épisode 1 # -*- coding: utf-8 -*- import arrow import sys import time from datetime import datetime # postgresql driver from txpostgres import txpostgres # autobahn from twisted.python import log from autobahn import wamp from twisted.internet.defer import inlineCallbacks , returnValue from autobahn.twisted.util import sleep from autobahn.twisted.wamp import ApplicationSession # th_rss lib from lib.feedsservice import Feeds class RssComponent ( ApplicationSession ): \"\"\" An application component that publishes an event \"\"\" @inlineCallbacks def onJoin ( self , details ): # registering print ( \"RSS session attached\" ) self . register ( self ) dbconfig = self . config . extra [ 'database' ] self . db = txpostgres . Connection () try : yield self . db . connect ( ** dbconfig ) except Exception as e : print ( \"could not connect to database: {0} \" . format ( e )) self . leave () return else : print ( \"[th_rss] PostgreSQL adapter connected to database\" ) # publishing while True : feeds = yield self . call ( u 'eu.trigger-happy.rss.feeds_url' ) for data in feeds : for item in data [ 'data' ]: self . publish ( u 'eu.trigger-happy.pushit' , { 'trigger_id' : data [ 'trigger_id' ], 'user_id' : data [ 'user_id' ], 'item' : item }) yield sleep ( 120 ) @wamp . register ( u 'eu.trigger-happy.rss.feeds_url' ) @inlineCallbacks def get_feeds_url ( self ): \"\"\" get the URL stored in the database \"\"\" query = \"SELECT date_triggered, name, url, trigger_id, user_id FROM \" query += \"django_th_rss AS R, \" query += \"django_th_triggerservice AS TS \" query += \" WHERE R.trigger_id=TS.id AND TS.id=1 \" query += \" AND TS.consumer_id = \" # subquery to get only the RSS provider query += \" (SELECT id FROM django_th_servicesactivated WHERE \" query += \"name = 'ServiceRss' AND status = True) \" query += \" AND TS.status = True \" # get only the activated triggers query += \" ORDER BY TS.date_triggered DESC \" rows = yield self . db . runQuery ( query ) feeds = [] print ( 'get the feeds url...' ) for feed in rows : print ( 'get feeds from {0} => {1} ' . format ( feed [ 1 ], feed [ 2 ])) data_parms = { 'url' : feed [ 2 ], 'date_triggered' : feed [ 0 ]} if feed [ 0 ] <= self . right_now (): feeds . append ({ 'trigger_id' : feed [ 3 ], 'user_id' : feed [ 4 ], 'data' : self . reworked_feeds ( ** data_parms )}) returnValue ( feeds ) def right_now ( self ): \"\"\" :return: \"\"\" return arrow . utcnow () . replace ( hour = 0 , minute = 0 , second = 0 ) . to ( self . config . extra [ 'TIME_ZONE' ]) def reworked_feeds ( self , ** parms ): \"\"\" get the feeds and \"translate\" the time.struct_time() from Feedparser to a classical timestamp \"\"\" data = Feeds ( ** { 'url_to_parse' : parms [ 'url' ]}) . datas () what = None my_date_time = datetime . fromtimestamp ( time . mktime ( ( 1980 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 0 )) ) published = datetime . now () for feed in data : date_triggered = arrow . get ( str ( parms [ 'date_triggered' ]), 'YYYY-MM-DD HH:mm:ss' ) . to ( self . config . extra [ 'TIME_ZONE' ]) if 'published_parsed' in feed : what = 'published_parsed' published = feed [ 'published_parsed' ] my_date_time = datetime . utcfromtimestamp ( time . mktime ( feed [ 'published_parsed' ])) elif 'updated_parsed' in feed : what = 'updated_parsed' published = feed [ 'updated_parsed' ] my_date_time = datetime . utcfromtimestamp ( time . mktime ( feed [ 'updated_parsed' ])) if my_date_time >= date_triggered . naive : # this is this property that cant be serialize # so we have to translate it to a timestamp and # replace the actual value with the new one if what is not None : feed [ what ] = time . mktime ( published ) else : # drop that old data data . pop () return data if __name__ == '__main__' : config = { 'database' : { 'port' : 5432 , 'database' : 'foobar' , 'user' : 'foobar' , 'password' : 'foobar' }, 'TIME_ZONE' : 'America/Chicago' , } log . startLogging ( sys . stdout ) from autobahn.twisted.wamp import ApplicationRunner runner = ApplicationRunner ( url = \"ws://127.0.0.1:8080/ws\" , realm = \"realm1\" , extra = config ) # app-level debugging runner . run ( RssComponent ) A présent, je peux exécuter mon script en ligne de commande, il utilisera ce qui est défini après le if __name__ . Par rapport à la version de l'épisode 1 ca ne change pas grand chose au fait d'avoir mis en dur la conf dans le code, sauf, sauf que maintenant, je peux plugger mon component à crossbar, et c'est la conf définie dans crossbar qui prévaudra. Ce qui me permet du coup d'avoir toute liberté pour exécuter mon composant dans son coin avec sa conf de test et l'utiliser avec crossbar avec une conf de prod par exemple. Voyons maintenant comment ca se goupille dans crossbar Dans l'épisode 2 on en était resté à une configuration définissant l'accès à un simple composant Hello. Voyons ce que ca donne avec mon composant en plus : { \"controller\" : {}, \"workers\" : [ { \"type\" : \"router\" , \"realms\" : [ { \"name\" : \"realm1\" , \"roles\" : [ { \"name\" : \"anonymous\" , \"permissions\" : [ { \"uri\" : \"*\" , \"publish\" : true , \"subscribe\" : true , \"call\" : true , \"register\" : true } ] } ] } ], \"transports\" : [ { \"type\" : \"web\" , \"endpoint\" : { \"type\" : \"tcp\" , \"port\" : 8080 }, \"paths\" : { \"/\" : { \"type\" : \"static\" , \"directory\" : \"../web\" }, \"ws\" : { \"type\" : \"websocket\" } } } ] }, { \"type\" : \"container\" , \"options\" : { \"pythonpath\" : [ \"..\" ] }, \"components\" : [ { \"type\" : \"class\" , \"classname\" : \"components.th_rss.back.RssComponent\" , \"realm\" : \"realm1\" , \"transport\" : { \"type\" : \"websocket\" , \"endpoint\" : { \"type\" : \"tcp\" , \"host\" : \"127.0.0.1\" , \"port\" : 8080 }, \"url\" : \"ws://127.0.0.1:8080/ws\" }, \"extra\" : { \"database\" : { \"host\" : \"127.0.0.1\" , \"port\" : 5432 , \"database\" : \"foobar\" , \"user\" : \"foobar\" , \"password\" : \"foobar\" } } }, { \"type\" : \"class\" , \"classname\" : \"components.th_evernote.front.EvernoteComponent\" , \"realm\" : \"realm1\" , \"transport\" : { \"type\" : \"websocket\" , \"endpoint\" : { \"type\" : \"tcp\" , \"host\" : \"127.0.0.1\" , \"port\" : 8080 }, \"url\" : \"ws://127.0.0.1:8080/ws\" }, \"extra\" : { \"database\" : { \"host\" : \"127.0.0.1\" , \"port\" : 5432 , \"database\" : \"foobar\" , \"user\" : \"foobar\" , \"password\" : \"foobar\" } } } ] } ] } avec cette configuration, dès le lancement de crossbar, il chargera les 2 composants qui demarreront respectivement le code se trouvant dans le onJoin la valeur de classname est à rallonge et est un classique chemin python. Cette config crossbar tient compte d'un fait important : mes composants sont dans le path de crossbar et donc sont en python 2.7 ceci m'est révélé par le paramètre \"options\" : { \"pythonpath\" : [ \"..\" ] } , Par contre, ce n'est pas parceque Crossbar n'est pas en python 3, que Autobahn|Python ne l'est pas, donc je peux tout à fait avoir des composants python 3 ailleurs, et si je veux que crossbar me charge mes composants, je lui fourni le path vers ceux ci comme suit (par exemple) : \"options\" : { \"pythonpath\" : [ \"..\" , \"/home/foxmask/Django-Virtualenv/wamp-th/wamp-th/\" ] } , et ça ne dérangera pas Crossbar pour un sou (au contraire:) un goodies en plus, on peut nommer le process qui se charge de gerer mes components, pour cela j'ajoute à mon parm \"options\", \"title\" comme suit : \"options\" : { \"title\" : \"triggerhappy\" , \"pythonpath\" : [ \"..\" , \"/home/foxmask/Django-Virtualenv/wamp-th/wamp-th/\" ] } , enfin on trouve bien évidement les mêmes noeuds \"config/extras\" qu'indiqués dans le if __name__ de chacun des composants pour définir une base de données différente (si besoin). Voici donc qui clos cet épisode 3. Je n'ai pas abordé toutes les options cette fois ci, je vous avouerai que je vous livre ici ce que j'ai découvert, qui est tout frais, mais je n'hésiterai pas à faire un billet de plus sur les nouveautés que j'aurai pu exploiter. L'épisode 4 sera sur le mode debug de crossbar","tags":"Techno","url":"https://foxmask.net/post/2015/05/12/quete-wampws-et-autobahn-episode-3-crossbar-config/","loc":"https://foxmask.net/post/2015/05/12/quete-wampws-et-autobahn-episode-3-crossbar-config/"},{"title":"OpenBSD : configurer le wifi","text":"Hey ! Comme à mon habitude, je mets là mes déboires/résolutions de problèmes de noob, pour les prochains noob passant par là ... Etat des lieux Découvrant OpenBSD, mis sur un laptop depuis 1 semaine, le besoin premier : accèder au net via le wifi. Par defaut j'avais pu configurer l'interface ethernet, mais se trimbaler le fil rj45 ... Donc bon... La config de départ sur BSD il s'avere que la partie gerant la config réseau se trouve comme chacun le sait (sauf moi depuis 5min :) dans /etc/hostname. Donc ayant déjà /etc/hostname.bge0 contenant la seule ligne dhcp, je l'ai copié en /etc/hostname.iwi0, qui est mon interface wifi. Ensuite il faut faire : ifconfig iwi0 down ifconfig iwi0 nwid \"Livebox-xxxx\" wpakey \"xxxxxxxxx\" ifconfig iwi0 up dhclient iwi0 et voilà :)","tags":"Techno","url":"https://foxmask.net/post/2015/05/08/openbsd-configurer-le-wifi/","loc":"https://foxmask.net/post/2015/05/08/openbsd-configurer-le-wifi/"},{"title":"Quête WampWS et Autobahn : Épisode 2 - démarrage en trombe","text":"Voici la suite de la quête WampWS et Autobahn (relire l'épisode 1) , dans cet épisode 2, une présentation permettant de démarrer, en deux coups les gros, un projet crossbar pour autobahn|python foxmask @foxmask : ~ $ virtualenv crossbar New python executable in crossbar / bin / python Installing distribute ............................................................................................................................................................................................. done . Installing pip ............... done . foxmask @foxmask : ~ $ cd crossbar / foxmask @foxmask : ~/ crossbar $ ls bin include lib local foxmask @foxmask : ~/ crossbar $ source bin / activate ( crossbar ) foxmask @foxmask : ~/ crossbar $ pip install crossbar Downloading / unpacking crossbar Downloading crossbar - 0.10 . 4. tar . gz ( 171 Kb ): 171 Kb downloaded Running setup . py egg_info for package crossbar [ ... ] résultat (non des moindres ;) ( crossbar ) foxmask @foxmask : ~/ crossbar $ pip freeze -- local Jinja2 == 2.7 . 3 MarkupSafe == 0.23 PyTrie == 0.2 PyYAML == 3.11 Pygments == 2.0 . 2 Twisted == 15.1 . 0 autobahn == 0.10 . 3 cffi == 1.0 . 0 b2 characteristic == 14.3 . 0 crossbar == 0.10 . 4 cryptography == 0.8 . 2 distribute == 0.6 . 24 enum34 == 1.0 . 4 mistune == 0.5 . 1 netaddr == 0.7 . 14 pyOpenSSL == 0.15 . 1 pyasn1 == 0.1 . 7 pyasn1 - modules == 0.0 . 5 pycparser == 2.12 requests == 2.7 . 0 service - identity == 14.0 . 0 shutilwhich == 1.1 . 0 six == 1.9 . 0 treq == 15.0 . 0 txaio == 1.0 . 0 zope . interface == 4.1 . 2 à présent on demarre un projet comme on le ferait avec django-admin.py de Django: ( crossbar ) foxmask @foxmask : ~/ crossbar $ crossbar init -- template hello : python -- app $ PWD / letsgodancing Crossbar . io application directory '/home/foxmask/crossbar/letsgodancing' created Initializing application template 'hello:python' in directory '/home/foxmask/crossbar/letsgodancing' Using template from '/home/foxmask/crossbar/local/lib/python2.7/site-packages/crossbar/templates/hello/python' Creating directory / home / foxmask / crossbar / letsgodancing / hello Creating directory / home / foxmask / crossbar / letsgodancing /. crossbar Creating file / home / foxmask / crossbar / letsgodancing / README . md Creating file / home / foxmask / crossbar / letsgodancing / setup . py Creating file / home / foxmask / crossbar / letsgodancing / MANIFEST . in Creating directory / home / foxmask / crossbar / letsgodancing / hello / web Creating file / home / foxmask / crossbar / letsgodancing / hello / hello . py Creating file / home / foxmask / crossbar / letsgodancing / hello / __init__ . py Creating file / home / foxmask / crossbar / letsgodancing / hello / web / index . html Creating file / home / foxmask / crossbar / letsgodancing /. crossbar / config . json Application template initialized To start your node , run 'crossbar start --cbdir /home/foxmask/crossbar/letsgodancing/.crossbar' Et donc on n'a plus qu'à s'exécuter comme le suggère la dernière ligne ci dessus ( crossbar ) foxmask @foxmask : ~/ crossbar $ crossbar start -- cbdir / home / foxmask / crossbar / letsgodancing /. crossbar 2015 - 05 - 07 22 : 08 : 42 + 0200 [ Controller 4481 ] Log opened . 2015 - 05 - 07 22 : 08 : 42 + 0200 [ Controller 4481 ] ==================== Crossbar . io ==================== 2015 - 05 - 07 22 : 08 : 42 + 0200 [ Controller 4481 ] Crossbar . io 0.10 . 4 starting 2015 - 05 - 07 22 : 08 : 42 + 0200 [ Controller 4481 ] Running on CPython using EPollReactor reactor 2015 - 05 - 07 22 : 08 : 42 + 0200 [ Controller 4481 ] Starting from node directory / home / foxmask / crossbar / letsgodancing /. crossbar 2015 - 05 - 07 22 : 08 : 42 + 0200 [ Controller 4481 ] Starting from local configuration '/home/foxmask/crossbar/letsgodancing/.crossbar/config.json' 2015 - 05 - 07 22 : 08 : 42 + 0200 [ Controller 4481 ] Warning , could not set process title ( setproctitle not installed ) 2015 - 05 - 07 22 : 08 : 42 + 0200 [ Controller 4481 ] Warning : process utilities not available 2015 - 05 - 07 22 : 08 : 42 + 0200 [ Controller 4481 ] Router created for realm 'crossbar' 2015 - 05 - 07 22 : 08 : 42 + 0200 [ Controller 4481 ] No WAMPlets detected in enviroment . 2015 - 05 - 07 22 : 08 : 42 + 0200 [ Controller 4481 ] Starting Router with ID 'worker1' .. 2015 - 05 - 07 22 : 08 : 42 + 0200 [ Controller 4481 ] Entering reactor event loop ... 2015 - 05 - 07 22 : 08 : 42 + 0200 [ Router 4484 ] Log opened . 2015 - 05 - 07 22 : 08 : 42 + 0200 [ Router 4484 ] Warning : could not set worker process title ( setproctitle not installed ) 2015 - 05 - 07 22 : 08 : 43 + 0200 [ Router 4484 ] Running under CPython using EPollReactor reactor 2015 - 05 - 07 22 : 08 : 43 + 0200 [ Router 4484 ] Entering event loop .. 2015 - 05 - 07 22 : 08 : 43 + 0200 [ Router 4484 ] Warning : process utilities not available 2015 - 05 - 07 22 : 08 : 43 + 0200 [ Controller 4481 ] Router with ID 'worker1' and PID 4484 started 2015 - 05 - 07 22 : 08 : 43 + 0200 [ Controller 4481 ] Router 'worker1' : PYTHONPATH extended 2015 - 05 - 07 22 : 08 : 43 + 0200 [ Controller 4481 ] Router 'worker1' : realm 'realm1' ( named 'realm1' ) started 2015 - 05 - 07 22 : 08 : 43 + 0200 [ Controller 4481 ] Router 'worker1' : role 'role1' ( named 'anonymous' ) started on realm 'realm1' 2015 - 05 - 07 22 : 08 : 43 + 0200 [ Router 4484 ] Site starting on 8080 2015 - 05 - 07 22 : 08 : 43 + 0200 [ Controller 4481 ] Router 'worker1' : transport 'transport1' started 2015 - 05 - 07 22 : 08 : 43 + 0200 [ Controller 4481 ] Starting Container with ID 'worker2' .. 2015 - 05 - 07 22 : 08 : 43 + 0200 [ Container 4493 ] Log opened . 2015 - 05 - 07 22 : 08 : 43 + 0200 [ Container 4493 ] Warning : could not set worker process title ( setproctitle not installed ) 2015 - 05 - 07 22 : 08 : 44 + 0200 [ Container 4493 ] Running under CPython using EPollReactor reactor 2015 - 05 - 07 22 : 08 : 44 + 0200 [ Container 4493 ] Entering event loop .. 2015 - 05 - 07 22 : 08 : 44 + 0200 [ Container 4493 ] Warning : process utilities not available 2015 - 05 - 07 22 : 08 : 44 + 0200 [ Controller 4481 ] Container with ID 'worker2' and PID 4493 started 2015 - 05 - 07 22 : 08 : 44 + 0200 [ Controller 4481 ] Container 'worker2' : PYTHONPATH extended 2015 - 05 - 07 22 : 08 : 44 + 0200 [ Controller 4481 ] Container 'worker2' : component 'component1' started 2015 - 05 - 07 22 : 08 : 44 + 0200 [ Container 4493 ] subscribed to topic 'onhello' : Subscription ( id = 400001378 , is_active = True ) 2015 - 05 - 07 22 : 08 : 44 + 0200 [ Container 4493 ] procedure add2 () registered : 2015 - 05 - 07 22 : 08 : 44 + 0200 [ Container 4493 ] published to 'oncounter' with counter 0 2015 - 05 - 07 22 : 08 : 45 + 0200 [ Container 4493 ] published to 'oncounter' with counter 1 2015 - 05 - 07 22 : 08 : 46 + 0200 [ Container 4493 ] published to 'oncounter' with counter 2 2015 - 05 - 07 22 : 08 : 47 + 0200 [ Container 4493 ] published to 'oncounter' with counter 3 2015 - 05 - 07 22 : 08 : 48 + 0200 [ Container 4493 ] published to 'oncounter' with counter 4 Petite explication de texte ici ! On a démarré le \"projet\" sans voir une ligne de code: wtf ? la ligne crossbar init -- template hello : python nous a produit, comme je le disais à la manière de django-admin.py, une arbo \"type\", avec une appli toute simple ( crossbar ) foxmask @foxmask : ~/ crossbar $ ls - lR letsgodancing / letsgodancing / : total 16 drwxr - xr - x 3 foxmask foxmask 4096 mai 7 22 : 08 hello - rw - r -- r -- 1 foxmask foxmask 30 mai 7 22 : 07 MANIFEST . in - rw - r -- r -- 1 foxmask foxmask 136 mai 7 22 : 07 README . md - rw - r -- r -- 1 foxmask foxmask 1859 mai 7 22 : 07 setup . py letsgodancing / hello : total 20 - rw - r -- r -- 1 foxmask foxmask 3205 mai 7 22 : 07 hello . py - rw - r -- r -- 1 foxmask foxmask 1913 mai 7 22 : 08 hello . pyc - rw - r -- r -- 1 foxmask foxmask 1569 mai 7 22 : 07 __init__ . py - rw - r -- r -- 1 foxmask foxmask 141 mai 7 22 : 08 __init__ . pyc drwxr - xr - x 2 foxmask foxmask 4096 mai 7 22 : 07 web letsgodancing / hello / web : total 8 - rw - r -- r -- 1 foxmask foxmask 5085 mai 7 22 : 07 index . html dans letsgodancing/hello/hello.py on trouve notre application autobahn qui comme on le voit à la fin des logs ci dessus, affiche un compteur qui s'incrémente toutes les secondes : class AppSession ( ApplicationSession ): @inlineCallbacks def onJoin ( self , details ): # SUBSCRIBE to a topic and receive events # def onhello ( msg ): print ( \"event for 'onhello' received: {} \" . format ( msg )) sub = yield self . subscribe ( onhello , 'com.example.onhello' ) print ( \"subscribed to topic 'onhello': {} \" . format ( sub )) # REGISTER a procedure for remote calling # def add2 ( x , y ): print ( \"add2() called with {} and {} \" . format ( x , y )) return x + y reg = yield self . register ( add2 , 'com.example.add2' ) print ( \"procedure add2() registered: {} \" . format ( reg )) # PUBLISH and CALL every second .. forever # counter = 0 while True : # PUBLISH an event # yield self . publish ( 'com.example.oncounter' , counter ) print ( \"published to 'oncounter' with counter {} \" . format ( counter )) counter += 1 # CALL a remote procedure # try : res = yield self . call ( 'com.example.mul2' , counter , 3 ) print ( \"mul2() called with result: {} \" . format ( res )) except ApplicationError as e : # ignore errors due to the frontend not yet having # registered the procedure we would like to call if e . error != 'wamp.error.no_such_procedure' : raise e yield sleep ( 1 ) je ne vais pas vous faire peur tout de suite (krkrkrkrkr), juste vous la faire courte. comment ce code est lancé par crossbar ? lors du lancement de crossbar start --cbdir /home/foxmask/crossbar/letsgodancing/.crossbar crossbar lit le fichier config.json de ce dossier, config.json qui contient la config de l'application hello ci dessus. le voici : { \"controller\" : { }, \"workers\" : [ { \"type\" : \"router\" , \"options\" : { \"pythonpath\" : [ \"..\" ] }, \"realms\" : [ { \"name\" : \"realm1\" , \"roles\" : [ { \"name\" : \"anonymous\" , \"permissions\" : [ { \"uri\" : \"*\" , \"publish\" : true , \"subscribe\" : true , \"call\" : true , \"register\" : true } ] } ] } ], \"transports\" : [ { \"type\" : \"web\" , \"endpoint\" : { \"type\" : \"tcp\" , \"port\" : 8080 }, \"paths\" : { \"/\" : { \"type\" : \"static\" , \"directory\" : \"../hello/web\" }, \"ws\" : { \"type\" : \"websocket\" } } } ] }, { \"type\" : \"container\" , \"options\" : { \"pythonpath\" : [ \"..\" ] }, \"components\" : [ { \"type\" : \"class\" , \"classname\" : \"hello.hello.AppSession\" , \"realm\" : \"realm1\" , \"transport\" : { \"type\" : \"websocket\" , \"endpoint\" : { \"type\" : \"tcp\" , \"host\" : \"127.0.0.1\" , \"port\" : 8080 }, \"url\" : \"ws://127.0.0.1:8080/ws\" } } ] } ] } Une fois crossbar démarré, l'application démarre sur onJoin et exécute le tout (que je vous décrirai dans un prochain billet) pour arriver au while true qui affiche le fameux compteur. Ainsi paré on est fin prêt à broder pour attaquer de nouvelles applications autobahn, aussi appelé \"component\". Dans le prochain épisode je vous parlerai du fameux config.json où on verra ce qu'on y mettre de plus et comment ! Si vous trépigner d'impatience ou avez des questions plus vaste (ou carrément spécifique) viendez me rejoindre avec quelques centaines d'autres ;)","tags":"Techno","url":"https://foxmask.net/post/2015/05/07/quete-wampws-et-autobahn-episode-2-demarrage-en-trombe/","loc":"https://foxmask.net/post/2015/05/07/quete-wampws-et-autobahn-episode-2-demarrage-en-trombe/"},{"title":"Gnome SystemD, Debian 8, *BSD","text":"Il était une fois une distrib que j'utilise depuis plus de 10ans ... et qui, comme elle prend un chemin de traverse qui ne me sied guère, va dégager de mes ordinateurs... Comme je ne suis qu'une poussière l'ocean, les dev de debian s'en foutent bien largement de mon avis puisque leur choix est fait. C'est bien justement parce qu'ils s'en foutent que je m'en vais vous dire tout le bien que j'en pense. Le chemin de traverse est systemd, le super génialissime \"administrateur de système et de services\" Qu'est-ce qui me déplait tant dans ce truc ? ) il provient de RedHat ... ) il a été poussé par Gnome ... ) Debian n'a pas daigné conserver la compatibilité avec SysV ) les logs en binaires : ca pu ) Linus Torvalds n'aime pas ca non plus , même s'il dit que ce n'est pas un si gros problème... ) le pauvre petite canard veut nous faire pleurer... ) Google inside ) le fait que systemd apparaisse sur Freedesktop.org, dénote d'un fait indéniable, tous les window manager qui passeraient par le projet Freedesktop adhéreraient à l'utilisation de Systemd... Et j'en passe et des meilleures. Donc j'ai passé un moment ce weekend à faire des tests d'installations : Mettre à jour Debian 7.8 à 8 puis virer systemd Installer Debian 8 \"KDE\" Installer FreeBSD 10 Installer OpenBSD 5.7 Dans le premier cas : virer systemd désinstalle gnome intégralement. Youpi ! ... Ca rend le desktop dans un état ... Dans le second cas : virer systemd ok mais remettre KDE .... pffff FreeBSD ne detecte pas mon firmware wifi du portable... J'ai quand meme pu finir par faire mon installation avec un cable ethernet, et démarrer GDM, DBUS et je ne sais plus quel service, mais une fois sur le GDM, impossible de récupérer une console par CTRL-ALT-1 ou 2 ou 3, la carte graphique par en sucette. Quand je me connecte \"normalement\" depuis GDM, le desktop freeze par intermitence... Ca rend le portable inutilisable... OpenBSD, après des heures de téléchargement pour installer gnome, puis xorg, pas moyen de lancer GDM ou gnome-session. J'ai juste eu droit au window manager par defaut : fvwm2. Comme je galère pour passer de Linux à BSD, et comme la communauté Linux, n'est pas tendre en vous sortant des RTFM à tirelarigot, je ne me suis même pas essayé à demander de l'aide sur BSD. Du coup j'ai remis une debian 7.8 en attendant un jour meilleur pour peut-etre tenter une nouvelle installation BSD. J'ai un an avant la fin du support de Wheezy :P Cela dit comme systemd est fortement lié au window manager, j'ai l'impression que les BSD et Linux sont mal partis. Si un window manager veut perdurer dans une distrib, il va lui falloir passer à systemd pour exister. Et si des OS ne s'y mettent pas qu'est ce qu'il leur arrivera ? Je ne suis pas un famillier de BSD mais si Gnome&KDE utilise systemd, comment les BSD vont ils gérer ? Le desktop sous fvwm2 et xfce ... Probable que les BSD ne sont pas pour le desktop après tout, que c'est pas pour les non adminsys. Mais la console ca va 5min hein, lire ses mails avec mutt, en passant par procmail/fetchmail, sendmail/postfix un moment ca lasse, thunderbird c'est pratique :P Mon utilisation est simple: je produis des appli web ou pas avec python et un IDE. Je veux pouvoir utiliser sur mes pc @ home ce que j'aurai en prod. Être \"iso-conf\", de sorte que si ca merde quelque part, que je n'ai pas à remettre en question l'OS et la version des services en place. C'est peut-être une déformation professionnelle mais au moins ça permet de ne pas se poser de question lors de la mise en prod. Et vous que pensez vous de tout ça ? Debian toujours ? BSD > Debian/Linux ? BSD est fait pour le desktop ? si oui lequel et avec quel window manager périn ? En gros je me pose un tas de questions... et c'est pas fini","tags":"Techno","url":"https://foxmask.net/post/2015/05/03/gnome-systemd-debian-8-bsd/","loc":"https://foxmask.net/post/2015/05/03/gnome-systemd-debian-8-bsd/"},{"title":"Django Trigger Happy 0.10.x and his friends","text":"I released a version 0.10.1 of the Trigger Happy project with : News : Search engine based on Haystack (when we have more than 30 triggers that begins to be useful:) a Holidays mode, permits to pause all the triggers until you return, thus you will be really \"disconnected\" during your holidays :) Improvments : support Django 1.8 support python 3 support of some not well handled accented characters now fixed with a html_entities php like contribution from Adrihein on the layout of the application His friends too : module Twitter : /!\\ a column changed of type : bigint vs int module Evernote : nothing special module Pocket : nothing special module RSS : nothing special module Readability : nothing special module Dummy : nothing special Next : I was not very productive on the version because in the meantime, I have lingered over Crossbar.io / WAMP.WS / Autobahn to produce a sandbox wamp-th . This one works great ! Some details need to be fixed and that should be ok ;) Once it's done, that should replace the \"managment commands\" which are actually put in a crontab. This should permit to speed up Trigger Happy again and make all funs interactions :) Have Fun ! where to find the project where to find the sources where to find the doc ps : I told myself there was no reason to not publish a post in English, as the majority of you on github dont come from France ;)","tags":"Techno","url":"https://foxmask.net/post/2015/04/23/django-trigger-happy-0-10-x-and-his-friends/","loc":"https://foxmask.net/post/2015/04/23/django-trigger-happy-0-10-x-and-his-friends/"},{"title":"Django Trigger Happy 0.10.x et ses amis","text":"Le projet Triger Happy sort en version 0.10.1 avec au programme : Nouveautés : Moteur de recherche basé sur haystack (quand on a plus de 30 triggers ca commence à en faire ;) Un mode vacances , permettant de mettre en pause tous les triggers jusqu'à votre retour de congés, histoire de vivre vraiment déconnecté ses vacances ;) Améliorations : support de Django 1.8 support python 3 support des caractères accentués mal affichés avec un html_entities php like contribution de Adrihein sur le layout de l'application Ses amis à jour aussi : module Twitter : /!\\ ici une colonne change de type : bigint vs int module Evernote : ras module Pocket : ras module RSS : ras module Readability : ras module Dummy : ras La suite : je n'ai pas été hyper productif sur cette version parce que dans le même temps, je me suis longuement attardé sur Crossbar.io / WAMP.WS / Autobahn pour faire un bac à sable wamp-th . Ce dernier marche parfaitement ! Encore des détails mais apres ca devrait etre au point. In fine ca devrait remplacer les \"managment commands\" qui sont actuellement mises dans une crontab. Ca devrait permettre d'encore booster Trigger Happy et rendre toutes les interactions fun ;) Amusez vous bien ! Où trouver le projet Où trouver les sources Où trouver la doc","tags":"Techno","url":"https://foxmask.net/post/2015/04/21/django-trigger-happy-0-10-x-et-ses-amis/","loc":"https://foxmask.net/post/2015/04/21/django-trigger-happy-0-10-x-et-ses-amis/"},{"title":"pypi nous saoule avec son erreur 400","text":"Bon alors un truc bien pénible avec PYPI ; l'erreur 400 Upload failed ( 400 ) : This filename has previously been used, you should use a different version. Je pense qu'ils n'ont pas réfléchi à l'impact très néfaste. Je maintiens tout juste 8 modules python ; dont 7 dépendent de django-th. Si je me goure d'une virgule dans ma doc et renvoie la même version je suis blackboulé et dois faire une version n+1 J'ai bien évidemment supprimer la version qui était concernée par ma coquille mais ca ne suffit pas, j'ai du coup supprimé le projet entier pour le renvoyer ; RIEN Y FAIT ! Or : cette manière de faire est récente, car lors de mes publications de novembre, je n'ai pas eu ce problème. imposer cette façon de gérer m'impose de reprendre 8 modules, de changer la version de chacun ... POUR RIEN ! Il n'y a eu AUCUNE COMMUNICATION sur le sujet de cette mesure... Je n'ose pas imaginer ceux qui en gèrent bien plus que moi... A quoi ca encourage à part se passer de pypi ?","tags":"Techno","url":"https://foxmask.net/post/2015/04/19/pypi-nous-saoule-avec-son-erreur-400/","loc":"https://foxmask.net/post/2015/04/19/pypi-nous-saoule-avec-son-erreur-400/"},{"title":"Django Migration 1.7 à 1.8, FormWizard, InlineFormSet, Url from Future","text":"Même si la la doc est superbement complète, je m'en vais vous narrer ci et là les surprises qui ont jonché le chemin de ma migration Comme tout à chacun j'ai donc fait : pip install - U django ... ./ manage . py test Je suis tombé sur des erreurs sur django-debug-toolbar parce que resté en 0.9.5, donc estompées dès la 1.3.0 mise à jour. Puis vint des suées : Traceback ( most recent call last ): File \"./manage.py\" , line 10 , in execute_from_command_line ( sys . argv ) File \"/home/foxmask/Django-VirtualEnv/django-trigger-happy/local/lib/python2.7/site-packages/django/core/management/__init__.py\" , line 338 , in execute_from_command_line utility . execute () File \"/home/foxmask/Django-VirtualEnv/django-trigger-happy/local/lib/python2.7/site-packages/django/core/management/__init__.py\" , line 312 , in execute django . setup () File \"/home/foxmask/Django-VirtualEnv/django-trigger-happy/local/lib/python2.7/site-packages/django/__init__.py\" , line 18 , in setup apps . populate ( settings . INSTALLED_APPS ) File \"/home/foxmask/Django-VirtualEnv/django-trigger-happy/local/lib/python2.7/site-packages/django/apps/registry.py\" , line 115 , in populate app_config . ready () File \"/home/foxmask/Django-VirtualEnv/django-trigger-happy/local/lib/python2.7/site-packages/debug_toolbar/apps.py\" , line 15 , in ready dt_settings . patch_all () File \"/home/foxmask/Django-VirtualEnv/django-trigger-happy/local/lib/python2.7/site-packages/debug_toolbar/settings.py\" , line 232 , in patch_all patch_root_urlconf () File \"/home/foxmask/Django-VirtualEnv/django-trigger-happy/local/lib/python2.7/site-packages/debug_toolbar/settings.py\" , line 220 , in patch_root_urlconf reverse ( 'djdt:render_panel' ) File \"/home/foxmask/Django-VirtualEnv/django-trigger-happy/local/lib/python2.7/site-packages/django/core/urlresolvers.py\" , line 550 , in reverse app_list = resolver . app_dict [ ns ] File \"/home/foxmask/Django-VirtualEnv/django-trigger-happy/local/lib/python2.7/site-packages/django/core/urlresolvers.py\" , line 352 , in app_dict self . _populate () File \"/home/foxmask/Django-VirtualEnv/django-trigger-happy/local/lib/python2.7/site-packages/django/core/urlresolvers.py\" , line 285 , in _populate for pattern in reversed ( self . url_patterns ): File \"/home/foxmask/Django-VirtualEnv/django-trigger-happy/local/lib/python2.7/site-packages/django/core/urlresolvers.py\" , line 402 , in url_patterns patterns = getattr ( self . urlconf_module , \"urlpatterns\" , self . urlconf_module ) File \"/home/foxmask/Django-VirtualEnv/django-trigger-happy/local/lib/python2.7/site-packages/django/core/urlresolvers.py\" , line 396 , in urlconf_module self . _urlconf_module = import_module ( self . urlconf_name ) File \"/usr/lib/python2.7/importlib/__init__.py\" , line 37 , in import_module __import__ ( name ) File \"/home/foxmask/Django-VirtualEnv/django-trigger-happy/django_th/django_th/urls.py\" , line 9 , in from django_th.views import TriggerListView , TriggerDeleteView , File \"/home/foxmask/Django-VirtualEnv/django-trigger-happy/django_th/django_th/views.py\" , line 13 , in from django.contrib.formtools.wizard.views import SessionWizardView ImportError : No module named formtools . wizard . views Alors là, le désagrément majeure pour bibi et \" Trigger Happy \", c'est que toutes mes pages permettant de créer des services, reposent entierement sur le Wizard... En creusant la doc on apprend que les wizard sont sortis des contrib pour devenir une app à part : Formtools Il faut donc ajouter \"formtools\" au INSTALLED_APPS après avoir fait un pip install django - formtools Puis on relance les tests : ./ manage . py test Creating test database for alias 'default' ... ........................................ ---------------------------------------------------------------------- Ran 40 tests in 1.998 s OK Destroying test database for alias 'default' ... ou avec un petit coup de Creating test database for alias 'default' ( ':memory:' ) ... Operations to perform : Synchronize unmigrated apps : staticfiles , django_th , messages , pocket , th_rss , django_js_reverse , th_pocket , debug_toolbar , formtools Apply all migrations : admin , contenttypes , auth , sessions Synchronizing apps without migrations : Creating tables ... Creating table django_th_servicesactivated Creating table django_th_userprofile Creating table django_th_userservice Creating table django_th_triggerservice Creating table django_th_rss Creating table django_th_pocket Running deferred SQL ... Installing custom SQL ... Running migrations : Rendering model states ... DONE Applying contenttypes . 0001 _initial ... OK Applying auth . 0001 _initial ... OK Applying admin . 0001 _initial ... OK Applying contenttypes . 0002 _remove_content_type_name ... OK Applying auth . 0002 _alter_permission_name_max_length ... OK Applying auth . 0003 _alter_user_email_max_length ... OK Applying auth . 0004 _alter_user_username_opts ... OK Applying auth . 0005 _alter_user_last_login_null ... OK Applying auth . 0006 _require_contenttypes_0002 ... OK Applying sessions . 0001 _initial ... OK test_servicesactivated ( django_th . tests . test_models_and_services . ServicesActivatedTest ) ... ok test_invalid_form ( django_th . tests . test_models_and_services . TriggerServiceTest ) ... ok test_triggerservice ( django_th . tests . test_models_and_services . TriggerServiceTest ) ... ok test_valid_form ( django_th . tests . test_models_and_services . TriggerServiceTest ) ... ok test_userprofile ( django_th . tests . test_models_and_services . UserProfileTest ) ... ok test_invalid_form ( django_th . tests . test_models_and_services . UserServiceTest ) ... ok test_userservice ( django_th . tests . test_models_and_services . UserServiceTest ) ... ok test_valid_form ( django_th . tests . test_models_and_services . UserServiceTest ) ... ok test_get ( django_th . tests . test_views . TriggerDeletedTemplateViewTestCase ) ... ok test_get ( django_th . tests . test_views . TriggerEditedTemplateViewTestCase ) ... ok test_context_data ( django_th . tests . test_views . TriggerListViewTestCase ) ... ok test_get ( django_th . tests . test_views . UserServiceAddedTemplateViewTestCase ) ... ok test_get ( django_th . tests . test_views . UserServiceDeletedTemplateViewTestCase ) ... ok test_context_data ( django_th . tests . test_views . UserServiceListViewTestCase ) ... ok ---------------------------------------------------------------------- Ran 14 tests in 0.889 s OK Destroying test database for alias 'default' ( ':memory:' ) ... si vous voulez voir la politique de dépréciation de Django . Bon je n'y ai pas trouvé le \"pourquoi\" cette \"contrib\", à l'origine, avait été retirée à présent. Le billet évoluera probablement plus tard selon mes aventures sur le sujet ;) Edit du 10/04 (minuit:) les inlineformset_factory ayant évolués, il n'est à présent plus possible de spécifier ET form_class ET fields dans la vue, c'est l'un ou l'autre voire même que dalle si dans le inlineformset_factory on colle le nom des fields qui nous interessent. tout dans la doc ;) De même en 1.7 on pouvait se contenter d'écrire : from django.db import models class Author ( models . Model ): name = models . CharField ( max_length = 100 ) class Book ( models . Model ): author = models . ForeignKey ( Author ) title = models . CharField ( max_length = 100 ) >>> from django.forms.models import inlineformset_factory >>> BookFormSet = inlineformset_factory ( Author , Book ) mais ceci ne va plus du tout, il faut bien spécifier une liste de fields pleine ou vide (si ca vous chante:) >>> from django.forms.models import inlineformset_factory >>> BookFormSet = inlineformset_factory ( Author , Book , fields = ( 'title' ,)) edit du 12/04 une nouvelle erreur qui saute aux yeux : / home / foxmask / Django - VirtualEnv / django - trigger - happy / local / lib / python2 . 7 / site - packages / django / templatetags / future . py : 25 : RemovedInDjango19Warning : Loading the ` url ` tag from the ` future ` library is deprecated and will be removed in Django 1.9 . Use the default ` url ` tag instead . RemovedInDjango19Warning ) celle ci est dûe à un { % load url from future % } dans les templates","tags":"Techno","url":"https://foxmask.net/post/2015/04/03/django-migration-1-7-a-1-8-formwizard/","loc":"https://foxmask.net/post/2015/04/03/django-migration-1-7-a-1-8-formwizard/"},{"title":"Quête  WampWS et Autobahn - épisode 1","text":"Une contré fort fort lointaine L'était une fois ... des druides, des gueux, des recettes magiques et un gentilhomme En chemin pour une contré fort fort lointaine, un gentilhomme (qu'on appellera DaveLooper ), passa par maintes tribulations et turpitudes tantôt armé de son lance pierres Perl , tantôt de son arbalète PHP , tantôt de son épée Python . Il se fraya un chemin à travers bois parce que les grands routes n'avait guère grâce à ses yeux. C'est ainsi que grâce à son épée, il parvint à se tailler des outils, tel TriggerHappy, un tireur toujours content, né à partir d'une partition entonnée par un troubadour nommé Django, qui lui servirent à se forger de nouvelles armes plus ou moins affûtées. La dernière arme en date fut WampWS un projet foufoufou, qui fait rêver à l'arme absolue. Si vous voulez découvrir, en langage profane, ce qu'un gueux peut attendre de ce projet, ruez vous chez sam et max Comme DaveLooper n'en est même pas à se forger le pommeau de cette dernière, il est loin de l'extase, mais il aime ripailler, et partager, du coup, il s'en va vous narrer comment il a déjà pu débuter ce morceau. Le socle de base de cette nouvelle arme est encore TriggerHappy. Il reste fidèle à son outil fétiche. - Il s'en va l'améliorer. L'idée est donc de reprendre le MCD existant, et de bâtir autour, une arme qui utiliserait le maillage de côte de crossbar / WampWS via autobahn . Pour ce faire DaveLooper a beaucoup échange de pigeons voyageur avec le druide détenant la recette de wampws, et ce dernier lui fourni les jarres qu'il jugeât les plus à même de remplir l'office. Ces derniers sont : Polymer + autobahnJS \\<=> crossbar => autobahnjs + twisted + txpostgres => postgresql. Le choix évoqué par l'érudit se justifie par la scalabilité. Ça fait un paquet de jattes à connaitre avant d'arriver à un résultat honorable diriez vous à DaveLooper . mais tout de même : Crossbar s'installe aisément, reste à creuser la doc pour obtenir une configuration digne de nom. Polymer+Autobahn sera l'étape suivante pour voir comment ça se goupille en récupérant les données de la base et en les affichant avec Polymer Autobahn+twisted+txpostgres : c'est cet ensemble que DaveLooper s'en va vous décrire. Il a préféré tâter le back-end avant le front au cas où le back-end aurait été un désastre, le front aurait été oublié. Donc, le principe est le suivant avec cette stack : crossbar oriente les requêtes entre des composants applicatifs, et ceux ci se parlent en temps réel et de façon asynchrone En 2 lignes voilà crossbar prêt crossbar init à ne faire que la première fois que vous installez crossbar crossbar start pour chaque lancement de l'outil Une fois fait, on peut à son tour démarrer les composants qui trépignent à se faire des causeries. On en lance un : python frontend.py session attached Got event: 0 Got event: 1 Got event: 2 Got event: 3 Got event: 4 Got event: 5 disconnected puis 2 : python backend.py session attached . . . . . . . . . . . et hoooo ça se cause vindiou. Simple efficace. What else ? Ben on se le demande ? allez on se le demande :D Pour partir sur du plus concret et proche de ce que DaveLooper a mis en branle avec l'heureux tireur \"TriggerHappy\", il se dit bien, quand le projet a démarré, seul Evernote causait avec des flux RSS. Repartons donc sur ces traces 'pour voir' ce que ces jarres recommandés par le druide peuvent réaliser Le pattern utilisé sera donc publisher/subscriber. Où Evernote sera le composant subscriber du composant RSS, le publisher. Déroulement : Pour obtenir les items des flux RSS, on récupère de la table qui les contient d'abord les URL de ces flux, puis on parcourt les flux eux-mêmes, et pour chaque item on \"publish\" le contenu de l'article au subscriber. Quand les données parviennent au composant subscriber, il n'a plus qu'à les enregistrer. On va donc voir ci dessous tout cette machinerie, puis comment on requête une base postgresql de façon asynchrone . le backend, the RSS Component # -*- coding: utf-8 -*- import arrow import sys from components.lib.feedsservice import Feeds from txpostgres import txpostgres from twisted.internet.defer import inlineCallbacks , returnValue from autobahn import wamp from twisted.python import log from autobahn.twisted.util import sleep from autobahn.twisted.wamp import ApplicationSession class RssComponent ( ApplicationSession ): \"\"\" An application component that publishes an event \"\"\" @inlineCallbacks def onJoin ( self , details ): print ( \"session attached\" ) pool = txpostgres . ConnectionPool ( None , port = 5432 , database = 'th' , user = 'th' , password = 'th' ) yield pool . start () print ( 'DB Connection pool started' ) self . db = pool # register all procedures on this class which have been # decorated to register them for remoting regs = yield self . register ( self ) print ( 'registered {} procedures' . format ( len ( regs ))) while True : feeds = yield self . call ( 'eu.trigger-happy.rss.feeds_url' ) for data in feeds : for item in data [ 'data' ]: print ( 'publishing {} ' . format ( item )) self . publish ( u 'eu.trigger-happy.rss' , { 'trigger_id' : data [ 'trigger_id' ], 'user_id' : data [ 'user_id' ], 'item' : item }) yield sleep ( 120 ) @wamp . register ( u 'eu.trigger-happy.rss.feeds_url' ) @inlineCallbacks def get_feeds_url ( self ): \"\"\" get the URL stored in the database \"\"\" query = \"SELECT date_triggered, name, url, trigger_id, user_id FROM \" query += \"django_th_rss AS R, \" query += \"django_th_triggerservice AS TS \" query += \" WHERE R.trigger_id=TS.id \" query += \" AND TS.status = True \" # get only the activated triggers query += \" ORDER BY TS.date_triggered DESC \" rows = yield self . db . runQuery ( query ) feeds = [] print ( 'get the feeds url...' ) for feed in rows : print ( 'get feeds from {0} => {1} ' . format ( feed [ 1 ], feed [ 2 ])) if feed [ 0 ] <= self . right_now (): feeds . append ({ 'trigger_id' : feed [ 3 ], \"user_id\" : feed [ 4 ], 'data' : Feeds ( ** { 'url_to_parse' : feed [ 2 ]}) . datas ()}) returnValue ( feeds ) def right_now ( self ): \"\"\" TODO import settings from a file or smth to get the TZ details :return: \"\"\" return arrow . utcnow () . replace ( hour = 0 , minute = 0 , second = 0 ) . to ( 'Europe/Paris' ) if __name__ == '__main__' : log . startLogging ( sys . stdout ) from autobahn.twisted.wamp import ApplicationRunner runner = ApplicationRunner ( url = \"ws://127.0.0.1:8080/ws\" , realm = \"realm1\" ) runner . run ( RssComponent ) le frontend - the Evernote Component from __future__ import unicode_literals import arrow import sys import json # evernote API from evernote.api.client import EvernoteClient from evernote.edam.notestore import NoteStore import evernote.edam.type.ttypes as Types from evernote.edam.error.ttypes import EDAMUserException # postgresql driver from txpostgres import txpostgres # autobahn from autobahn import wamp from twisted.python import log from twisted.internet import reactor from twisted.internet.defer import inlineCallbacks , returnValue from autobahn.twisted.wamp import ApplicationSession from sanitize import sanitize class EvernoteComponent ( ApplicationSession ): \"\"\" An application component that subscribes and receives events \"\"\" @inlineCallbacks def onJoin ( self , details ): print ( \"session attached\" ) pool = txpostgres . ConnectionPool ( None , port = 5432 , database = 'th' , user = 'th' , password = 'th' ) yield pool . start () print ( 'DB Connection pool started' ) self . db = pool # register all procedures on this class which have been # decorated to register them for remoting regs = yield self . register ( self ) print ( 'registered {} procedures' . format ( len ( regs ))) @inlineCallbacks def on_event ( data ): print ( json . dumps ( data , indent = 4 )) yield self . call ( 'eu.trigger-happy.evernote.save' , data ) try : yield self . subscribe ( on_event , u 'eu.trigger-happy.rss' ) print ( \"subscribe topic\" ) except Exception as e : print ( \"could not subscribe to topic: {0} \" . format ( e )) #yield self.subscribe(on_event, u'eu.trigger-happy.pocket') #yield self.subscribe(on_event, u'eu.trigger-happy.twitter') def onDisconnect ( self ): print ( \"disconnected\" ) reactor . stop () @wamp . register ( u 'eu.trigger-happy.evernote.token' ) @inlineCallbacks def get_token ( self , user_id ): \"\"\" get the token of the user that owns the trigger need to link the table with django_th_triggerservice to get the user id :param user_id: :return: a generator \"\"\" query = \"SELECT token FROM django_th_userservice \" query += \" WHERE name_id='ServiceEvernote' \" query += \" AND user_id= {0} \" . format ( user_id ) rows = yield self . db . runQuery ( query ) for row in rows : token = row returnValue ( token ) @wamp . register ( u 'eu.trigger-happy.evernote.trigger' ) @inlineCallbacks def get_trigger ( self , trigger_id ): \"\"\" get information for the current trigger such as notebook, tag, description :param trigger_id: :return: a generator \"\"\" query = \"SELECT notebook, tag, TS.description FROM django_th_evernote AS E, \" query += \" django_th_triggerservice AS TS \" query += \" WHERE E.trigger_id=TS.id \" query += \" AND trigger_id=' {0} '\" . format ( trigger_id ) print ( query ) rows = yield self . db . runQuery ( query ) for notebook , tag , description in rows : print ( notebook , tag , description ) data = { 'notebook' : notebook , 'tag' : tag , 'description' : description } returnValue ( data ) @wamp . register ( u 'eu.trigger-happy.evernote.save' ) @inlineCallbacks def save_data ( self , stuff ): \"\"\" save the data coming from the subscribed service :param stuff: contain a table of : user_id, trigger_id, item (the main content) :return: nothing \"\"\" user_id = stuff [ 'user_id' ] trigger_id = stuff [ 'trigger_id' ] token = yield self . call ( 'eu.trigger-happy.evernote.token' , user_id ) token = token [ 0 ] content = '' status = False data = stuff [ 'item' ] \"\"\" .... ici .... le long traitement pour exploiter les data et les envoyer sur son compte evernote \"\"\" if __name__ == '__main__' : log . startLogging ( sys . stdout ) from autobahn.twisted.wamp import ApplicationRunner runner = ApplicationRunner ( url = \"ws://127.0.0.1:8080/ws\" , realm = \"realm1\" ) runner . run ( EvernoteComponent ) Donc on lance le subscriber puis le publisher et quand le RssComponent trouve des éléments à publier il les envoie directement au EvernoteComponent Tout cela se lit dans les consoles respectives instantanément. Voici pour le coté \"bout de code concret qui marche\" Écueils : Comme à chaque fois qu'on débute avec de nouveaux outils, on se tape sur les doigts en se ratant/pensant enfoncer le clou. Écueil 1 : erreur 111, la fameuse erreur 111... $ python frontend.py Traceback ( most recent call last ) : File \"frontend.py\" , line 62 , in runner.run ( Component ) File \"...ocal/lib/python2.7/site-packages/autobahn/twisted/wamp.py\" , line 199 , in run raise connect_error.exception twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 111 : Connection refused. ca veut dire que le composant ne parvient pas à contacter ... crossbar ... Ouais ouais ouais crossbar. Comme DaveLooper avait déjà vu cette erreur par le passé en des temps reculés, il se dit comme à l'époque, \"bon la base est pas joignable\" Mais à tord Donc 2 choses à checker : le port ouvert par crossbar dans .crossbar/config.json \"transports\" : [ { \"type\" : \"web\" , \"endpoint\" : { \"type\" : \"tcp\" , \"port\" : 8090 }, \"paths\" : { \"/\" : { \"type\" : \"static\" , \"directory\" : \"..\" }, \"ws\" : { \"type\" : \"websocket\" } } } ] et celui qu'on a défini dans le script : runner = ApplicationRunner ( url = \"ws://127.0.0.1:8090/ws\" , ... ) s'ils sont identiques, alors crossbar n'est pas démarré Écueil 2 : debug Il est trop trop souvent arrivé à DaveLooper de voir ses petites cailloux, les print, retraçant le cheminement de l'enchainement des étapes ... S'arrêter sans rien dire du tout. Aussi est il allé se rendre dans quelques échoppes quérir de l'aide sur sans en trouver... Et comme Sam pourrait encore le dire en rigolant : chaque fois que DaveLooper pose une question, il fini par trouver la réponse de lui même Ba oui DaveLooper est borné et cherche toujours jusqu'à la solution quand les réponses se font rares. Donc il a fini par trouver ceci : runner = ApplicationRunner ( url = \"ws://127.0.0.1:8090/ws\" , realm = \"realm1\" , debug = False , # low-level WebSocket debugging debug_wamp = False , # WAMP protocol-level debugging debug_app = False ) # app-level debugging Je vous dispenserai les logs, ils sont s'y verbeux qu'on croirait qu'on compile le kernel linux ;) Du coup quand les print s'arrêtent DaveLooper est content de savoir enfin pourquoi. Écueil 3 : leS elf ;) Quand utiliser ces 4 là et comment. yield self . subscriber () yield self . publish () De ce que DaveLooper a pu tester, les 2 premiers yield sont assez explicites d'eux même. yield self . call () self . method () Par contre pour savoir quand faire self.call ou sel.method c'est une autre histoire. Le self.call coté publisher aura permis de récupérer les données pour le subscriber Cote subscriber self.call ne convient pas pour sauvegarder les données. Pourquoi ? Parce qu'il fallait \"décorer\" la methode déclenchée lors du subscribe, ici on_event() et donc rajouter yield devant. Sinon, si on n'a pas besoin d'appels RPC, on utilisera self.method() DaveLooper , n'utilisant ces jarres que depuis samedi dernier, a eu mal à comprendre tout ça sans des explications fournies sur indexerror To be continued Nous voici donc arrivé au terme de ce petit voyage, de DaveLooper toujours en quête de nouveauté, où en même temps, très peu et beaucoup de choses ont été abordées. Dans les prochains épisodes, DaveLooper a en tête quelques sujets comme : démarrer un projet de zéro proprement, ou tout du moins prêt à l'emploi la configuration de crossbar, pour alléger le code ci dessus et retirer les appels fait en dur à une base donnée par exemple le \"mode debug\" de crossbar/wamp wamplet nota : si vous avez réperé des coquille(tte)s dans le code, faites en part en commentaire, comme ca fait que 5jours que DaveLooper utilise le tout Crossbar/wamp/autobahn, il y a forcement beaucoup à améliorer, et ca sera fait avec les prochains billets ;)","tags":"Techno","url":"https://foxmask.net/post/2015/02/27/quete-wampws-et-autobahn-episode-1/","loc":"https://foxmask.net/post/2015/02/27/quete-wampws-et-autobahn-episode-1/"},{"title":"Trigger Happy : Créer un module - la doc","text":"Hello, Lors du Meetup Django Paris , le sieur Linovia m'avait demandé s'il existait une doc pour produire un plugin/module, et j'avais répondu que non, \"un peu trop vite\". En effet j'avais déjà abordé le sujet dans un précédant billet , mais plutôt que de le laisser végéter dans un coin je l'ai rajouté à la doc \" readthedocs \" du projet. Ainsi donc dans cette doc, vous trouverez : comment est constitué un module Trigger Happy son forms.py son models.py et sa classe centrale avec ce qu'elle doit contenir pour lui permettre de récupérer des informations provenant d'autres sources de données pour lui d'une part et en fournir pour les autres. Je ne m'étalerai pas plus dans le billet pour éviter un redit de la doc. Par contre, comme à l'accoutumé, je prends tout commentaire sur le sujet. Maintenant, vous n'avez plus d'excuses pour ne pas vous zamuser à spread ze web avec vos zouéts. Un peu Noël avant l'heure non ? ;)","tags":"Techno","url":"https://foxmask.net/post/2014/12/01/trigger-happy-creer-un-module/","loc":"https://foxmask.net/post/2014/12/01/trigger-happy-creer-un-module/"},{"title":"Django Paris - Novembre 2014 - Numa : Demo Live","text":"Voici un cours billet qui devrait tomber rapidement sur mon compte Twitter , juste pour la forme ;) Et illustrant l'utilisation de Trigger Happy, avec le talk en live dispo ici Aussi... la petite image qui va bien (ouais ya pas que des chats sur l'net), pour illustrer qu'avec Evernote tout est repris puisque ce billet tombe aussi par ici","tags":"Techno","url":"https://foxmask.net/post/2014/11/27/django-paris-numa-novembre-2014-demo-live/","loc":"https://foxmask.net/post/2014/11/27/django-paris-numa-novembre-2014-demo-live/"},{"title":"Django Tutorial, une App Téléchargeable ?","text":"\"Hey J'ai une idée\" (dans la veine des idées que j'aurais pas le temps de faire ), je m'en mordrais surement les doigts après, mais lançons nous :) En échangeant sur divers sujets avec Mr Linovia (dont un truc zarbi avec makemigration), je lui dis grosso modo : Tiens j'ai pas encore vu de version téléchargeable du code des tuto django pour que tout à chacun, découvrant le framework, puisse les exécuter simplement, ou juste voir comment le code est fait à titre d'exemple. Et vous ça vous intéresserait d'avoir le code des tutos à disposition ? Il peut arriver quand on débute, de ne pas comprendre une erreur qui n'est dûe qu'à une saleté de typo. Du coup comparer le code qu'on a soigneusement rédiger à partir des tuto de la doc, avec celui téléchargeable, peut vite permettre de comprendre où on a raté une marche (entre autre). Je pensais faire un dépôt par tutorial (qui en compte 6 + 2 \"advanced\"), sur github par exemple, plus un final qui soit l'appli \"Poll\" tout entière. Ainsi elle sera aussi utilisable dans la vraie vie réelle. [poll id=\"4\"] Si vous vous en tapez le coquillard avec des tibias de gastéropodes ; just let me know ;)","tags":"Techno","url":"https://foxmask.net/post/2014/11/06/django-tutorial-app-telechargeable/","loc":"https://foxmask.net/post/2014/11/06/django-tutorial-app-telechargeable/"},{"title":"Django Trigger Happy - déclenche une heureuse rafale de mises à jour","text":"Après quelques mois de tests et tergiversations, j'ai fini par me fendre d'une nouvelle version du projet \"Trigger Happy\" et de tous ses modules. Si vous découvrez tout juste le projet, follow ici de quoi obtenir un aperçu ( et en images ) Voici la listes des nouveautés & améliorations produites. A- Mises à jour A tout seigneur tout honneur donc, on commence par le coeur du projet : django-th : le Core Partie Backend suppression des doctest pour des tests unitaires mise à jour Django 1.7 migration des 2 batches fire.py et fire_as.py en management command ce qui permet, comme chacun doit savoir, de lancer les commandes aisément depuis manage.py . La 2nde commande exploite asyncio et donc peut-être lancée depuis python 3.4 Doc au format Sphinx en cas de besoin ;) amélioration des perfs en réduisant les requêtes SQL via Queryset.select_related() Partie Front : intégration de BootStrap 3 (vs version 2) amélioration de l'interface à laquelle est ajoutée de l'Ajax (pour les urls est utilisent django-js-reverse ) permettant d'activer ou non un trigger (sans recharger la page évidemment) Packaging : ajout d'un fichier MODULES.rst permettant de recenser les modules existants et dispo sur pypi Modules \"Trigger Happy\" Ensuite tous les modules \"Trigger Happy\" ont subit (quasiment) les mêmes mises à jour : Partie Backend : mise à jour Django 1.7 Doc au format Sphinx en cas de besoin ;) suppression des doctest pour des tests unitaires Partie Front : intégration de BootStrap 3 (vs version 2) Celles-ci concernent : django-th-dummy : module pour se bootstrap son propre module Trigger Happy, django-th-evernote : module pour gérer les données de son compte Evernote, django-th-pocket : module pour gérer les données de son compte Pocket, django-th-rss : module pour gérer les données de flux RSS, django-th-readability : module pour gérer les données de son compte Readability B - Nouveauté nouveau module flambant neuf pour encore plus de liberté, django-th-twitter : Avec ce dernier vous pouvez envoyer des données d'un des services ci dessus sur votre compte Twitter, ou récupérer des infos depuis Twitter (via un Tag ou le compte d'un Twittos) pour les expédier vers l'un des services ci dessus. Genre une news tombe chez nos amis de Sam et Max et vous voulez la publier cache sur Twitter, ou à l'inverse vous suivez leur compte Twitter et voulez ne rien rater de leur Tweets pour les balancer sur votre compte Pocket, ce module est là pour ça. C- Ze Stuff Où trouver toute la clic ? sur le classique Pypi : Django Trigger Happy Evernote Pocket Readability RSS Twitter ou sur les dépôts GitHub : Django Trigger Happy Evernote Pocket Readability RSS Twitter A venir Qu'est-il prévu pour la suite ? Un petit tour sur la liste des tickets & milestones vous en dira plus What's about you ? Si une question vous taraude sur le projet, ou envie de laisser un petit mot, n'hésitez pas.","tags":"Techno","url":"https://foxmask.net/post/2014/10/22/django-trigger-happy-declenche-heureuse-rafale-maj/","loc":"https://foxmask.net/post/2014/10/22/django-trigger-happy-declenche-heureuse-rafale-maj/"},{"title":"CentOs Oracle Could not open/read file:///etc/pki/rpm-gpg/RPM-GPG-KEY-oracle","text":"Quand on a besoin de faire un yum update suivi d'un yum install foobar il peut arriver que sur votre CentOS vous ayez un problème avec vos packages Oracle : warning: rpmts_HdrFromFdno: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY Retrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-oracle GPG key retrieval failed: [ Errno 14 ] Could not open/read file:///etc/pki/rpm-gpg/RPM-GPG-KEY-oracle Pour y remédier 2 commandes et puis s'en va [ root@localhost ~ ] # rpm --import http://oss.oracle.com/ol6/RPM-GPG-KEY-oracle [ root@localhost ~ ] # rpm -q gpg-pubkey-ec551f03-4c2d256a et au yum install suivant tout glisse. source","tags":"Techno","url":"https://foxmask.net/post/2014/10/17/centos-oracle-could-not-openread-fileetcpkirpm-gpgrpm-gpg-key-oracle/","loc":"https://foxmask.net/post/2014/10/17/centos-oracle-could-not-openread-fileetcpkirpm-gpgrpm-gpg-key-oracle/"},{"title":"Installation Oracle 10G en mode spéléo","text":"Bon, j'avais un wiki, au temps jadis, que j'avais basardé parce que je m'étais dit \"Oracle 10 n'est plus supporté, je n'aurai plus à me faire suer pour l'installer\" ... Et c'est toujours dans ces cas de figure qu'on se maudit quand le moment poindre le bout du nez. Voilà donc un billet qui part en spéléo vous narrez quelques trucs qui depuis le temps sont surement déjà acquises (j'ose espérer ;) Donc pour installer Oracle 10g, je vous souhaite d'avoir les archives dans un coin car ça n'est plus dispo chez Oracle via la manager qui va bien. Après les avoir décompressées, temps il est de lancer l'installeur : cd database ./runInstaller La plupart du temps on n'installe pas Oracle sur une workstation ou sur un serveur disposant d'un écran et d'un clavier. Or l'installeur Oracle utilise \"OUI\" (Oracle Universal Installer), qui lance une GUI ! Donc avant même de lancer cet installeur, on est bon pour : installer la machinerie pour compiler installer des lib X11 configurer le forwarding X11 1- la machinerie pour compiler yum install binutils gcc glibc glibc-headers glibc-kernheaders glibc-devel compat-libstdc++ cpp compat-gcc make compat-db compat-gcc-c++ compat-libstdc++-devel openmotif openmotif21 setarch pdksh libaio libaio-devel libXt.i686 libXt-devel.x86_64 libXt-devel.i686 libXtst.i686 libXtst-devel.i686 libXtst-devel.x86_64 2- installer des lib X11 ceci vous evitera de rencontrer ce genre d'erreur : java.lang.UnsatisfiedLinkError: /tmp/OraInstall../jre/1.4.2/lib/i386/libawt.so: libXtst.so.6: cannot open shared object file: No such file or directory 3- configurer le forwarding X11 pour faire du X11 forwarding, tout le monde sait qu'on fait ssh -X login@server mais si on regarde de plus près, ça ne suffit souvent pas : ssh -v -X login@server ... debug1: Remote: No xauth program ; cannot forward with spoofing. X11 forwarding request failed on channel 0 comme l'avant dernière ligne l'indique, il n'y a pas xauth de dispo, donc let's go yum install xauth Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile * base: centos.quelquesmots.fr * extras: centos.mirror.fr.planethoster.net * updates: centos.crazyfrogs.org base | 1 .1 kB 00 :00 extras | 2 .1 kB 00 :00 updates | 1 .9 kB 00 :00 Setting up Install Process Resolving Dependencies --> Running transaction check ---> Package xorg-x11-xauth.i386 1 :1.0.1-2.1 set to be updated --> Finished Dependency Resolution Dependencies Resolved ================================================================================================================================================================================ Package Arch Version Repository Size ================================================================================================================================================================================ Installing: xorg-x11-xauth i386 1 :1.0.1-2.1 base 31 k Transaction Summary ================================================================================================================================================================================ Install 1 Package ( s ) Upgrade 0 Package ( s ) Total download size: 31 k Is this ok [ y/N ] : Y Downloading Packages: xorg-x11-xauth-1.0.1-2.1.i386.rpm | 31 kB 00 :00 Running rpm_check_debug Running Transaction Test Finished Transaction Test Transaction Test Succeeded Running Transaction Installing : xorg-x11-xauth 1 /1 Installed: xorg-x11-xauth.i386 1 :1.0.1-2.1 Complete! Ensuite il se peut que ca ne suffise toujours pas, il faut donc editer le fichier <code< etc ssh sshd_config< code> pour mettre ces parametres : X11Forwarding yes X11UseLocalhost no puis relancer sshd. Testons à présent ssh -v -X oracle@server debug1: Requesting X11 forwarding with authentication spoofing. debug1: Sending environment. debug1: Sending env LANG = fr_FR.utf8 Last login: Thu Oct 9 09 :40:29 2014 from /usr/bin/xauth: creating new authority file /u01/app/oracle/.Xauthority [ oracle@server ~ ] $ xclock debug1: client_input_channel_open: ctype x11 rchan 2 win 65536 max 16384 debug1: client_request_x11: request from 127 .0.0.1 41557 debug1: channel 1 : new [ x11 ] debug1: confirm x11 debug1: channel 1 : FORCE input drain ouais vous la voyez pas sur cette page l'horloge, faut pas pousser non plus ;) Si ça ne marchait toujours pas chez vous, vérifiez bien que X11Forwarding est activé sur le serveur : grep X11Forwarding /etc/ssh/sshd_config #X11Forwarding no X11Forwarding yes Maintenant zont n'est prêt pour lancer l'installeur. cd /u01/app/oracle/binaires/database/ ./runInstaller -ignoreSysPrereqs et le GUI démarre, zavez plus qu'à vous faire plaisir. le -ignoreSysPrereqs indique à Oracle de me lâcher la grappe sur les prérequis système, notamment sur l'OS qui n'est pas RedHat. Et c'est pas fini © Je n'ai volontairement pas abordé les prérequis techniques sur les sémaphores et tout le bataclan, le net en regorge, ce billet est un nième noeud à mon mouchoir pour pouvoir lancer le GUI :P Une solution existe pour lancer l'installer sans le GUI avec l'option -silent et avec un fichier de réponses ( -reponseFile /full/path/to/responsefile.rsp ) contenant déjà tout le paramétrage qui va bien. Mais souvent les fichiers de réponse ne sont pas à jour. Ne serait-ce que la structure du fichier d'une version mineure à l'autre de Oracle. Je l'ai juste citée en cas de besoin ;)","tags":"Techno","url":"https://foxmask.net/post/2014/10/09/installation-oracle-10g-gui/","loc":"https://foxmask.net/post/2014/10/09/installation-oracle-10g-gui/"},{"title":"Django, Le Magic System T'y Es Fou !","text":"Voici viendu la billet django de la rentrée, zenfin ! Comme on n'en fini jamais de faire le tour de django, après avoir poussé jusqu'au bout du bout la gestion des forms , me voici parti sur une nouvelle aventure, celle de la Management Commands . Dans la suite nous verrons comment, avant de tomber sur ces \"Management Commands\", je m'y prenais (pas parfaitement) pour packager des batches avec mes applications, puis nous verrons ensuite comment on produit un batch à la sauce Django, sans effort. Avant : Pas de Magie, juste un truc qui marche, mais archaïque & rigide. Le batch se nomme \" fire.py \" et fait 100 lignes à tout casser. Comment est-il lancé ? Ce script est appelé depuis un virtualenv. j'ai besoin que python trouve le settings.py de mon appli d'où python os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"django_th.settings\") et pour les besoins de la v 1.7 de Django il m'a fallu rajouter python import django django.setup() Ce qui donne #!/usr/bin/env python # -*- coding: utf-8 -*- from __future__ import unicode_literals import os import datetime import time import arrow os . environ . setdefault ( \"DJANGO_SETTINGS_MODULE\" , \"django_th.settings\" ) import django django . setup () from django.conf import settings from django_th.services import default_provider from django_th.models import TriggerService from django.utils.log import getLogger # create logger logger = getLogger ( 'django_th.trigger_happy' ) def go (): \"\"\" run the main process \"\"\" trigger = TriggerService . objects . filter ( status = True ) if trigger : for service in trigger : # flag to know if we have to update to_update = False # flag to get the status of a service status = False # counting the new data to store to display them in the log count_new_data = 0 # provider - the service that offer datas service_name = str ( service . provider . name . name ) service_provider = default_provider . get_service ( service_name ) # consumer - the service which uses the data service_name = str ( service . consumer . name . name ) service_consumer = default_provider . get_service ( service_name ) # check if the service has already been triggered if service . date_triggered is None : logger . debug ( \"first run for %s => %s \" % ( str ( service . provider . name ), str ( service . consumer . name . name ))) to_update = True # run run run else : # 1) get the datas from the provider service # get a timestamp of the last triggered of the service datas = getattr ( service_provider , 'process_data' )( service . provider . token , service . id , service . date_triggered ) #consumer = getattr(service_consumer, 'save_data') published = '' which_date = '' # flag to know if we can push data to the consumer proceed = False # 2) for each one for data in datas : # if in a pool of data once of them does not have # a date, will take the previous date for this one # if it's the first one, set it to 00:00:00 # let's try to determine the date contained in the data... published = to_datetime ( data ) if published is not None : # get the published date of the provider published = arrow . get ( str ( published ), 'YYYY-MM-DD HH:mm:ss' ) . to ( settings . TIME_ZONE ) # store the date for the next loop # if published became 'None' which_date = published #... otherwise set it to 00:00:00 of the current date if which_date == '' : # current date which_date = arrow . utcnow () . replace ( hour = 0 , minute = 0 , second = 0 ) published = which_date if published is None and which_date != '' : published = which_date # 3) check if the previous trigger is older than the # date of the data we retreived # if yes , process the consumer # add the TIME_ZONE settings date_triggered = arrow . get ( str ( service . date_triggered ), 'YYYY-MM-DD HH:mm:ss' ) . to ( settings . TIME_ZONE ) # if the published date if greater or equal to the last # triggered event ... : if date_triggered is not None and published is not None and published . date () >= date_triggered . date (): # if date are the same ... if published . date () == date_triggered . date (): # ... compare time and proceed if needed if published . time () >= date_triggered . time (): proceed = True # not same date so proceed ! else : proceed = True if proceed : if 'title' in data : logger . info ( \"date {} >= date triggered {} title {} \" . format ( published , date_triggered , data [ 'title' ])) else : logger . info ( \"date {} >= date triggered {} \" . format ( published , date_triggered )) #status = consumer( # service.consumer.token, service.id, **data) to_update = True count_new_data += 1 # otherwise do nothing else : if 'title' in data : logger . debug ( \"data outdated skiped : [ {} ] {} \" . format ( published , data [ 'title' ])) else : logger . debug ( \"data outdated skiped : [ {} ] \" . format ( published )) # update the date of the trigger at the end of the loop sentance = \"user: {} - provider: {} - consumer: {} - {} \" if to_update : if status : logger . info (( sentance + \" new data\" ) . format ( service . user , service . provider . name . name , service . consumer . name . name , service . description , count_new_data )) update_trigger ( service ) else : logger . info (( sentance + \" AN ERROR OCCURS \" ) . format ( service . user , service . provider . name . name , service . consumer . name . name , service . description )) else : logger . info (( sentance + \" nothing new\" ) . format ( service . user , service . provider . name . name , service . consumer . name . name , service . description )) else : print ( \"No trigger set by any user\" ) def update_trigger ( service ): \"\"\" update the date when occurs the trigger \"\"\" now = arrow . utcnow () . to ( settings . TIME_ZONE ) . format ( 'YYYY-MM-DD HH:mm:ss' ) TriggerService . objects . filter ( id = service . id ) . update ( date_triggered = now ) def to_datetime ( data ): \"\"\" convert Datetime 9-tuple to the date and time format feedparser provides this 9-tuple \"\"\" my_date_time = None if 'published_parsed' in data : my_date_time = datetime . datetime . fromtimestamp ( time . mktime ( data . published_parsed )) elif 'updated_parsed' in data : my_date_time = datetime . datetime . fromtimestamp ( time . mktime ( data . updated_parsed )) elif 'my_date' in data : my_date_time = arrow . get ( str ( data [ 'my_date' ]), 'YYYY-MM-DD HH:mm:ss' ) return my_date_time def main (): default_provider . load_services () # let's go go () if __name__ == \"__main__\" : main () Avantage(s) La commande à taper est un simple ./fire.py dès lors que le fichiers settings est trouvé par le script tout passe tout seul Inconvénient(s) Le packaging est empirique, il faut rajouter au setup.py la prise en compte de ce batch par : entry_points = { 'console_scripts' : [ 'trigger-happy = django_th.fire:go' , ], } afin que la commande soit disponible sous la main dès l'installation via un pip install django_th Point noir que tout le monde aura vu, le script appelle en dur le fichier de settings de django_th. Ce qui empêche une intégration parfaite avec une autre application Django voisine déjà installée. Ou alors il faut retoucher au batch une fois installer pour changer le nom du settings à utiliser. Après Le \"Magic System\" : Comme j'en ai eu marre de me coltiner le gros point noir au milieu de la tronche, j'ai voulu trouvé Ze moyen de ne plus me prendre la tête avec ce settings à définir dans mon batch. Et c'est à ce moment là que j'ai croisé les Management Commands. Au début le terme et leur définition dans la doc m'a fait penser que c'était dédié à la partie backend de l'appli. Mais en rererererelisant attentivement, je me suis aperçu que ça collait parfaitement à mon besoin, qui est de faire du traitement de données, autrement que via l'interface web. J'ai donc appliqué, au pied de la lettre, l'archi de la doc, pour donc déplacer mon fire.py dans un sous-sous-dossier management/commands de mon appli django_th, sous le nom fire_th.py Avantage(s): Plus d'appel en dur d'un settings qui n'est pas celui de mon application courante. La commande est dispo de facto, dès lors que l'application est présente dans INSTALLED_APPS dans le settings.py. pas de modification du setup.py nécessaire Pour s'en assurer on tapera python manage.py help pour trouver la commande ;) Ce script a donc été modifié en : supprimant l'appel du settings supprimant \"def main\" jusqu'à la fin du script remplaçant la fonction go() par handle() requise par BaseCommand déplaçant les 2 fonctions update_trigger() et to_datetime() au debut de la class Command remplaçant print() par self.stdout.write() Ce qui donne : #!/usr/bin/env python # -*- coding: utf-8 -*- from __future__ import unicode_literals import datetime import time import arrow from django.core.management.base import BaseCommand , CommandError from django.conf import settings from django_th.services import default_provider from django_th.models import TriggerService from django.utils.log import getLogger # create logger logger = getLogger ( 'django_th.trigger_happy' ) class Command ( BaseCommand ): help = 'Trigger all the services' def update_trigger ( self , service ): \"\"\" update the date when occurs the trigger \"\"\" now = arrow . utcnow () . to ( settings . TIME_ZONE ) . format ( 'YYYY-MM-DD HH:mm:ss' ) TriggerService . objects . filter ( id = service . id ) . update ( date_triggered = now ) def to_datetime ( self , data ): \"\"\" convert Datetime 9-tuple to the date and time format feedparser provides this 9-tuple \"\"\" my_date_time = None if 'published_parsed' in data : my_date_time = datetime . datetime . fromtimestamp ( time . mktime ( data . published_parsed )) elif 'updated_parsed' in data : my_date_time = datetime . datetime . fromtimestamp ( time . mktime ( data . updated_parsed )) elif 'my_date' in data : my_date_time = arrow . get ( str ( data [ 'my_date' ]), 'YYYY-MM-DD HH:mm:ss' ) return my_date_time def handle ( self , * args , ** options ): \"\"\" run the main process \"\"\" trigger = TriggerService . objects . filter ( status = True ) if trigger : for service in trigger : # flag to know if we have to update to_update = False # flag to get the status of a service status = False # counting the new data to store to display them in the log count_new_data = 0 # provider - the service that offer datas service_name = str ( service . provider . name . name ) service_provider = default_provider . get_service ( service_name ) # consumer - the service which uses the data service_name = str ( service . consumer . name . name ) service_consumer = default_provider . get_service ( service_name ) # check if the service has already been triggered if service . date_triggered is None : logger . debug ( \"first run for %s => %s \" % ( str ( service . provider . name ), str ( service . consumer . name . name ))) to_update = True # run run run else : # 1) get the datas from the provider service # get a timestamp of the last triggered of the service datas = getattr ( service_provider , 'process_data' )( service . provider . token , service . id , service . date_triggered ) consumer = getattr ( service_consumer , 'save_data' ) published = '' which_date = '' # flag to know if we can push data to the consumer proceed = False # 2) for each one for data in datas : # if in a pool of data once of them does not have # a date, will take the previous date for this one # if it's the first one, set it to 00:00:00 # let's try to determine the date contained in the data... published = self . to_datetime ( data ) if published is not None : # get the published date of the provider published = arrow . get ( str ( published ), 'YYYY-MM-DD HH:mm:ss' ) . to ( settings . TIME_ZONE ) # store the date for the next loop # if published became 'None' which_date = published #... otherwise set it to 00:00:00 of the current date if which_date == '' : # current date which_date = arrow . utcnow () . replace ( hour = 0 , minute = 0 , second = 0 ) published = which_date if published is None and which_date != '' : published = which_date # 3) check if the previous trigger is older than the # date of the data we retreived # if yes , process the consumer # add the TIME_ZONE settings date_triggered = arrow . get ( str ( service . date_triggered ), 'YYYY-MM-DD HH:mm:ss' ) . to ( settings . TIME_ZONE ) # if the published date if greater or equal to the last # triggered event ... : if date_triggered is not None and published is not None and published . date () >= date_triggered . date (): # if date are the same ... if published . date () == date_triggered . date (): # ... compare time and proceed if needed if published . time () >= date_triggered . time (): proceed = True # not same date so proceed ! else : proceed = True if proceed : if 'title' in data : logger . info ( \"date {} >= date triggered {} title {} \" . format ( published , date_triggered , data [ 'title' ])) else : logger . info ( \"date {} >= date triggered {} \" . format ( published , date_triggered )) status = consumer ( service . consumer . token , service . id , ** data ) to_update = True count_new_data += 1 # otherwise do nothing else : if 'title' in data : logger . debug ( \"data outdated skiped : [ {} ] {} \" . format ( published , data [ 'title' ])) else : logger . debug ( \"data outdated skiped : [ {} ] \" . format ( published )) # update the date of the trigger at the end of the loop sentance = \"user: {} - provider: {} - consumer: {} - {} \" if to_update : if status : logger . info (( sentance + \" new data\" ) . format ( service . user , service . provider . name . name , service . consumer . name . name , service . description , count_new_data )) self . update_trigger ( service ) else : logger . info (( sentance + \" AN ERROR OCCURS \" ) . format ( service . user , service . provider . name . name , service . consumer . name . name , service . description )) else : logger . info (( sentance + \" nothing new\" ) . format ( service . user , service . provider . name . name , service . consumer . name . name , service . description )) else : self . stdout . write ( \"No trigger set by any user\" ) Le script est plus long de (vraiment) quelques lignes, mais le jeu en vaut la chandelle. Enfin : Reste plus qu'à taper sa commande python manage . py fire_th And Just in case Une fois que vous aurez fini votre script, pour le tester, si vous tapez comme, votre serviteur, la commande ./ fire_th . py depuis le dossier management/commands, vous aurez une surprise au premier import mon_appli.whatelse présent dans votre script, vous indiquant que le module n'existe pas. Et effectivement, en se remettant les neurones à leur place, on se rendra compte qu'on se devra de taper python manage . py fire_th depuis le folder contenant manage.py :P Conclusion: Une nouvelle fonctionnalité tout simple mais c'est de la simplicité que s'exprime toute la puissance de la chose ;) Hey pssssssssssttttttttt : Tous les commentaires sont les bienvenus.","tags":"Techno","url":"https://foxmask.net/post/2014/09/23/django-le-magic-system-ty-es-fou/","loc":"https://foxmask.net/post/2014/09/23/django-le-magic-system-ty-es-fou/"},{"title":"Debian et le \"Menu Principal\" perdu","text":"Je mets ça là au cas où ... Pour une raison qui m'échappe, je n'ai plus eu de menu \"Menu Principal\", accessible en principe depuis le menu \"Activités\" > \"Applications\" Or sans ce dernier, pour rajouter des applications qui ne sont pas issues de package debian, au hasard, un éditeur particulier, c'est peine perdue, obliger d'en passer par la ligne de commandes. Le Sésame vient du package debian \" alacarte \" apt-get install alacarte zavourez comme nom de package, c'est mnémotechnique à ... mourir... pas loin de nom à la ikéa :P et donc revoilà le menu On est fin prêt pour remettre les applications de son choix dans le menu et via les Extensions , on retrouve son application dans le menu en haut de votre desktop","tags":"Techno","url":"https://foxmask.net/post/2014/09/07/debian-et-le-menu-principal-perdu/","loc":"https://foxmask.net/post/2014/09/07/debian-et-le-menu-principal-perdu/"},{"title":"Django Form sous ses plus beaux atours","text":"Les forms django sont vraiment indéniablement très bien foutus. Par contre si on aime l'aspect brute on est servie ;) La mouvance étant à ~~la fainéantise~~ Twitter Bootstrap3, je revêts les plus beaux atours pour mes formulaires. Pour l'exemple, voici une liste déroulante customisée Là je me la joue cool parce que pas de données d'une table liée à rajouter :P un bout du forms.py breakfast_lunch_diner = forms . ChoiceField ( widget = forms . Select ( attrs = { 'class' : 'form-control' })) Partout dans les forms on peut rajouter une classe qui sera gérée par Bootstrap3 aisément comme par exemple rajouter un type email pour que les mobiles switchent directement le clavier quand votre petit doigt sélectionnera le champ de saisie \"courriel\". customer_mail = forms . EmailField ( widget = forms . TextInput ( { 'class' : 'form-control' , 'type' : 'email' })) Tout va bien dans le meilleurs des mondes tant que je n'ai pas à me farcir l'habillage des listes déroulantes issues de tables liées par une FK. Là c'est une autre histoire... ou pas ;) J'ai creusé la toile, cette inépuisable ressource, (merci les arraignées ;) à la recherche du truc qui ferait la différence. On m'avait suggeré form-crispy et autre joyeuseté un peu too much pour le peu que j'avais à en tirer, et la solution est viendu de StackOverFlow Dans l'init du Form on rajoute tout simplement : def __init__ ( self , * args , ** kwargs ): super ( JeSuisSupperEnForm , self ) . __init__ ( * args , ** kwargs ) self . fields [ 'mon_field_cette_fk' ] . widget . attrs [ 'class' ] = 'form-control' Et hop roule ma poule","tags":"Techno","url":"https://foxmask.net/post/2014/07/15/django-form-sous-ses-plus-beaux-atours/","loc":"https://foxmask.net/post/2014/07/15/django-form-sous-ses-plus-beaux-atours/"},{"title":"Importer un dump Oracle sans l'exporter physiquement avant","text":"Donc s'il vous arrive (comme bibi) d'être short en place disque pour faire vos backups oracle pour ensuite importer le(s) schéma(s), une solution ultime subsiste, quand même, pour nous autre \"pauvres\" petits \"exportateurs\" de données :P Comment ça ? La subtilité réside dans la possibilité d'Oracle à importer les données dans le schéma cible, via un DBLink . Pas croyable hein ? let's voyons this right maintenant: Creation du DBLink sur le serveur \"cible\" : CREATE DATABASE LINK chicago CONNECT TO admin1 IDENTIFIED BY windy USING 'CHI' ; Ici, le nom du schema \"source\" est admin1 , le nom du DBLink chicago. Ensuite vient l'import lui même sur le serveur \"cible\" impdp admin2 / market TABLES = customers , sales DIRECTORY = dpump1 NETWORK_LINK = chicago ici l'import (la commande impdp) demande à oracle de se connecter avec le user admin2, d'importer 2 tables, provenant du schéma se cachant derrière le DBLink \"chicago\" Oracle exportera donc admin1, sans stocker le dump dans un fichier physique , depuis le serveur nommé \"CHI\" (ou l'alias qui est défini dans le tnsnames.ora de votre serveur oracle), puis l'importera localement dans le schema admin2 et le tour est joué ;) L'intéret est que je n'aurai pas à exporter mes centaines de giga sur un serveur sans place disque (ou pas assez) pour stocker le dump puis le transferer sur le serveur cible et l'importer. Là, tout se fait d'une traite. Si on a plusieurs schémas, il faut que le user du dblink ait les droits d'y accéder sur la source. Si c'est le cas, ensuite on peut faire : impdp admin2 / market SCHEMAS = schema1 , schema2 REMAP_SCHEMA = schema1 : mon_nouveau_schema1 , schema2 : mon_nouveau_schema2 DIRECTORY = dpump1 NETWORK_LINK = chicago et roule ma poule","tags":"Techno","url":"https://foxmask.net/post/2014/07/02/importer-un-dump-oracle-sans-lexporter-physiquement-avant/","loc":"https://foxmask.net/post/2014/07/02/importer-un-dump-oracle-sans-lexporter-physiquement-avant/"},{"title":"Django Jeu, FormSet et match","text":"Le FormSets kézako ? Une extension d'un formulaire standard, d'un Forms quoi Il a plusieurs buts : il permet d'afficher dans un formulaire \"parent\", les données dans un (ou plusieurs) formulaire(s) enfant il permet d'enregistrer les données enfants ajoutées, en même temps que les données parentes Exemple concret : Des entêtes de facture et des lignes de facture, peuvent être manipulées conjointement Je vais illustrer le présent sujet avec un bout de code tiré tout droit d'un projet tout neuf dont j'ai parlé par ici . L'exemple consistera à afficher un \"Examen\" et les \"détails\" qui l'ont composé. Dans un premier temps les models & formulaire parent Examinations & ExamensForm seuls se composent : du models.py class Examinations ( models . Model ): \"\"\" Examinations \"\"\" user = models . ForeignKey ( User ) examination_types = models . ForeignKey ( ExaminationTypes ) comments = models . TextField () date_examination = models . DateField () hour_examination = models . TimeField ( null = True ) created = models . DateTimeField ( auto_now_add = True ) modified = models . DateTimeField ( auto_now = True ) et du forms.py : class ExamsForm ( forms . ModelForm ): \"\"\" Exams Form \"\"\" # to \" suit \" the HTML textearea comments = forms . CharField ( widget = forms . Textarea ( { 'class' : 'form-control' , 'rows' : '3' })) date_examination = forms . DateField ( widget = forms . TextInput ( attrs = { 'class' : 'form-control' })) hour_examination = forms . TimeField ( widget = forms . TextInput ( attrs = { 'class' : 'form-control' })) def save ( self , user = None ): self . myobject = super ( ExamsForm , self ) . save ( commit = False ) self . myobject . user = user self . myobject . save () class Meta : model = Examinations fields = [ 'examination_types' , 'comments' , 'date_examination' , 'hour_examination' ] exclude = ( 'user' ,) C'est un formulaire tout ce qu'il y a de plus classique qui sera affiché, je vous dispense donc de la CBV pour le gérer. S'ajoute alors les models & formulaire \"enfant\" ExaminationDetails & ExamDetailsForm suivants : models.py : class ExaminationDetails ( models . Model ): \"\"\" ExaminationDetails \"\"\" examination = models . ForeignKey ( Examinations ) title = models . CharField ( max_length = 255 ) value = models . DecimalField ( max_digits = 15 , decimal_places = 5 ) created = models . DateTimeField ( auto_now_add = True ) modified = models . DateTimeField ( auto_now = True ) class Meta : verbose_name = 'Examination Details' verbose_name_plural = 'Examination Details' def show ( self ): return \"Examination Details %s %s %s %s %s \" % ( self . examination_id , self . title , self . value , self . created , self . modified ) def __unicode__ ( self ): return \" %s \" % ( self . title ) Et son petit forms.py : class ExamDetailsForm ( forms . ModelForm ): \"\"\" Details of Exams Form \"\"\" title = forms . CharField ( widget = forms . TextInput ( attrs = { 'class' : 'form-control' })) value = forms . DecimalField ( widget = forms . TextInput ( attrs = { 'class' : 'form-control' , 'type' : 'number' })) class Meta : model = ExaminationDetails fields = [ 'title' , 'value' ] Encore une fois, rien de transcendant là dedans, c'est bateau. Coté doc, sur formset , django nous montre comment on produit des formsets d'un form unique. Me voilà frais avec mes 2 forms distincts :P D'autant que pour corser le tout, j'utilise des ModelForms, pas de simple Forms. En creusant d'avantage, on trouve le sésame via django.forms.models.inlineformset_factory qui traite de ModelForms. Ce qui ceci donne dans mon forms.py : # a formset based on the model of the Parent and Child + 2 new empty extra lines ExamDetailsFormSet = inlineformset_factory ( Examinations , ExaminationDetails , extra = 2 ) Et oui mon bon monsieur, ma bonne dame, ca tient bien en UNE ligne. Ici j'indique à la factory inlineformset, les 2 models qui m'interessent en commençant par le parent et continuant par l'enfant (tout deux finalement liés par ma FK qu'on a vu dans le model plus haut) A présent je dispose d'un FormSet avec Parent et Enfant. Toute la smala va se retrouver dans le même formulaire sous vos yeux ébahis. Maintenant que sont liés les models et que sont définis les forms , coté views.py reste à exploiter le ExamDetailsFormSet . Mais comme je suis \"jusqueboutiste\", je ne me contenterai pas d'une simple fonction pour gérer celui-ci, j'utilise une CreateView , cette CBV donne ceci (explications de texte plus bas): class ExamsCreateView ( CreateView ): \"\"\" to Create Exams \"\"\" form_class = ExamsForm template_name = \"dj_diabetes/exams_form.html\" def form_valid ( self , form ): if self . request . POST : formset = ExamDetailsFormSet ( self . request . POST , instance = self . object ) if formset . is_valid (): self . object = form . save ( user = self . request . user ) formset . instance = self . object formset . save () else : formset = ExamDetailsFormSet ( instance = self . object ) return HttpResponseRedirect ( reverse ( 'exams' )) def get_context_data ( self , ** kw ): data = Examinations . objects . all () . order_by ( '-created' ) #paginator vars record_per_page = 15 page = self . request . GET . get ( 'page' ) # paginator call data = page_it ( data , record_per_page , page ) context = super ( ExamsCreateView , self ) . get_context_data ( ** kw ) context [ 'action' ] = 'add_exam' context [ 'data' ] = data if self . request . POST : context [ 'examsdetails_form' ] = ExamDetailsFormSet ( self . request . POST ) else : context [ 'examsdetails_form' ] = ExamDetailsFormSet ( instance = self . object ) return context dans le template on obtient enfin ceci (attention c'est verbeux, mais zavez un snapshot plus bas ;) : { % extends \"base.html\" % } { % load url from future % } { % load i18n % } { % block title % }{ % trans \"My Glucose Manager\" % }{ % endblock % } { % block content % } { % csrf_token % } {{ form . non_field_errors }} { % if action = 'add_exam' % } { % trans \"Exams\" % } { % else % } { % trans 'Edition of the examination' % } { % endif % } { % if form . examination_types . errors % } {{ form . examination_types . errors }} { % endif % } { % trans \"Type\" % } {{ form . examination_types }} { % if form . comments . errors % } {{ form . comments . errors }} { % endif % } { % trans \"Comments\" % } {{ form . comments }} { % if form . date_examination . errors % } {{ form . date_examination . errors }} { % endif % } { % trans \"Date\" % } {{ form . date_examination }} { % if form . hour_examination . errors % } {{ form . hour_examination . errors }} { % endif % } { % trans \"Hour\" % } {{ form . hour_examination }} { % trans \"Examinations details\" % } {{ examsdetails_form . management_form }} { % trans \"Title\" % } { % trans \"Value\" % } { % for form in examsdetails_form % } {{ form . id }} { % if form . title . errors % } {{ form . title . errors }} { % endif % } {{ form . title }} { % if form . value . errors % } {{ form . value . errors }} { % endif % } {{ form . value }} { % endfor % } { % if action = 'add_exam' % } { % trans \"Add it\" % } { % else % } { % trans \"Edit it\" % } { % endif % } { % trans \"Last examinations\" % } { % trans \"Date\" % } { % trans \"Type\" % } { % trans \"Comments\" % } { % trans \"Actions\" % } { % for line in data % } {{ line . date_examination }} {{ line . examination_types . title }} {{ line . comments }} { % endfor % } { % if data . has_previous % } { % trans \"previous\" % } { % endif % } { % blocktrans with page_number = data . number total_of_pages = data . paginator . num_pages % } Page {{ page_number }} of {{ total_of_pages }} { % endblocktrans % } { % if data . has_next % } { % trans \"next\" % } { % endif % } { % endblock % } { % block extrajs % } //< ! [ CDATA [ $ ( function (){ $ ( '#id_date_examination' ) . datepicker ({ format : 'yyyy-mm-dd' }); }); // ]] > { % endblock % } Explications de texte : Le lecteur averti aura remarqué que ma CBV ne se contente pas que d'afficher un formulaire, puisque dans le \"context\" (modifié dans la methode get_context_data), j'ai rajouté la liste de tous les examens (en les paginant par dessus le marché, je vous ai dit \"jusqueboutiste\"). La page contient donc un formulaire de saisie + la liste complète des examens. Tout cela donne ce rendu : Voilà j'espère que le triplet FormSet de ModelForm et CBV ne sera plus un secret pour vous ;) Edit le 2/7 @ 10:30: on m'a gentiment suggéré un truc pour alléger le code des forms. Faire un form(self.request.POST or None) ce qui transforme ceci def form_valid ( self , form ): if self . request . POST : formset = ExamDetailsFormSet ( self . request . POST , instance = self . object ) if formset . is_valid (): self . object = form . save ( user = self . request . user ) formset . instance = self . object formset . save () else : formset = ExamDetailsFormSet ( instance = self . object ) return HttpResponseRedirect ( reverse ( 'exams' )) en cela def form_valid ( self , form ): formset = ExamDetailsFormSet (( self . request . POST or None ), instance = self . object ) if formset . is_valid (): self . object = form . save ( user = self . request . user ) formset . instance = self . object formset . save () return HttpResponseRedirect ( reverse ( 'exams' )) ca evite tous les tests sur if self.request.POST et instancie le form une fois quelque soit le cas de figure.","tags":"Techno","url":"https://foxmask.net/post/2014/07/01/django-formset-et-match/","loc":"https://foxmask.net/post/2014/07/01/django-formset-et-match/"},{"title":"Gérer son Diabète, il y a une appli web pour ça","text":"En ce weekend de merde (pourri pluvieux) et envie de faire l'ermite, je me suis occupé en rendant un service sur une migration WordPress, et ai produit une application pour gérer son quotidien de diabétique. Non que je ne le sois, mais je m'ennuyais et ai demandé à un pote : \"si tu avais du temps, quelles applications développerais tu ?\" Les réponses furent : un client IRC web avec des fonctions que je tairais pour l'heure, une application pour enregistrer des métriques du quotidien de diabétiques A partir du MCD qu'il m'a fourni j'ai produis \" dj-diabetes \" avec Django et bootstrap3 . Le but étant de savoir, pour le pote, à quel moment il est le \"mieux\". Que ça soit après un exercice, après ou avant un repas etc. Avec ces informations, il peut voir le doc' avec des infos précises et affiner le traitement. En image ça donne ça Aparté : Techniquement, aux amis poneys, j'ai dû m'éclater la tête contre les inlineformset_factory dans des CBV avec des ModelForm. Je referai un billet sur les formset plus tard, là c'était juste pour présenter l'appli elle même ;) ps : l'appli est en constante évolution puisque tout fraiche. Si vous l'utilisez/testez et trouvez des soucis ouvrez un ticket afin que je regarde ce qu'il en est.","tags":"Techno","url":"https://foxmask.net/post/2014/06/29/gerer-son-diabete-il-y-a-une-appli-web-pour-ca/","loc":"https://foxmask.net/post/2014/06/29/gerer-son-diabete-il-y-a-une-appli-web-pour-ca/"},{"title":"Debian Network Manager et Eth0 non gérée","text":"Quand on est une faignasse qui n'a pas envie de se plonger dans les arcanes des fichiers de config pour gérer l'accès réseau et qu'on n'a pas envie de faire ça sur sa workstation, on a un truc qui répond au besoin avec le NetworkManager disponible sous gnome. En général ce dernier est installé mais ne permet pas de configurer sa carte réseau depuis gnome. On a généralement ceci : ](/static/2014/06/lan_ko.png) Du coup on est dans l'incapacité de gérer l'interface réseau de son choix ... En cherchant très très loin dans la doc on trouve une ligne de configuration à switcher de valeur. On editera donc le fichier /etc/NetworkManager/NetworkManager.conf dans lequel on changera le false en true [ifupdown] managed = true Ensuite on relance : # /etc/init.d/network-manager restart [ ok ] Stopping network connection manager: NetworkManager. [ ok ] Starting network connection manager: NetworkManager. Et on obtient enfin ceci : Ce qui nous permet à présent d'ouvrir la config de l'interface elle-même ! Un dernier tips en passant, si vous tripotez votre config réseau via l'interface graphique, mettez de coté le fichier /etc/resolv.conf , afin de ne pas perdre votre résolution DNS au cas où vous vous tromperiez dans votre conf réseau. Dès que vous appliquez vos paramètres, le fichier /etc/resolv.conf est dynamiquement regénéré . Dans un prochain billet, la config d'un accès avec OpenVPN (où j'aborderai de nouveau ces subtilités) par exemple si ça vous botte.","tags":"Techno","url":"https://foxmask.net/post/2014/06/03/debian-network-manager-et-eth0-non-geree/","loc":"https://foxmask.net/post/2014/06/03/debian-network-manager-et-eth0-non-geree/"},{"title":"Django Trigger Happy 0.9.0 is là et Python 3.4.0","text":"this article is also available in english here Intro : Trigger Happy est un project (ecrit en python avec django ) opensource qui a pour but d'être une alternative libre de IFTTT.com . Trigger Happy peut être défini comme un micro ESB . And here we go (again:) : Il y a 2 mois je publiais la dernière version \"uniquement\" compatible python 2.7.x Depuis la sortie de Python 3.4.0 et quelques intéressantes fonctionnalités (que j'essaye encore d'exploiter pour améliorer Trigger Happy), j'ai décidé que le moment de sauter le pas vers Python 3 était venu avec Django Trigger Happy. C'est maintenant chose faite. Après 2 mois à creuser si tous les services que j'utilisais étaient compatibles, j'ai fini par publier la version 0.9.0 . Donc dans cette version, peu de chose du core ont changé (quasiment rien en fait). J'ai essentiellement consolidé le code existant pour qu'il soit utilisable avec Python 3.4.x et les lib tierces pour chaque service que nous souhaitons utiliser comme Evernote , Pocket , Readability . Actuellement, seul Pocket fourni une version installable pour Python 3 version depuis la command pip Evernote fourni aussi une version pour Python 3 mais seulement depuis github , car aucune version finale et officielle n'existe sur Pypi, on doit donc l'installer à la main ce qui n'est pas super pratique comparer à pip. Pour readability ca devrait être dispo assez vite . Tout ceci justifie le choix de permuter (de Evernote à Pocket) le service par défaut utilisé par Trigger Happy pour stocker vos news (par exemple ou tout autre chose). Pour ce qui est de la partie front/web, j'ai migré de Bootstrap 2 à 3 et ajouté quelques petites choses pour que l'appli soit plus facile d'utilisation. Enfin tout n'a donc pas été sans mal mais ça marche bien à présent. Read the docs : J'ai enfin publié la doc sur readthedocs . Au cas où ;) Roadmap : Et pour la suite ? Améliorer Trigger Happy pour qu'il soit plus rapide en utilisant asyncio ou un équivalent Améliorer l'UI de Trigger Happy. Quand vous regardez IFTTT et Trigger Happy, vous pouvez facilement imaginer la somme de travail à accomplir pour atteindre le même résultat. Mais comme je ne suis un designer je fais des choses simple, mais des choses qui marchent Nouveau(x) service(s) ? : Il y a quelque mois j'avais émis un sondage pour savoir quel service vous aimeriez le plus utilisez avec Trigger Happy, et le gagnant fut Twitter, mais je n'étais pas très motivé pour m'y mettre. Je pense qu'à présent je vais pouvoir y retourner ;) D'autres idées dans votre propre liste ? Vous pouvez aussi forker le projet , contribuer, reporter des bugs edit: dans le pipe j'ai débuté une lib pour traiter des imports/exports OPML","tags":"Techno","url":"https://foxmask.net/post/2014/05/14/django-trigger-happy-0-9-0-is-la-python-3-4-0/","loc":"https://foxmask.net/post/2014/05/14/django-trigger-happy-0-9-0-is-la-python-3-4-0/"},{"title":"Django Trigger Happy 0.9.0 is out and Python 3.4.0","text":"cet article est aussi dispo en français ici Intro : Trigger Happy is a project (written in python with django ) which aims to be a free and opensource alternative to IFTTT.com . Trigger Happy can be defined as a micro ESB . And here we go (again:) : 2 months ago I published the version 0.8.3 which was the last release working only with Python 2.7.x. Since the release of Python 3.4.0 and some very interesting features (that I still try to exploit to improve Trigger Happy), I decided that was the moment to dive into Python 3 with Django Trigger Happy. It's now a thing done. After 2 month to dig if all the existing services I used were compatible, I finished by releasing a 0.9.0 . So in the version, a few things changed in the core (almost nothing in fact). I just consolidated the most part of the code to be able to be used with python 3.4.x and third party lib for each service we would like to use like Evernote , Pocket , Readability . Actually just Pocket provides a Python 3 version installable from pip command. Evernote provides also a Python 3 version but only from github , as no final official release from Pypi exists yet, we have to install it by hand, which is not very pretty simple compared to pip. For readability It should be Ok for python 3 too, soon . All of this justify the choice to switch from Evernote to Pocket as the default service used by Trigger Happy to store your news/stuff/whatever. About the Front part, I migrated from Bootstrap 2 to 3 and add some little things to be easier to use. All was not painless but now everything works fine (again;) Read the docs : I also pushed the doc on readthedocs . Roadmap : What Do I plan now ? Improving Trigger Happy to be faster by using asyncio or equivalent Improving the UI of Trigger Happy. When you see IFTTT and Trigger Happy, you can imagine the work to do to reach the same UX. But as i'm not a designer I do simple things, but things that work New service(s) ? : Some months ago I did a poll to know which website/service you will use with Trigger Happy, the winner was Twitter, but I was not motivated enough to do it. I think the time has come to try again to manage it Some ideas from your wishlist ? feel free to fork it , contribute, report bug","tags":"Techno","url":"https://foxmask.net/post/2014/05/14/django-trigger-happy-0-9-0-is-out-python-3-4-0/","loc":"https://foxmask.net/post/2014/05/14/django-trigger-happy-0-9-0-is-out-python-3-4-0/"},{"title":"Django filtrer les données d'une ListView avec Q()","text":"J'étais parti pour vous faire un article funny avec James Bond, Monneypenny, et Q, mais restons soft ;) Donc dans cet article je vais aborder une fonction toute \"simple\" et ultra pratique : Lookup with Q Objects . Le but de cette fonction est de gérer des requêtes SQL pour produire quelque chose du genre : SELECT col1 , col2 FROM table1 WHERE user = 'foobar' AND ( col1 = '2' OR col2 = '2' ); C'est la partie après le AND qui est gérée par Q() Ca serait simple si on se contentait d'utiliser cela dans une FBV en récupérant le paramètre nommé dans urls.py, mais là, chez bibi, c'est dans une CBV ListView que ça va servir, donc un poil plus subtile que ça. Dans la version précédente ma ListView avait cette tête : class TriggerListView ( ListView ): context_object_name = \"triggers_list\" queryset = TriggerService . objects . all () template_name = \"home.html\" paginate_by = 7 def get_queryset ( self ): # get the Trigger of the connected user if self . request . user . is_authenticated (): return self . queryset . filter ( user = self . request . user ) . order_by ( '-date_created' ) # otherwise return nothing return TriggerService . objects . none () Là, aucun filtrage des données depuis le template, affichage pour et simple de toutes les données de l'utilisateur connecté. L'ajout du filtrage va donc toucher les éléments suivants : views.py pour l'ajout des données peuplant la liste depuis get_context_data de la ListView le template pour afficher la liste des valeurs permettant le filtrage urls.py pour ajouter le mapping url/views views.py pour ajouter la logique de traitement de filtrage dans la methode get_queryset de la ListView 1 - Ajout des données peuplant la liste du point 1 class TriggerListView ( ListView ): \"\"\" list of Triggers the list can be filtered by service \"\"\" context_object_name = \"triggers_list\" queryset = TriggerService . objects . all () template_name = \"home.html\" paginate_by = 7 def get_context_data ( self , ** kw ): \"\"\" List of triggers activated by the user \"\"\" if self . request . user . is_authenticated (): context [ 'trigger_filter_by' ] = UserService . objects . filter ( user = self . request . user ) return context 2 - Template, affichage de la liste pour filtrage { % for trigger_filter in trigger_filter_by % } {{ trigger_filter . name | service_readable }} { % endfor % } 3 - urls.py url ( r '&#94;th/$' , TriggerListView . as_view (), name = 'base' ), #ajouté pour gerer le filtrage url ( r '&#94;th/trigger/by/(?P[a-zA-Z]+)$' , TriggerListView . as_view (), name = 'trigger_filter_by' ), 4 - Logique de traitement du filtrage class TriggerListView ( ListView ): \"\"\" list of Triggers the list can be filtered by service \"\"\" context_object_name = \"triggers_list\" queryset = TriggerService . objects . all () template_name = \"home.html\" paginate_by = 7 def get_queryset ( self ): trigger_filter_by = None # get the Trigger of the connected user if self . request . user . is_authenticated (): # if the user selected a filter, get its ID if 'trigger_filter_by' in self . kwargs : user_service = UserService . objects . filter ( user = self . request . user , name = self . kwargs [ 'trigger_filter_by' ]) trigger_filter_by = user_service [ 0 ] . id # no filter selected : display all if trigger_filter_by is None : return self . queryset . filter ( user = self . request . user ) . order_by ( '-date_created' ) # filter selected : display all related trigger else : # here the queryset will do : # 1) get trigger of the connected user AND # 2) get the triggers where the provider OR the consumer match # the selected service return self . queryset . filter ( user = self . request . user ) . filter ( Q ( provider = trigger_filter_by ) | Q ( consumer = trigger_filter_by )) . order_by ( '-date_created' ) # otherwise return nothing when user is not connected return TriggerService . objects . none () Ainsi donc ici on voit où entre en compte le Q objects : . filter ( Q ( provider = trigger_filter_by ) | Q ( consumer = trigger_filter_by ) ceci produit le \"OR\" attendu puisque je souhaite bel et bien TOUS les triggers de l'utilisateur connecté ET soit les provider soit les consumer contenant la valeur recherchée dans la liste déroulante. Avant d'arriver à cette solution, je cherchais à ajouter un Form à ListView pour produire une dropdown (un select html)... mais vu la complexité du code pour gérer l'ajout du form au context ou pas, je me suis rabattu sur une simple liste html, et hop Q() à la rescousse ;) Tout est dans la \"Simplicity, Efficiency, Beauty\" ;) si besoin, pour les curieux, Le code source original de cette Views.py sur Github","tags":"Techno","url":"https://foxmask.net/post/2014/05/06/django-filtrer-listview/","loc":"https://foxmask.net/post/2014/05/06/django-filtrer-listview/"},{"title":"export DISPLAY from a VMWare virtual machine","text":"I have to install an Oracle server on a remote machine. And ... It's not my first time :P As we need to use a GUI Wizard, usually we do an export of the DISPLAY to a computer which can handle the X session and then everything works great... Until now. I have 2 virtual machine, one with VMWare one with another VM system (KVM or XEN ; i dont remember ;) both with a CentOS 6.5 with SELinux disable and no firewalling setup Both have a role of oracle server. My installation on the \"non-VMWare\" worked fine with the usual process : xhost + new_server ssh -X -Y login@new_server export DISPLAY = workstation_ip:0.0 cd /u01/app/oracle/database ./runInstaller and I received the wizard on my workstation. Now, with the VMWare Virtual machine, the same process fails when starting anything that require a X, even xclock for example. So I compared the ssh config of the both server, to check if the forwarding is set to on, but are the same. So I am asking myself if there is no config to set up on the VMWare virtual machine to permit the X session ?","tags":"Techno","url":"https://foxmask.net/post/2014/04/17/export-display-from-vmware/","loc":"https://foxmask.net/post/2014/04/17/export-display-from-vmware/"},{"title":"Trigger Happy, l'avenir en bleu","text":"Hey ! Suite à la publication de mon projet Trigger Happy , permettant à tout à chacun de se gérer l'agrégation des flux d'informations avec son client favori (Evernote, Pocket, ReadItLater, Feedly, Wallabag ), un géant du web m'a contacté pour que je le rejoigne et permette d'inclure cette fonctionnalité à plus grand échelle, pour ses clients/utilisateurs. Ainsi ces derniers pourront peupler de news, leur mur, sans lever le petit doigt. Pour le coup, Feedly va prendre un grand coup en pleine tête et Facebook va pouvoir grapiller les utilisateurs de Google Reader restés sur le carreau ou déçus des alternatives existantes. Dans le même temps je vais rejoindre Nicosomb , parti vers ces mêmes cieux ! En effet nous avions déjà échangé sur nos projets respectifs qui s'interconnecteraient parfaitement avec l'API de la v2 . Ca sera l'occasion de concrétiser tout cela ensemble. Trigger Happy works with @Pocket . Next step: the same thing with #poche http://t.co/Xju8WGOuA8 /cc @foxmask — wallabag (@wallabagapp) 3 Décembre 2013","tags":"Techno","url":"https://foxmask.net/post/2014/04/01/trigger-happy-avenir-en-bleu/","loc":"https://foxmask.net/post/2014/04/01/trigger-happy-avenir-en-bleu/"},{"title":"DjangoCong.euh 2014 rencontrez Sam et Max","text":"Intro: Après m'être lancé dans l'univers Python et Django récemment, comme j'apprécie énormément la communauté, et afin de lui rendre tout ce qu'elle m'apporte, voici le premier évènement que j'organise. En détails : Devenez Ludovisiens pour la DjangoCong.euh 2014.4.1 qui se tiendra à \" Mon Vieil Ami \" ( Métro Pont Marie ) le 31 avril 2014. Dans un cadre très touristique où tout à chacun pourra à loisir se laisser allez à écouter parler de poneys dans un programme des plus dense . Et last but not least, en Guest Star Sam et Max nous parleront de leur expérience avec notre Poney favori. Tarif ? Gratuit ! Sponsors ? plein ! : là , là , là , là , et enfin là","tags":"Techno","url":"https://foxmask.net/post/2014/04/01/djangocong-euh-2014-rencontrez-sametmax/","loc":"https://foxmask.net/post/2014/04/01/djangocong-euh-2014-rencontrez-sametmax/"},{"title":"extraire une portion d'une archive distante sans la télécharger","text":"Bon c'est certes pas nouveau mais à force de la chercher, alors que je m'en servais à un temps que les moins de 20 ans ne peuvent pas connaitre, je la mets là, c'est pas pour emporter c'est pour manger tout de suite. Le truc ici va consister à être une partie d'une archive sans avoir à la télécharger. On fait comme ca : wget -O - url | tar -xvzf - [ dossier | path/vers/fichier ] je vous laisse jouer sur les options de tar mais l'idée est là ;) genre ça fait quelques années lumières que je n'ai pas vu la tête d'une archive du kernel linux de près, let's go dancing : wget -O - ftp://ftp.kernel.org/pub/linux/kernel/v3.x/testing/linux-3.11-rc6.tar.gz | tar tvzf - --2014-03-25 01 :42:05-- ftp://ftp.kernel.org/pub/linux/kernel/v3.x/testing/linux-3.11-rc6.tar.gz = > «-» Résolution de ftp.kernel.org ( ftp.kernel.org ) ... 198 .145.20.140, 149 .20.4.69, 199 .204.44.194 Connexion vers ftp.kernel.org ( ftp.kernel.org ) | 198 .145.20.140 | :21...connecté. Ouverture de session en anonymous...Session établie! == > SYST ... complété. == > PWD ... complété. == > TYPE I ... complété. == > CWD ( 1 ) /pub/linux/kernel/v3.x/testing ... complété. == > SIZE linux-3.11-rc6.tar.gz ... 113162100 == > PASV ... complété. == > RETR linux-3.11-rc6.tar.gz ... complété. Taille: 113162100 ( 108M ) ( non certifiée ) 0 % [ ] 37 648 183K/s drwxrwxr-x root/root 0 2013 -08-18 23 :36 linux-3.11-rc6/ -rw-rw-r-- root/root 1097 2013 -08-18 23 :36 linux-3.11-rc6/.gitignore -rw-rw-r-- root/root 4465 2013 -08-18 23 :36 linux-3.11-rc6/.mailmap -rw-rw-r-- root/root 18693 2013 -08-18 23 :36 linux-3.11-rc6/COPYING -rw-rw-r-- root/root 95317 2013 -08-18 23 :36 linux-3.11-rc6/CREDITS drwxrwxr-x root/root 0 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ -rw-rw-r-- root/root 107 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/.gitignore -rw-rw-r-- root/root 16957 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/00-INDEX drwxrwxr-x root/root 0 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/ -rw-rw-r-- root/root 3284 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/README drwxrwxr-x root/root 0 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/obsolete/ -rw-rw-r-- root/root 248 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/obsolete/proc-sys-vm-nr_pdflush_threads -rw-rw-r-- root/root 1296 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/obsolete/sysfs-bus-usb -rw-rw-r-- root/root 1063 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/obsolete/sysfs-class-rfkill -rw-rw-r-- root/root 2820 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/obsolete/sysfs-driver-hid-roccat-koneplus -rw-rw-r-- root/root 3657 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/obsolete/sysfs-driver-hid-roccat-kovaplus -rw-rw-r-- root/root 3767 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/obsolete/sysfs-driver-hid-roccat-pyra 0 % [ ] 79 640 183K/s drwxrwxr-x root/root 0 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/removed/ -rw-rw-r-- root/root 471 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/removed/devfs -rw-rw-r-- root/root 664 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/removed/dv1394 -rw-rw-r-- root/root 310 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/removed/ip_queue -rw-rw-r-- root/root 449 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/removed/o2cb -rw-rw-r-- root/root 663 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/removed/raw1394 -rw-rw-r-- root/root 751 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/removed/video1394 drwxrwxr-x root/root 0 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/stable/ -rw-rw-r-- root/root 4140 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/stable/firewire-cdev Vous pouvez faire varier les plaisirs avec un grep au bout evidement, genre je ne veux que ce qui cause crypto : wget -O - ftp://ftp.kernel.org/pub/linux/kernel/v3.x/testing/linux-3.11-rc6.tar.gz | tar tvzf - | grep crypto --2014-03-25 01 :50:56-- ftp://ftp.kernel.org/pub/linux/kernel/v3.x/testing/linux-3.11-rc6.tar.gz = > «-» Résolution de ftp.kernel.org ( ftp.kernel.org ) ... 199 .204.44.194, 198 .145.20.140, 149 .20.4.69 Connexion vers ftp.kernel.org ( ftp.kernel.org ) | 199 .204.44.194 | :21...connecté. Ouverture de session en anonymous...Session établie! == > SYST ... complété. == > PWD ... complété. == > TYPE I ... complété. == > CWD ( 1 ) /pub/linux/kernel/v3.x/testing ... complété. == > SIZE linux-3.11-rc6.tar.gz ... 113162100 == > PASV ... complété. == > RETR linux-3.11-rc6.tar.gz ... complété. Taille: 113162100 ( 108M ) ( non certifiée ) 0 % [ ] 41 992 166K/s -rw-rw-r-- root/root 1188 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/ABI/testing/debugfs-pfo-nx-crypto 1 % [ > ] 1 369 808 828K/s drwxrwxr-x root/root 0 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/crypto/ -rw-rw-r-- root/root 6569 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/crypto/api-intro.txt -rw-rw-r-- root/root 11244 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/crypto/asymmetric-keys.txt -rw-rw-r-- root/root 9347 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/crypto/async-tx-api.txt -rw-rw-r-- root/root 17200 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/crypto/descore-readme.txt drwxrwxr-x root/root 0 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/devicetree/bindings/crypto/ -rw-rw-r-- root/root 399 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/devicetree/bindings/crypto/fsl-imx-sahara.txt -rw-rw-r-- root/root 2784 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/devicetree/bindings/crypto/fsl-sec2.txt -rw-rw-r-- root/root 14202 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/devicetree/bindings/crypto/fsl-sec4.txt -rw-rw-r-- root/root 544 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/devicetree/bindings/crypto/mv_cesa.txt -rw-rw-r-- root/root 789 2013 -08-18 23 :36 linux-3.11-rc6/Documentation/devicetree/bindings/crypto/picochip-spacc.txt 2 % [== > ] 3 075 552 1 ,31M/s L'intérêt est de ne pas encombrer son HDD de saloperies \"temporaires\" qui finissent toujours par durer. Voilou, je n'ai plus à me demander \"boudiou c'est comment que je faisais dans l'ancien temps avec mes doigts\"","tags":"Techno","url":"https://foxmask.net/post/2014/03/27/extraire-une-portion-dune-archive-distante-sans-la-telecharger/","loc":"https://foxmask.net/post/2014/03/27/extraire-une-portion-dune-archive-distante-sans-la-telecharger/"},{"title":"Trigger Happy, lire plus tard avec agilité","text":"En cette nouvelle année, voici venu un nouveau service géré par Trigger Happy , qu'est Readability après avoir intégrés Evernote et Pocket voici un extrait depuis Readability : et les données reçues de Readability dans Pocket : L'inverse fonctionne également ;) Les Sources du projets sont par ici Les services existants : Ainsi donc cela fait 4 services gérés : RSS , Evernote , Pocket , Readability . Chacun d'eux pouvant échanger des informations entre eux, 2 à 2. Dans la foulée j'en ai profité pour sortir une nouvelle version de tous les services existants. Vous pouvez tous les retrouver sur Pypi En cadeau bonusque ; j'ai produit un module django-th-dummy que vous pouvez simplement cloner pour produire un nouveau module pour Trigger Happy les doigts dans le nez (atchoum!) :P Ce petit module vient en complément du billet précédant \" comment pondre son propre module \". Vous n'avez à présent plus aucune excuse pour ne pas produire votre propre module et enfin prendre en main votre trousseau d'autorisation d'accès entre vos services. (merci prism :P) Aparté En passant je suis allé voir Feedly.com (par curiosité pour trouver l'inspiration :) donc si vous utilisez feedly.com, il vous en coutera 5\\$/mois pour obtenir vos nouvelles automatiquement envoyées dans vos compte Evernote et Pocket ... Ca laisse songeur hein ? cloudWord et autres services similaires se régalent. 2014 le changement (si vous voulez vous en donner la peine/plaisir de l'installer pour le tester... c'est mon cadeau de ce nouvel an) ;)","tags":"Techno","url":"https://foxmask.net/post/2014/01/06/trigger-happy-lire-plus-tard-avec-agitilite/","loc":"https://foxmask.net/post/2014/01/06/trigger-happy-lire-plus-tard-avec-agitilite/"},{"title":"Trigger Happy met un éléphant dans sa poche","text":"Voilà voilà... Si vous découvrez tout juste \"Trigger Happy\", en voici une introduction préalable ;) Après avoir bien intégrés Evernote et RSS ensemble avec Trigger Happy , j'ai étendu le noyau du service afin de pouvoir à présent intégrer n'importe quel service. Le nouveau venu est Pocket . A présent donc tout ce qui vient des flux RSS de votre choix et/ou de votre compte Evernote se retrouvera dans votre Pocket et inversement tout ce qui est mis dans Pocket tombera dans le carnet de notes de votre choix dans Evernote. Voici en images le résultat du passage de l'un à l'autre dans les 2 sens. De Pocket à Evernote ... depuis Pocket : Via la console ... ... arrivée dans Evernote l'inverse se produit également bien évidement.","tags":"Techno","url":"https://foxmask.net/post/2013/12/17/trigger-happy-met-un-elephant-dans-sa-poche/","loc":"https://foxmask.net/post/2013/12/17/trigger-happy-met-un-elephant-dans-sa-poche/"},{"title":"Trigger Happy comment pondre son propre module","text":"Intro: Tout comme vous avez des outils pour produire vos sites avec des CMS ou Blogs, Trigger Happy entre dans une nouvelle catégorie d'outils vous permettant de gérer cette fois ci l'interconnexion de services internet, en fonction d'évènements : sur la toile sur vos propres comptes \"internet\". Exemples : Quand une nouvelle est publiée sur un site web, je souhaite qu'elle soit stockée dans mon compte Evernote ou Pocket, publiée sur Twitter, son mur Facebook, etc... Quand j'ajoute une note dans mon compte Evernote, je souhaite l'envoyer dans mon compte Pocket Quand un tweet sur un sujet tombe, l'envoyer dans mon compte Pocket/Evernote/Whatever etc... Tout ceci est très largement inspiré du très bon service IFTTT permettant ces échanges. Ce qui va suivre va donc vous montrer (avec du code \"oui-oui\"), comment (vous) \"brancher\" un service de plus avec Trigger Happy et (presque) ne plus vous en préoccuper ;) Pré-requis: Ce qu'il vous faut posséder en premier lieu pour produire votre module Trigger-Happy : Connaitre Django un compte sur le service \"cible\" (c'est possible sans, mais c'est mieux avec, on va dire hein) et ... de l'huile de coude (encore que ça ne soit pas la mer à boire après ce qui suit) Quick start: si vous êtes pressé, le plus simple est encore de voir le README du module Trigger Happy : Pocket , de cloner le module et de changer partout \"pocket\" par \"montrucquidechireduponey\" partout ;) Architecturationesse: Le service que vous produirez se compose comme un module python. Pour bien visualiser les explications ci dessous je prendrai comme exemple django-th-pocket . L'architecturationnesseeee en elle-même est identique à celle d'un dossier d'un module django. Exemple : th_pocket / : forms . py __init__ . py models . py my_pocket . py th_pocket / templates / pocketform : wz - 1 - form . html wz - 3 - form . html th_pocket / templates / pocketprovider : edit_provider . html En détails, fichier par fichier Pas de panique c'est court ! En premier le modèle : models.py # -*- coding: utf-8 -*- from django.db import models from django_th.models.services import Services class Pocket ( Services ): tag = models . CharField ( max_length = 80 , blank = True ) url = models . URLField ( max_length = 255 ) title = models . CharField ( max_length = 80 , blank = True ) tweet_id = models . CharField ( max_length = 80 , blank = True ) trigger = models . ForeignKey ( 'TriggerService' ) class Meta : app_label = 'django_th' ici 2 impératifs, 1) appeler le modèle service de Trigger Happy pour en hériter, soit : from django_th.models.services import Services class Pocket ( Services ): ... 2) pour la lisibilite de votre base mettre un app_label cohérent, soit : class Meta : app_label = 'django_th' En second, le formulaire : forms.py C'est un simple formulaire Django, un ModelForm banal, qui appellera le modèle susmentionné # -*- coding: utf-8 -*- from django import forms from th_pocket.models import Pocket class PocketForm ( forms . ModelForm ): \"\"\" for to handle Pocket service \"\"\" class Meta : model = Pocket fields = ( 'tag' ,) class PocketProviderForm ( PocketForm ): pass class PocketConsummerForm ( PocketForm ): pass Ici en sus, on fournit 2 classes supplémentaires PocketProviderForm et PocketConsummerForm pour permettre lors de la création d'un Trigger, de disposer du formulaire de Pocket, soit quand on décide que Pocket est la source de données (donc le Provider) soit la cible (le Consummer) les Templates Ensuite arrive les templates situés dans th_pocket/templates/pocketform. Ceux ci sont curieusement nommé wz-1-form.html et wz-3-form.html Explications : Trigger Happy créé ses triggers avec un Wizard constitué de 5 étapes. Etape 1 je choisi le service source (wz-0-form.html) Etape 2 j'indique où sont les informations du service choisi étape 1 (wz-1-form.html) Etape 3 je choisi le service cible (wz-2-form.html) Etape 4 je saisie les infos de stockage du service choisi étape 3 (wz-3-form.html) Etape 5 je nomme mon Trigger ;) (wz-4-form.html) Les 2 formulaires ici sont donc pour les étapes 2 et 4. Quasiment tous les services peuvent servir de source ou cible, d'où la présence de ces 2 formulaires. Ainsi en utilisant comme source de données Evernote j'appellerai le form th_evernote/templates/evernoteform/wz-1-form.html puis pour Pocket comme \"cible\" th_pocket/templates/pocketform/wz-3-form.html. Le dernier formulaire th_pocket/templates/pocketprovider/edit_provider.html permet quant à lui de modifier des informations relatives à Pocket pour le Trigger voulu. Chaque service disposant de son propre template pour la même fonction. la Glue Tout ceci permet d'indiquer, via les formulaires du wizard, d'où vient l'information et où je la stocke. Mais il manque le plus important, la glue entre tout ceci. C'est là le le rôle du module my_pocket.py Avant d'éventuellement vous faire mal aux yeux, je vais vous écrire en français dans le texte, le but du module : Celui ci permet 3 choses : ) authentifier \"son\" Trigger Happy aupres de Pocket (via la methode auth) ) récupérer les données depuis son compte Pocket, en l'occurence, les URL qu'on y aura mises (via la méthode process_data) ) enregistrer les données provenant d'un service 'Provider' (via la méthode save_data) # -*- coding: utf-8 -*- # django_th classes from django_th.services.services import ServicesMgr from django_th.models import UserService , ServicesActivated # django classes from django.conf import settings from django.core.urlresolvers import reverse from django.utils.log import getLogger # pocket API import pocket from pocket import Pocket import datetime import time \"\"\" handle process with pocket put the following in settings.py TH_POCKET = { 'consummer_key': 'abcdefghijklmnopqrstuvwxyz', } \"\"\" logger = getLogger ( 'django_th.trigger_happy' ) class ServicePocket ( ServicesMgr ): def process_data ( self , token , trigger_id , date_triggered ): \"\"\" get the data from the service \"\"\" datas = list () # pocket uses a timestamp date format date_triggered = int ( time . mktime ( datetime . datetime . timetuple ( date_triggered ))) if token is not None : pocket_instance = pocket . Pocket ( settings . TH_POCKET [ 'consummer_key' ], token ) # get the data from the last time the trigger have been started # timestamp form pockets = pocket_instance . get ( since = date_triggered ) if len ( pockets [ 0 ][ 'list' ]) > 0 : for pocket in pockets [ 0 ][ 'list' ] . values (): datas . append ({ 'tag' : '' , 'link' : pocket [ 'resolved_url' ], 'title' : pocket [ 'resolved_title' ], 'tweet_id' : 0 }) return datas def save_data ( self , token , trigger_id , ** data ): \"\"\" let's save the data \"\"\" from th_pocket.models import Pocket if token and len ( data [ 'link' ]) > 0 : # get the pocket data of this trigger trigger = Pocket . objects . get ( trigger_id = trigger_id ) pocket_instance = pocket . Pocket ( settings . TH_POCKET [ 'consummer_key' ], token ) title = '' title = ( data [ 'title' ] if 'title' in data else '' ) item_id = pocket_instance . add ( url = data [ 'link' ], title = title , tags = ( trigger . tag . lower ())) sentance = str ( 'pocket {} created' ) . format ( data [ 'link' ]) logger . debug ( sentance ) else : logger . critical ( \"no token provided for trigger ID %s and link %s \" , trigger_id , data [ 'link' ]) def auth ( self , request ): \"\"\" let's auth the user to the Service \"\"\" callbackUrl = 'http:// %s%s ' % ( request . get_host (), reverse ( 'pocket_callback' )) request_token = Pocket . get_request_token ( consumer_key = settings . TH_POCKET [ 'consummer_key' ], redirect_uri = callbackUrl ) # Save the request token information for later request . session [ 'request_token' ] = request_token # URL to redirect user to, to authorize your app auth_url = Pocket . get_auth_url ( code = request_token , redirect_uri = callbackUrl ) return auth_url def callback ( self , request ): \"\"\" Called from the Service when the user accept to activate it \"\"\" try : # finally we save the user auth token # As we already stored the object ServicesActivated # from the UserServiceCreateView now we update the same # object to the database so : # 1) we get the previous objet us = UserService . objects . get ( user = request . user , name = ServicesActivated . objects . get ( name = 'ServicePocket' )) # 2) then get the token access_token = Pocket . get_access_token ( consumer_key = settings . TH_POCKET [ 'consummer_key' ], code = request . session [ 'request_token' ]) us . token = access_token # 3) and save everything us . save () except KeyError : return '/' return 'pocket/callback.html' enfin comme on peut l'apercevoir dans le code, des appels à la configuration du module ont lieu, voici donc à quoi elle ressemblerait pour le module Trigger Happy Pocket: settings.py TH_SERVICES = ( ... 'th_pocket.my_pocket.ServicePocket' , ... ) INSTALLED_APPS = ( .... 'th_pocket' , ... ) # votre clé perso fournie par le service POCKET TH_POCKET = { 'consumer_key' : 'abcdefghijklmnopqrstuvwxyz' , } Si vous ne connaissez pas IFTTT et/ou pour les plus courageux qui se sont donnés la peine de lire jusque là et/ou les curieux de comprendre comme la machinerie tourne : Sous le capot : mais comment ça marche-t-il-tu-c'truc ? Il y a 2 parties: la première visible, vous permet de \"brancher\" vos services entre eux via un wizard de 5 pages et de voir quand tel ou tel service a fourni des nouvelles neuves ;) la seconde est un batch qui tourne à intervalle régulier qui va s'occuper d'aller récupérer ses fameuses \"nouvelles neuves\" et les expédier là où vous l'avez décidé Comment ce batch s'en sort, alors que je ne sais pas d'avance quels sont les services existants ? Ceci est géré par le biais de la variable TH_SERVICES, indiquant les services que VOUS avez ajouté (tout comme vous mettez le nom de l'application \"django\" dans INSTALLED_APPS). Ensuite \"Trigger Happy\" va utiliser cette variable pour vous permettre de vous en servir dans le Wizard mentionné plus tôt. Ensuite le batch récupère ces services activés par vos soins, et utilise les \"Glues\" de chaque service existant my_pocket, my_evernote, etc, et joue son scénario ! Voilà ! A présent vous êtes devenu maitre de vos données persos !","tags":"Techno","url":"https://foxmask.net/post/2013/12/09/trigger-happy-comment-pondre-son-propre-module/","loc":"https://foxmask.net/post/2013/12/09/trigger-happy-comment-pondre-son-propre-module/"},{"title":"Trigger Happy in ze Pocket","text":"Il y a peu, je lançais un sondage , pour savoir, vers quel outil orienté mes prochains développements de \"Trigger Happy\" (le \"machin bidule truc chouette\" pour agréger soit même des news qui vous sied, de toute la toile, dans l'outil de votre choix, un truc comme le service IFTTT \"If This Then That\"). Malgré le résultat écrasant pour Twitter, que j'ai commencé à traiter, je me suis occupé de Pocket , un autre \"machin chose\" qui vous permet de stocker des URLs pour, plus tard, aller lire ce que vous avez glané dans le journée et mis de coté parce que pas le temps de lire. Et là..., j'ai pu le tester avec (un certain) succès (jouissif sisi:). A présent \"Trigger Happy\" sait : Lire un flux RSS et envoyer ce qu'il ramasse aussi bien sur votre compte Pocket que sur votre compte Evernote . Je vais tester que je puisse faire communiquer mes comptes Pocket et Evernote entre eux. Genre quand une URL tombe sur Pocket, l'ajouter à son compte Evernote, ou quand une note est créée, AVEC une URL, qu'elle soit ajoutée à son compte Pocket. Pocket ne gérant que des URLs, si on en n'a pas ; pas de bras pas de chocolat. Enfin vu l'étonnante facilité d'intégration de l'api Pocket, c'est le pied intégral, de voir le truc fonctionner du 1ier coup. (surtout quand on doit déclencher une demande d'autorisation de connexion à son compte Pocket/Evernote depuis Trigger Happy et que _ça_ marche). Ceci est de bon augure pour intégrer Poche , une version libre du sieur @nico_somb quand la v2 poindra le bout de son tarin ;)","tags":"Techno","url":"https://foxmask.net/post/2013/12/03/trigger-happy-in-ze-pocket/","loc":"https://foxmask.net/post/2013/12/03/trigger-happy-in-ze-pocket/"},{"title":"SQL Full Table Scan vs Index - The Usual Suspects","text":"Hello, Voici un condensé sur ma semaine de formation au Tuning Oracle ;) Un condensé de conneries à proscrire pour avoir un RDBMS qui marche mieux du point de vue du dev, tout du moins, parce que je n'irai pas saouler le dev avec des noms barbares sur la ram et tout le bataclan :) Ainsi voici une liste de requêtes SQL à éviter à tout prix au risque de vous retrouver à faire faire des full scan sur vos \"petites\" tables. Requête moisie n°1 SELECT * FROM salarie WHERE salaire * 12 > 6000 ici on voudrait les salariés ayant un salaire mensuel supérieur à 60000 par an. Or, quand bien même salaire serait un index, Oracle partirait bel et bien en Full Scan. La preuve avec l'exécution du \"plan d'exécution\" aka \"explain plan\" qui vous le sortira la preuve par neuf ;) Pour que cette requête fasse son effet avec l'index il faut l'écrire ainsi : SELECT * FROM salarie WHERE salaire > 60000 / 12 Requête moisie n°2 Un autre exemple avec celle-ci SELECT * FROM table WHERE upper ( name ) = 'FOXMASK' les fonctions sur les index partent en full scan systématiquement. Si vous tenez à faire cela, il faut explicitement pondre un \"function index\" avec, lui aussi upper(name) dessus. CREATE INDEX func_upper ON TABLE ( UPPER ( name )); Requête moisie n°3 sur index non discriminant Autre cas de figure avec des index inutiles : Les tables avec des données dans une colonne insuffisamment discriminante provoquera un full scan car le moteur du RDBMS considérera moins coûteux le full scan que l'index qui n'est pas assez discriminant. Par exemple une colonne \"sexe\" pour une table \"profil\". Encore une fois si vous tenez à cet index, il faudra alors le définir différemment : en BITMAP et là ça \"déchirera\" :) Cadeau Bonusque en cadeau bonusque pour ceux ce que ca nain téreeserrait, 2 trucs : 1) Cas d'une requête entre grosses tables : postulat : j'ai 2 tables une A, de quelques millions de lignes, une B de quelque milliers de lignes SELECT * FROM A , B where A . id = B . id AND B . status = 1 Là, la requête c'est de la bonne daube puisqu'on se farcie l'inner join pour chaque ID de B et A . Or l'ordre dans la clause where compte. Donc pour limiter le coût de la requête il faudrait la faire comme suit : SELECT * FROM A , B where B . status = 1 AND A . id = B . id Évidemment, le moteur du RDBMS devrait trouver le plan d'exécution le plus adapté mais des fois ce n'est pas le cas, et il faut en passer par des HINT. 2) Cas des requêtes retournant les mêmes résultats mais identifiées différentes : comment est-ce possible ? Ceci : SELECT * FROM A , B WHERE A . id = B . id AND B . status = 1 et Cela (nota j'ai dû shooter la coloration syntaxique qui corrigeait tout seul ma prose ;) select * from A, B where A.id=B.id and B.status = 1 donneront le même résultat c'est sûr mais consommera de la mémoire 2 fois au lieu d'une car elles ne sont pas écrites de la même façon... le même cas de figure arrivera avec des espaces en trop entre les verbes/instructions/tables/colonnes. Ok, ça sera le même résultat mais l'optimisation pour le moteur du RDBMS au lieu de chopper la requête dans le cache, ira en coller une de plus dans le cache. Donc le coût peut s'avérer monstrueux si chacun code comme il le veut dans son coin sa requête SQL. Conclusion : alors oui j'ai parlé de Oracle mais coté PostGreSQL et vraie RDBMS opensource, il y a fort à parier que de tels comportements se produisent au détriment de l'application bien sûr.","tags":"Techno","url":"https://foxmask.net/post/2013/11/15/sql-full-table-scan-vs-index/","loc":"https://foxmask.net/post/2013/11/15/sql-full-table-scan-vs-index/"},{"title":"Incontournables Pythonerie : ImportError: No module named 'xxx' dans un virtualenv","text":"Ça pourrait s'intituler \"C'est l'histoire d'un mec... qu'est su' l'pont de l'Alma pi qui regarde dans l'eau et qui dit *j'ai fait tomber mes lunettes dans la Loire*\" Une histoire d'un biglotron quoi. Donc voilà . On se fait un script python qui marche au quart de poil et pour le coller sur un serveur en prod on se dit que quand même si on l'intégrait dans un virtualenv ça serait 'ach'ment mieux à gérer. je m'en vais donc me faire ce virtualenv tout QQ foxmask @localhost : ~/ virtualenv / apps / $ virtualenv toto New python executable in toto / bin / python Installing distribute ............................................................................................................................................................................................. done . Installing pip ............... done . foxmask @localhost : ~/ virtualenv / apps / $ cd $ _ foxmask @localhost : ~/ virtualenv / apps / toto $ ( toto ) foxmask @localhost : ~/ virtualenv / apps / toto $ source bin / activate ( toto ) foxmask @localhost : ~/ virtualenv / apps / toto $ mkdir tata ( toto ) foxmask @localhost : ~/ virtualenv / apps / toto $ cd $ _ ( toto ) foxmask @localhost : ~/ virtualenv / apps / toto / tata $ cp ~/ monscriptdelamort . py . ( toto ) foxmask @localhost : ~/ virtualenv / apps / toto / tata $ cp ~/ requirements . txt . ( toto ) foxmask @localhost : ~/ virtualenv / apps / toto / tata $ cat requirements . txt Genshi == 0.7 distribute == 0.6 . 24 lxml == 3.2 . 3 relatorio == 0.6 . 0 ( toto ) foxmask @localhost : ~/ virtualenv / apps / toto / tata $ pip install - r requirements . txt je vous passe la compilation de lxml ... ( toto ) foxmask @localhost : ~/ virtualenv / apps / toto / tata $ pip freeze -- local Genshi == 0.7 distribute == 0.6 . 24 lxml == 3.2 . 3 relatorio == 0.6 . 0 et là où ça devient intéressant c'est quand je lance monscriptdelamort.py et qu'il me sort la superbe erreur : from relatorio.templates.opendocument import Template ImportError : No module named relatorio . templates . opendocument or comme on le voit au dessus il est bien là ce module de même que ( toto ) foxmask @localhost : ~/ virtualenv / apps / toto / tata $ python >>> import relatorio.templates.opendocument >>> print ( relatorio . templates . opendocument ) >>> ça t'en bouche un coin public hein ? Au bout d'un long moment à tourner en rond autour de la poubelle de la machine à café en panne (elle aussi) je rouvre mon script et vois la lumière sur la ligne 1 : #!/usr/bin/python hé oué public t'as tout compris . Comme dis au début ; j'ai fait le script avant de l'intégrer dans un virtualenv donc quand je le lancais, j'appelais le binaire python outside of ze Virtualenv ... A bientôt pour une nouvelle bourde \"inside\".","tags":"Techno","url":"https://foxmask.net/post/2013/11/05/incontournables-pythonerie-importerror-no-module-named-dans-un-virtualenv/","loc":"https://foxmask.net/post/2013/11/05/incontournables-pythonerie-importerror-no-module-named-dans-un-virtualenv/"},{"title":"fabric ImportError: No module named main","text":"Voici peut-être une nouvelle série de billets sur Fabric cette fois ci avec des retours d'xp des plus couillons au moins balot ;) Donc comme chaque nouveau sujet que je découvre je \"partage\" les aneries de débutant, histoire de bien faire marrer la grand majorité des pro du Python mais rassurer les autres débutants comme mézigues ;) Ainsi donc, avec Fabric , me voilà à la découverte d'un nouveau monde. Ce billet sera rikiki pouce-pouce puisque ne concernera qu'un problème con comme là lune : ImportError : No module named main Ce dernier se produit au lancement de fabric .. pratique ... à débuger comme ça ... même avec la stacktrace plus bas ;) J'ai installé un virtualenv qui a cette allure : ( install_release ) foxmask@localhost:~/apps/install_release$ ls -l total 52 drwxr-xr-x 3 foxmask foxmask 4096 oct. 24 16 :15 autodeploy drwxr-xr-x 2 foxmask foxmask 4096 oct. 24 16 :02 bin drwxr-xr-x 2 foxmask foxmask 4096 oct. 24 16 :04 fabric drwxr-xr-x 2 foxmask foxmask 4096 oct. 23 14 :43 include -rw-r--r-- 1 foxmask foxmask 93 oct. 24 16 :17 __init__.py -rw-r--r-- 1 foxmask foxmask 118 oct. 23 14 :56 install_release.sublime-project -rw-r--r-- 1 foxmask foxmask 19863 oct. 23 17 :03 install_release.sublime-workspace drwxr-xr-x 3 foxmask foxmask 4096 oct. 23 14 :43 lib drwxr-xr-x 2 foxmask foxmask 4096 oct. 23 14 :43 local en me rendant dans mon dossier fabric et en tapant une commande fab foobar je me mange systématiquement Traceback ( most recent call last ): File \"/home/foxmask/apps/install_release/bin/fab\" , line 9 , in load_entry_point ( 'Fabric==1.8.0' , 'console_scripts' , 'fab' )() File \"/home/foxmask/apps/install_release/local/lib/python2.7/site-packages/distribute-0.6.24-py2.7.egg/pkg_resources.py\" , line 337 , in load_entry_point return get_distribution ( dist ) . load_entry_point ( group , name ) File \"/home/foxmask/apps/install_release/local/lib/python2.7/site-packages/distribute-0.6.24-py2.7.egg/pkg_resources.py\" , line 2279 , in load_entry_point return ep . load () File \"/home/foxmask/apps/install_release/local/lib/python2.7/site-packages/distribute-0.6.24-py2.7.egg/pkg_resources.py\" , line 1989 , in load entry = __import__ ( self . module_name , globals (), globals (), [ '__name__' ]) ImportError : No module named main Mais en y regardant de plus près... ma structure de folders, qu'on voit 2 paragraphes plus haut, ne colle pas du tout ... alors qu'ici ( install_release ) foxmask@localhost:~/apps/install_release$ ls -l total 44 drwxr-xr-x 2 foxmask foxmask 4096 oct. 24 16 :02 bin drwxr-xr-x 2 foxmask foxmask 4096 oct. 23 14 :43 include drwxr-xr-x 4 foxmask foxmask 4096 oct. 24 16 :18 install_release -rw-r--r-- 1 foxmask foxmask 118 oct. 23 14 :56 install_release.sublime-project -rw-r--r-- 1 foxmask foxmask 19863 oct. 23 17 :03 install_release.sublime-workspace drwxr-xr-x 3 foxmask foxmask 4096 oct. 23 14 :43 lib drwxr-xr-x 2 foxmask foxmask 4096 oct. 23 14 :43 local ( install_release ) foxmask@localhost:~/apps/install_release$ ls -l install_release total 12 drwxr-xr-x 3 foxmask foxmask 4096 oct. 24 16 :15 autodeploy drwxr-xr-x 2 foxmask foxmask 4096 oct. 24 16 :18 fabric -rw-r--r-- 1 foxmask foxmask 93 oct. 24 16 :17 __init__.py elle convient bien mieux et fab est content avec son pitit message \"done\" final qui me sied tout autant. :)","tags":"Techno","url":"https://foxmask.net/post/2013/10/24/fabric-importerror-no-module-named-main/","loc":"https://foxmask.net/post/2013/10/24/fabric-importerror-no-module-named-main/"},{"title":"Django FormWizard Dynamique, encore plus loin","text":"Le FormWizard encore plus loin Dans le billet précédent traitant en détail de comment afficher un formulaire en fonction d'un choix fait dans un formulaire précédent, j'ai poussé le bouchon encore plus loin. En effet, dans mon workflow des 5 formulaires, les formulaires 1 et 3 listent les services disponibles. Or bien évidement, quand j'en ai pris un sur le formulaire 1, inutile de le réafficher dans ma liste sur le formulaire 3. Ainsi donc le code suivant qui fonctionne, ne convient plus à ce \"nouveau\" prérequis : def get_form ( self , step = None , data = None , files = None ): \"\"\" change the form instance dynamically from the data we entered at the previous step \"\"\" if step is None : step = self . steps . current if step == '1' : # change the form prev_data = self . get_cleaned_data_for_step ( '0' ) service_name = str ( prev_data [ 'provider' ]) . split ( 'Service' )[ 1 ] class_name = 'th_' + service_name . lower () + '.forms' form_name = service_name + 'ProviderForm' form_class = class_for_name ( class_name , form_name ) form = form_class ( data ) elif step == '3' : # change the form prev_data = self . get_cleaned_data_for_step ( '2' ) service_name = str ( prev_data [ 'consummer' ]) . split ( 'Service' )[ 1 ] class_name = 'th_' + service_name . lower () + '.forms' form_name = service_name + 'ConsummerForm' form_class = class_for_name ( class_name , form_name ) form = form_class ( data ) else : # get the default form form = super ( UserServiceWizard , self ) . get_form ( step , data , files ) return form Il faut intervenir lors de l'affichage du formulaire 3, correspondant au \"step2\", ce qui donne : def get_form ( self , step = None , data = None , files = None ): \"\"\" change the form instance dynamically from the data we entered at the previous step \"\"\" if step is None : step = self . steps . current if step == '1' : # change the form prev_data = self . get_cleaned_data_for_step ( '0' ) service_name = str ( prev_data [ 'provider' ]) . split ( 'Service' )[ 1 ] class_name = 'th_' + service_name . lower () + '.forms' form_name = service_name + 'ProviderForm' form_class = class_for_name ( class_name , form_name ) form = form_class ( data ) elif step == '2' : # je veux recuperer les données du premier formulaire data = self . get_cleaned_data_for_step ( '0' ) # initialisation du form avec la liste deroulante sans la valeur saisie au step 0 form = ConsummerForm ( initial = { 'provider' : data [ 'provider' ]}) elif step == '3' : # change the form prev_data = self . get_cleaned_data_for_step ( '2' ) service_name = str ( prev_data [ 'consummer' ]) . split ( 'Service' )[ 1 ] class_name = 'th_' + service_name . lower () + '.forms' form_name = service_name + 'ConsummerForm' form_class = class_for_name ( class_name , form_name ) form = form_class ( data ) else : # get the default form form = super ( UserServiceWizard , self ) . get_form ( step , data , files ) return form et dans mon forms.py ca donne : from django import forms from django_th.models import ServicesActivated from django.utils.translation import ugettext as _ class ServiceChoiceForm ( forms . Form ): def activated_services ( self , provider = None ): \"\"\" get the activated services added from the administrator \"\"\" services = ServicesActivated . objects . filter ( status = 1 ) choices = [] datas = () if provider is not None : services = services . exclude ( name__exact = provider ) for class_name in services : datas = ( class_name , class_name . name . rsplit ( 'Service' , 1 )[ 1 ]) choices . append ( datas ) return choices class ProviderForm ( ServiceChoiceForm ): provider = forms . ChoiceField () def __init__ ( self , * args , ** kwargs ): super ( ProviderForm , self ) . __init__ ( * args , ** kwargs ) self . fields [ 'provider' ] . choices = self . activated_services () class ConsummerForm ( ServiceChoiceForm ): consummer = forms . ChoiceField () def __init__ ( self , * args , ** kwargs ): super ( ConsummerForm , self ) . __init__ ( * args , ** kwargs ) # get the list of service without the one selected in # the provider form self . fields [ 'consummer' ] . choices = self . activated_services ( self . initial [ 'provider' ]) class ServicesDescriptionForm ( forms . Form ): description = forms . CharField ( widget = forms . TextInput ( attrs = { 'placeholder' : _ ( 'A description for your new service' )}) ) class DummyForm ( forms . Form ): pass ... et le Fail ! Ainsi au formulaire 1 (step 0) j'utilise ProviderForm qui récupère tous les services activés. Au formulaire 3 (step 2) j'utilise ConsummerForm qui récupère tous les services en excluant celui saisi dans le formulaire 1 C'est le but du bout de code suivante de ma views.py : elif step == '2' : # je veux recuperer les données du premier formulaire data = self . get_cleaned_data_for_step ( '0' ) # initialisation du form avec la liste deroulante \"moins\" la valeur saisi au step 0 form = ConsummerForm ( initial = { 'provider' : data [ 'provider' ]}) Tout ça est super et ... ne marche pas ! Nan nan nan ca marche pas ! Ca fait une boucle infinie ... A chaque submit du formulaire 3 je retourne dessus quoique je fasse... Pourtant c'est tout beau tout propre hein ? Après avoir bien creusé 2jours ; enquiquiné StackOverflow (pour rien puisque pas eu de réponse :) et #django-fr @ freenode (merci magopian pour PDB :) j'ai entrevu la lumière que je vous montre à présent : And ze Winner iz La solution est dans la views.py... pour changer ;) elif step == '2' : step0_data = self . get_cleaned_data_for_step ( '0' ) form = ConsummerForm ( data , initial = { 'provider' : step0_data [ 'provider' ]}) tadaaaa !!! Qu'est-ce qui change ? avant : \"data\" contenait les données saisies du formulaire 0. Donc en validant le formulaire, django pensait que j'avais à faire à ProviderForm (pour schématiser) et me sortait en background une erreur \"champ obligatoire\" sur le field provider au lieu consummer. (un vrai sac de noeuds à débuger) après : je récupère les données du formulaire 0, non pas dans ma variable \"data\", mais dans une autre variable \"step0_data\", et évite de toucher à \"data\" passée à get_form() pour ensuite les refiler proprement à mon form \"ConsummerForm()\" qui exclue les données non souhaitées et le tour est joué. voici la méthode complète de get_form dans views.py : def get_form ( self , step = None , data = None , files = None ): \"\"\" change the form instance dynamically from the data we entered at the previous step \"\"\" if step is None : step = self . steps . current if step == '1' : # change the form prev_data = self . get_cleaned_data_for_step ( '0' ) service_name = str ( prev_data [ 'provider' ]) . split ( 'Service' )[ 1 ] class_name = 'th_' + service_name . lower () + '.forms' form_name = service_name + 'ProviderForm' form_class = class_for_name ( class_name , form_name ) form = form_class ( data ) elif step == '2' : step0_data = self . get_cleaned_data_for_step ( '0' ) form = ConsummerForm ( data , initial = { 'provider' : step0_data [ 'provider' ]}) elif step == '3' : # change the form prev_data = self . get_cleaned_data_for_step ( '2' ) service_name = str ( prev_data [ 'consummer' ]) . split ( 'Service' )[ 1 ] class_name = 'th_' + service_name . lower () + '.forms' form_name = service_name + 'ConsummerForm' form_class = class_for_name ( class_name , form_name ) form = form_class ( data ) else : # get the default form form = super ( UserServiceWizard , self ) . get_form ( step , data , files ) return form et cette fois ci tout glisse jusqu'au bout ! Cheminements menant à la solution Ce qui m'a mis la puce à l'oreille c'est en comparant le code des autres \"step\" (1 et 3) avec le step2 : form = form_class ( data ) en effet ici j'appelle la class \"form_class\" d'un formulaire lambda tout en lui passant les données de la variable \"data\". la différence avec le step2 c'est qu'en plus il me fallait fournir une valeur initiale via initial={} . J'avais bien capté ce qu'il fallait faire pour passer une valeur initiale mais m'étais troué en passant au travers du passage de data au form ConsummerForm() Vala qui clot un nouveau chapitre sur mes pérégrinations FormWizard ;)","tags":"Techno","url":"https://foxmask.net/post/2013/10/16/django-formwizard-dynamique-encore-plus-loin/","loc":"https://foxmask.net/post/2013/10/16/django-formwizard-dynamique-encore-plus-loin/"},{"title":"Django FormWizard Dynamique","text":"Intro Ce billet aura pour but d'illustrer une fonctionnalité de Django qu'est le FormWizard . La particularité de ce dernier est qu'on ne se contente pas d'un formulaire, mais qu'on veut en enchainer plusieurs comme par exemple pour passer une commande sur un site de e-commerce. Ça, ça se passe bien quand le scénario est figé dans le marbre et qu'aucune déviance n'est possible, c'est à dire quand les étapes sont connues d'avance. Seulement arrive le moment où ce chemin tout tracé ne peut plus coller à son besoin car les étapes qui suivent sont conditionnées par les étapes courantes, comme dans la vie courante;) Ainsi dans ce qui va suivre, je vais de nouveau aborder Trigger Happy , ce petit projet, fait en Django, permettant de grabber des info d'une source d'info de son choix pour les envoyées vers un service tiers de son choix. Actuellement pour rappel tout ce que le projet sait faire c'est récupérer des flux RSS et envoyés ce qu'il a trouvé dans un carnet de note Evernote. Le scénario était figé dans le marbre, il fallait bien commencé par un embryon de bidule qui me permette d'éprouver la solution et d'appréhender le framework ;) J'avais donc 3 pauvres pages : une pour le nom du site qui fournissait l'url du flux RSS et le flux RSS itself ;) une seconde pour indiquer dans quel carnet stocker le flux dans Evernote et la dernière, une description du \"trigger\" pour être en mesure de le modifier à posteriori. A présent que c'est fait, suite à un sondage , j'ai dû m'atteler à revoir le code, pour cette fois-ci le faire voler en éclat et gérer le FormWizard, dynamiquement ! Mon scénario initial change de 2 pages de plus, mais restera à 5 pages quoiqu'il arrive. Par ailleurs je connais 3 des 5 étapes d'avance : choisir un service qui fourni l'information (*) nommer le service d'origine puis sélectionner l'information à utiliser choisir un service qui stocke l'information (*)indiquer où stocker l'information dans le service choisi à l'étape précédente donner une description au trigger les informations statiques sont celles qui n'ont pas de (*). Ceci étant dit, ne vous attendez donc pas dans la suite de ce billet à voir le code traiter d'une quantité de formulaires fluctuant à votre gré. Non, là on restera à 5 pages, toutes obligatoires. D'où je partais voici en peu de lignes, le résumé du wizard statique \"avant\" que je ne le remanie: views.py from th_rss.forms import RssForm from th_evernote.forms import EvernoteForm from django_th.forms.base import ServicesDescriptionForm FORMS = [( \"rss\" , RssForm ), ( \"evernote\" , EvernoteForm ), ( \"services\" , ServicesDescriptionForm ), ] TEMPLATES = { '0' : 'rss/wz-rss-form.html' , '1' : 'evernote/wz-evernote-form.html' , '2' : 'services_wizard/wz-description.html' } class UserServiceWizard ( SessionWizardView ): instance = None def get_form_instance ( self , step ): \"\"\" Provides us with an instance of the Project Model to save on completion \"\"\" if self . instance is None : self . instance = TriggerService () return self . instance def done ( self , form_list , ** kwargs ): \"\"\" Save info to the DB \"\"\" trigger = self . instance trigger . provider = UserService . objects . get ( name = 'ServiceRss' , user = self . request . user ) trigger . consummer = UserService . objects . get ( name = 'ServiceEvernote' , user = self . request . user ) trigger . user = self . request . user trigger . status = True # save the trigger trigger . save () #...then create the related services from the wizard for form in form_list : if form . cleaned_data [ 'my_form_is' ] == 'rss' : from th_rss.models import Rss Rss . objects . create ( name = form . cleaned_data [ 'name' ], url = form . cleaned_data [ 'url' ], status = 1 , trigger = trigger ) if form . cleaned_data [ 'my_form_is' ] == 'evernote' : from th_evernote.models import Evernote Evernote . objects . create ( tag = form . cleaned_data [ 'tag' ], notebook = form . cleaned_data [ 'notebook' ], status = 1 , trigger = trigger ) return HttpResponseRedirect ( '/' ) Comme on le voit ici, tout est purement statique, ça marche pour ce que ça fait mais il n'est pas du tout possible d'étendre les fonctionnalités de l'application en pluggant un pauvre module de plus. Où je suis parti Ce qui suit m'a permit de modifier le FormWizard à la volée en interceptant les données saisies dans la page de choix des services. urls.py tout d'abord j'ai mis les formulaires de mon wizard dans urls.py comme ceci: url ( r '&#94;service/create/$' , UserServiceWizard . as_view ([ ProviderForm , DummyForm , ConsummerForm , DummyForm , ServicesDescriptionForm ]), si ca vous botte vous pouvez aussi la jouer comme ceci : url ( r '&#94;service/create/$' , UserServiceWizard . as_view ( 'django_th.views.get_my_form_list' ), et définir une methode get_my_form_list() dans votre views.py et l'appeler dans l'__init__ de votre FormWizard. Ici on notera que j'ai noté sciemment 2 fois DummyForm. L'intéret est tout con, quand le wizard se déroule il vous indique en haut de chaque page: etape 1/5 etape 2/5 etape 3/5 etape 4/5 etape 5/5 Donc si je ne mets pas des \"faux\" Form je vais avoir mon wizard qui m'affichera 1/3,2/3,3/3,3/3,3/3, et ca sera le bordel dans le traitement qui s'en suivra. On notera aussi que je n'ai pas mis DummyForm 2 fois au pifomètre, non ! C'est à ces endroits que je les remplacerai par les form des services choisis dans ProviderForm et ConsummerForm. Voici à présent le FormWizard : views.py import importlib def class_for_name ( module_name , class_name ): \"\"\" import the class from the given module and class \"\"\" m = importlib . import_module ( module_name ) c = getattr ( m , class_name ) return c class UserServiceWizard ( SessionWizardView ): def __init__ ( self , ** kwargs ): self . form_list = kwargs . pop ( 'form_list' ) return super ( UserServiceWizard , self ) . __init__ ( ** kwargs ) def get_form_instance ( self , step ): if self . instance is None : self . instance = UserService () return self . instance def get_context_data ( self , form , ** kwargs ): data = self . get_cleaned_data_for_step ( self . get_prev_step ( self . steps . current )) if self . steps . current == '1' : service_name = str ( data [ 'provider' ]) . split ( 'Service' )[ 1 ] #services are named th_ #call of the dedicated ProviderForm form = class_for_name ( 'th_' + service_name . lower () + '.forms' , service_name + 'ProviderForm' ) elif self . steps . current == '3' : service_name = str ( data [ 'consummer' ]) . split ( 'Service' )[ 1 ] #services are named th_ #call of the dedicated ConsummerForm form = class_for_name ( 'th_' + service_name . lower () + '.forms' , service_name + 'ConsummerForm' ) context = super ( UserServiceWizard , self ) . get_context_data ( form = form , ** kwargs ) return context Ça marche super bien... j'ai bien écrasé le form DumyForm par celui que me fourni le service que j'ai choisi dans la page précédente. C'est ce que récupere service_name, puis j'appelle le form de mon service via class_for_name. Donc tout va bien... tant qu'on n'arrive pas à la méthode done() qui gère elle, la validation de tous les formulaires saisis. Là j'ai eu beau me farcir (pendant des semaines de tests) toutes les méthodes du FormWizard pour creuser dans laquelle je pouvais mettre la \"bonne liste\" de Formulaire que je générai dynamiquement, rien y faisait, self.form_list restait immuable, impossible à changer... sauf une, celle de la solution ... The Right way to do it ! A force de creuser pleins de solutions j'en suis arrivé à ne plus avoir les idées claires pour un truc pourtant simplissime. J'ai donc demandé de l'aide via une barre chocolatée (aka un bounty) sur StackOverFlow, ce qui m'a permit de trouver la voie ! Plutôt que tripatouiller self.form_list du FormWizard puisqu'il était tout le temps remis aux valeurs indiquées dans mon urls.py , il fallait laisser choir get_context_data() pour le simplissime get_form() comme suit : def get_form ( self , step = None , data = None , files = None ): \"\"\" change the form instance dynamically from the data we entered at the previous step \"\"\" if step is None : step = self . steps . current if step == '1' : # change the form prev_data = self . get_cleaned_data_for_step ( '0' ) service_name = str ( prev_data [ 'provider' ]) . split ( 'Service' )[ 1 ] class_name = 'th_' + service_name . lower () + '.forms' form_name = service_name + 'ProviderForm' form_class = class_for_name ( class_name , form_name ) form = form_class ( data ) #edition du billet le 16/10 elif step == '2' : step0_data = self . get_cleaned_data_for_step ( '0' ) form = ConsummerForm ( data , initial = { 'provider' : step0_data [ 'provider' ]}) elif step == '3' : # change the form prev_data = self . get_cleaned_data_for_step ( '2' ) service_name = str ( prev_data [ 'consummer' ]) . split ( 'Service' )[ 1 ] class_name = 'th_' + service_name . lower () + '.forms' form_name = service_name + 'ConsummerForm' form_class = class_for_name ( class_name , form_name ) form = form_class ( data ) else : # get the default form form = super ( UserServiceWizard , self ) . get_form ( step , data , files ) return form c'est un peu verbeux et aurait pû être plus court pour tenir sur une ligne mais en fait pep8 me faisait un caca nerveu alors j'ai alloué mes vars service_name , class_name , form_name tranquillou Bon avec tout ça, là, on arrive au même point que ce que j'ai fait plus haut avec le get_context_data() . Ici, le \"plus\" est que je récupère l'instance de ma classe avec les données saisies du form ! (même si j'avais faire un form class_for_name(...)(data) ca passait pas ) la méthode La Fée Reste ensuite à traiter toutes ces données dans la méthode done() . La méthode done dans le FormWizard est la seule obligatoire à fournir systématiquement. Toutes les autres ne sont pas nécessaire à son fonctionnement, sauf celle ci qui si elle n'est pas définie pêtera une erreur, recta. Et pour cause, c'est avec elle que vous vous chargerez de vérifier la validité de vos données avant de les enregistrer. La gymnastique ici, va consister à enregistrer les données sans connaitre la moindre property des modèles correspondant aux services sélectionnés. Enfin, quand je dis que je ne les connais pas, je ne les connais pas au moment où je code. Je ne peux pas me permettre ici aussi, de hardcoder le nom d'une seule d'entre elles, puisque tout est dynamique ici. Comme dit plus tôt, je connais exactement 3 des 5 pages que je remplis, donc je grabbe ces infos pour les stocker dans un premier modèle TriggerService . Ensuite je grabbe les info saisies pour le formulaire que je nomme \"provider\" puis celui de \"consummer\" et je donne à chacun de ces 2 modèles le lien vers le modèle TriggerService (c'est dans mon MCD pour pouvoir revenir modifier mon trigger) Now, follow me ;) import importlib def class_for_name ( module_name , class_name ): # load the module, will raise ImportError if module cannot be loaded m = importlib . import_module ( module_name ) # get the class, will raise AttributeError if class cannot be found c = getattr ( m , class_name ) return c def get_service_model ( what , data ): \"\"\" get the service name then load the model \"\"\" service_name = str ( data [ what ]) . split ( 'Service' )[ 1 ] return class_for_name ( 'th_' + service_name . lower () + '.models' , service_name ) def done ( self , form_list , ** kwargs ): \"\"\" Save info to the DB The process is : 1) get the infos for the Trigger from step 0, 2, 4 2) save it to TriggerService 3) get the infos from the \"Provider\" and \"Consummer\" services at step 1 and 3 4) save all of them \"\"\" # get the datas from the form for TriggerService i = 0 for form in form_list : # cleaning data = form . cleaned_data # get the service we selected at step 0 : provider if i == 0 : trigger_provider = UserService . objects . get ( name = data [ 'provider' ], user = self . request . user ) model_provider = get_service_model ( 'provider' , data ) # get the service we selected at step 2 : consummer elif i == 2 : trigger_consummer = UserService . objects . get ( name = data [ 'consummer' ], user = self . request . user ) model_consummer = get_service_model ( 'consummer' , data ) # get the description we gave for the trigger elif i == 4 : trigger_description = data [ 'description' ] i += 1 # save the trigger trigger = TriggerService ( provider = trigger_provider , consummer = trigger_consummer , user = self . request . user , status = True , description = trigger_description ) trigger . save () model_fields = {} # get the datas from the form for Service related # save the related models to provider and consummer i = 0 for form in form_list : model_fields = {} data = form . cleaned_data # get the data for the provider service if i == 1 : for field in data : model_fields . update ({ field : data [ field ]}) # additionnal properties model_fields . update ({ 'trigger_id' : trigger . id , 'status' : True }) model_provider . objects . create ( ** model_fields ) # get the data for the consummer service elif i == 3 : for field in data : model_fields . update ({ field : data [ field ]}) # additionnal properties model_fields . update ({ 'trigger_id' : trigger . id , 'status' : True }) model_consummer . objects . create ( ** model_fields ) i += 1 return HttpResponseRedirect ( '/' ) le début est surtout de la tambouille interne à mon workflow ; ce qui import ici c'est l'usage de : model_provider . objects . create ( ** model_fields ) model_consummer . objects . create ( ** model_fields ) qui permet pour un modèle inconnu d'y stocker des properties via un kwargs dont le contenu est tout aussi inconnu, mais valide ! Voilà ; le code peut allègrement être amélioré sans aucun doute ;) Conclusion: Ainsi, comme on peut le voir à présent, il n'y a aucune limite possible, autre que de ne pas avoir accès à l'API d'un service tiers ;) Tout ce qui fourni de l'information et peut en stocker peut être utiliser dans les 2 sens, que ca soit comme provider ou consummer. A présent je vais pouvoir passer à pondre de nouveaux services comme ceux indiqués dans le sondage :D J'aurai mis le temps mais le résultat en vaut la peine. D'autres liens traitant du sujets Ci dessous, une liste de sujets sur le FormWizard par lesquels je suis passé pour tenter de trouver mon bonheur qui pourraient quand même vous servir, just in case : Dynamic number of step using Django Wizard changer les formulaires de questionnaires à la volée How to pass previous form data to the constructor of a DynamicForm in FormWizard How do you dynamically create a formset based on previous step edit J'adresse un petit merci pour l'écoute de chacun que j'ai pu enquiquiner sur le sujet sur django-fr et les pistes fournies par un certain Spoutnik ;)","tags":"Techno","url":"https://foxmask.net/post/2013/10/07/django-formwizard-dynamique/","loc":"https://foxmask.net/post/2013/10/07/django-formwizard-dynamique/"},{"title":"Django Aptana gestion du PYTHONPATH avec un VirtualEnv","text":"Avec l'éditeur Aptana, comme dit dans un précédant billet , on peut faire joujou avec le manager.py de django. Ici, je vais montrer comment gérer le PYTHONPATH d'un projet Django qu'on a mis dans un virtualenv de surcroit. Le but de ce PYTHONPATH avec Aptana va être de nous affranchir d'avoir en rouge tous les imports django comme par exemple : from django import forms et dans le même temps cela permettra d'avoir une completion du code des classes les doigts dans le nez. Pour arriver au but on fera un click droit sur le nom du projet dans la perspective PyDev puis \"Propriétés\" ce qui nous donnera cette image : on va procéder ici en 2 étapes : \"Pydev - interpreter/grammar\" \"PyDev - PYTHONPATH\" Etape 1 : On sélectionne la ligne de gauche \"Pydev - interpreter/grammar\" Puis on sélectionne le lien \"click to configure an interpreter not listed\" Ici vous allez aller chercher \"python\" dans le dossier \"bin\" de votre virtualenv en cliquant sur \"new\" Une fois fait on clique sur \"apply\" (en bas à droite de la popup) et Aptana nous charge tout le contenu de \"lib\". Avec cette étape les \"import django.xxx\" ne seront plus rouge ;) Etape 2 : On sélectionne la ligne de gauche \"PyDev - PYTHONPATH\" Sur l'onglet \"Sources folder\" on doit déjà trouver le path vers django dans la liste, suite à la manipulation précédante On va à présent ajouter ici le path vers le dossier de son application Django. Pour le localiser, ce dernier est au même niveau que le folder \"bin\" de votre virtualenv. Voilà :) Pour en voir les effets, un petit click droit sur projet puis \"PyDev\" > \"Code analasis\"","tags":"Techno","url":"https://foxmask.net/post/2013/09/26/django-aptana-pythonpath-virtualenv/","loc":"https://foxmask.net/post/2013/09/26/django-aptana-pythonpath-virtualenv/"},{"title":"Relatorio et la Secret Release Story","text":"Il y a 6 mois tout pile je testais relatorio avec succès sur un petit script pour un besoin précis et n'avait plus le temps d'approfondir le sujet pour le boulot. Les choses avançant (enfin), j'ai pu finir de faire le script python qui me permet à présent de produire des documents openoffice automatiquement à partir : d'un modèle openoffice (comme on en ferait un pour word en utilisant la \"fusion & publipostage\" pour ceux à qui ça parle plus) et d'une source de données au format XML provenant de Autodeploy . Autodeploy servant à manager pas moins de 160 environnements jEE pour les installations et mises à jour. Une fois les release produites, installées et testées il faut les livrer aux clients. Et qui trouve-t-on au bout de la chaine pour tout cela ? ouais ouais vous avez compris :) Pendant longtemps déjà je me coltinais la création des livraisons aux clients à la mimine (avec un mkdir -p des dossiers requis + wget des 'n' archives dans chaque folder, vous voyez le patacaisse) jusqu'au moment où la quantité de livraisons à effectuer dépassait 1j/homme et ai fini par produire Make delivery , un script python parcourant le fichier XML d'Autodeploy. Ce dernier en extrait les tarball avec le nom du package et les colle dans une arborescence de folders définie avant de tout compresser pour l'envoyer au client. Temps passé avant make delivery : 1heure par livraison Temps passé depuis make delivery : 15min par livraison et je suis large Seulement dans le process de livraison, on fourni quand même une @#*! de release note faite ... MAIN ! release note qu'on faisait vérifier par le consultant afin de s'assurer que le contenu du document collait parfaitement au livrable. Je vous laisse imaginer le temps perdu, les ratés potentiels, et la suite chez le client qui installe tout le toutim. A présent tout ça c'est FINI ! Bibi a fait l'(comic) script ! Le script \"MakeDelivery\" sera donc suivi par un second \"MakeRelease\" pour la création de la doc, et aucune erreur de numéro de version ne sera plus possible ! Grâce aux gars de la prod et du build process basé sur maven, on dispose des numeros de version dans les urls de chaque package, d'où j'en extrais le nom et la version que je refile dans un beau \"dict()\" lequel \"dict\" est avalé par relatorio qui pond le \"document sésame\" sans erreur possible ! Tout cela est tout beau mais attention quand même aux surprises de dernières minutes. Comme je l'ai dit au début de ce billet j'avais commencé mon script en février et mon pip install relatorio ne m'a pas chatouillé à l'époque. Entre temps une release majeure debian est passée sur ma machine et surtout une mise à jour relatorio ! En reprenant donc le cours de mon installation je me refais un virtualenv je lance sereinement le pip install précédent. Une fois tout installé (enfin dirai-je) je fini mes devs sur mon script make_release.py et me dit \"bon ce coup ci tout est prêt pour produire enfin un doc Ooo\" et au premier lancement zboing la fin des haricots ! Traceback ( most recent call last ): File \"run.py\" , line 172 , in main () File \"run.py\" , line 166 , in main generate_doc ( customer_release ) File \"run.py\" , line 130 , in generate_doc basic = Template ( source = None , filepath = 'Foo_bar.odt' ) File \"[...]/lib/python2.7/dist-packages/relatorio/templates/opendocument.py\" , line 245 , in __init__ encoding , lookup , allow_exec ) File \"[...]/lib/python2.7/dist-packages/genshi/template/markup.py\" , line 67 , in __init__ allow_exec = allow_exec ) File \"[...]/lib/python2.7/dist-packages/genshi/template/base.py\" , line 417 , in __init__ source = BytesIO ( source ) TypeError : expected read buffer , NoneType found Un truc aurait dû me mettre la puce à l'oreille, la flopée de modules python supplémentaires à compiler/installer, me dis-je, incriminant d'emblée relatorio et sa v 0.6.0 puisque même les exemples fournis sur le wiki ne passaient pas. Comme je me farcie pas mal de problèmes revêches quotidiennement, je procède avec dichotomie afin de trouver qui de relatorio ou de genshi me casse vraiment les pieds. Je laisse la 0.6.0 de relatorio et supprime genshi 0.7.0 et mets une 0.6.0 à la place en me disant si cela ne passe pas, au pire, je me ferai un nouveau virtualenv tout neuf avec un relatorio 0.5.0. je relance mon script et là ! ô joie, plus d'erreur et ma release note est là toute belle ! Gain de temps final ? : je ne vous en parle même pas ! plus de besoin de faire 'n' validations \"humaines\" inutiles ! plus besoin de lancer OpenOffice pour produire le document. Tout va être scripté mon bon môsieur ! edit : la solution pour utiliser la dernière version de genshi est en commentaire et tout rentre dans l'ordre ;)","tags":"Techno","url":"https://foxmask.net/post/2013/08/29/relatorio-secret-release-story/","loc":"https://foxmask.net/post/2013/08/29/relatorio-secret-release-story/"},{"title":"Trigger-Happy.eu s'ouvr.eu","text":"Hello, A ceux qui suivent mes dernières expériences pythono-djanguesques vous ne serez pas étonnés d'apprendre ce qui suit, aux nouveaux venus, welcome et bonne lecture ;). Présentation Ainsi je me suis lancé dans un projet nommé Django Trigger Happy permettant depuis une source de données depuis l'autre coté de la toile, d'alimenter un autre service tel Evernote, Facebook, Twitter, Readability, Pocket, Feedly et j'en passe et des meilleurs. Pour le moment le projet ne sait faire que 2 choses : lire des flux RSS/ATOM depuis ces derniers, écrire des notes dans Evernote, et en fonction de chacun des flux on peut décider de dispatcher dans un carnet plutôt qu'un autre. Pourquoi Comment ? Tout comme il existe des logiciels pour héberger son blog, ou d'autres pour produire son site web avec Drupal, Magento, ou encore d'autre pour des forums, ... Django Trigger Happy se veut être une solution personnelle de gestion de services depuis des sources éparses comme citées plus haut. je ne détaillerai pas plus pour en avoir longuement parlé en long en large et en travers dans ces billets ;) Le but principal de ce billet est juste de vous annoncer que j'ai ouvert Trigger-Happy.eu afin que vous puissiez l'essayer si vous disposez d'un jeu de flux RSS et bien sûr surtout, d'un compte Evernote. Ainsi si c'est le cas, après vous êtes enregistré sur Trigger-Happy.eu , activez les 2 services ... Création d'un trigger ... pour les utiliser avec le wizard que vous trouverez à cet effet, vous demandant le nom et l'url du flux RSS/ATOM \"source\" et le carnet evernote \"cible\" où stocker les infos comme suit : ce qui vous donnera quelque chose comme ceci : Une fois tout ceci effectué, toutes les 10 minutes, tous les flux RSS/ATOM seront passés en revu et une note sera créée dans les carnets que vous aurez décidé le cas échéant. Le mot de la fin Pour finir, la roadmap concernant les prochains services qui seront intégrés seront ceux-ci après que la vox populi se soit exprimée (et je l'en remercie ;) Si vos tests sont concluant, n'hésitez pas à vous installer votre propre \"Trigger Happy\" sur votre serveur/hébergement en suivant cette procédure . Si vous ne pouvez pas installer \"Trigger Happy\" sur un serveur personnel, libre à vous d'utiliser le site Trigger-Happy.eu pour vos propres besoins, mais sachez qu'aucune (vile) exploitation de vos données personnelles avec ce projet ne sera faite, tant mercantile que statistiques :P English version here is a short version of the announcement Today we can host our own blog website with dedicated software like Wordpress, or build our own website with soft like Magento, Drupal. Now Django Trigger Happy, an opensource project, offers the possibility to host your own service to manage all datas from all around the web. Today, Django Trigger Happy can do only 2 things : read RSS/ATOM Feeds and from this data, create notes in Evernote So i opened Trigger-Happy.eu , to let you try to play with your sets of RSS/ATOM. You just need to register and then once it's done, go to the services pages and activate each of them. Once it's done you are able to create a trigger and fill the expected informations. Then once everything is ready, each 10 minutes, the RSS/ATOM Feeds will be read and notes Evernote will be create at the right moment at the right place. notice : none of the data you will provide will be used for mercantile purpose nor statistics.","tags":"Techno","url":"https://foxmask.net/post/2013/08/13/trigger-happy-eu-souvr-eu/","loc":"https://foxmask.net/post/2013/08/13/trigger-happy-eu-souvr-eu/"},{"title":"Incontournables Pythonerie : le fichier requirements.txt","text":"pour faire écho au billet de Sam et Max sur requirements.txt , voici un tout petit billet encore sur distutils :) Mon application dépend de plein d'autres, c'est pour cette raison qu'on a produit requirements.txt. Mais il n'est pas juste là pour faire joli, ou dire \"les mecs faites pip install -r de mon fichier\"... Il faut bien l'exploiter d'une manière ou d'une autre pour que le moment venu, notamment lors de l'installation de son application via pip install , que ces prérequis soient installés automatiquement tant qu'à faire. Pour cela on va retourcher le setup.py comme ceci (snipset recupéré d'un des setup.py du sieur David Larlet:) : import os def strip_comments ( l ): return l . split ( '#' , 1 )[ 0 ] . strip () def reqs ( * f ): return list ( filter ( None , [ strip_comments ( l ) for l in open ( os . path . join ( os . getcwd (), * f )) . readlines ()])) install_requires = reqs ( 'requirements.txt' ) setup ( ... install_requires = install_requires , ) ce qui aura pour effet que lors de l'installation, le fichier requirements.txt soit lu puis que chaque ligne de ce fichier soit appelée pour être installée. Ensuite quand vous testez (j'espere) que votre packaging fonctionne via un pip install , si vous rencontrez ce problème : Downloading / unpacking django - th Running setup . py egg_info for package django - th Traceback ( most recent call last ): File \"\" , line 14 , in File \"/home/foxmask/Django-VirtualEnv/foobar/build/django-th/setup.py\" , line 12 , in install_requires = reqs ( 'requirements.txt' ) File \"/home/foxmask/Django-VirtualEnv/foobar/build/django-th/setup.py\" , line 10 , in reqs os . path . join ( os . getcwd (), * f )) . readlines ()])) IOError : [ Errno 2 ] No such file or directory : '/home/foxmask/Django-VirtualEnv/foobar/build/django-th/requirements.txt' Complete output from command python setup . py egg_info : Traceback ( most recent call last ): File \"\" , line 14 , in File \"/home/foxmask/Django-VirtualEnv/foobar/build/django-th/setup.py\" , line 12 , in install_requires = reqs ( 'requirements.txt' ) File \"/home/foxmask/Django-VirtualEnv/foobar/build/django-th/setup.py\" , line 10 , in reqs os . path . join ( os . getcwd (), * f )) . readlines ()])) IOError : [ Errno 2 ] No such file or directory : '/home/foxmask/Django-VirtualEnv/foobar/build/django-th/requirements.txt' c'est que votre package ne contient pas ledit requirements.txt Si vous utilisez un MANIFEST.in il faut ajouter une ligne : include *. txt qui permettra à la commande python setup . py sdist d'ajouter tout fichier txt, et là ca ira tout de suite mieux Downloading/unpacking django-th Downloading django_th-0.3.0.tar.gz Running setup.py egg_info for package django-th warning: no previously-included files found matching 'django_th/local_settings.py' Downloading/unpacking Django == 1 .4.3 ( from django-th ) Downloading Django-1.4.3.tar.gz ( 7 .7Mb ) : 7 .7Mb downloaded Running setup.py egg_info for package Django Downloading/unpacking batbelt == 0 .4 ( from django-th ) Downloading batbelt-0.4.tar.gz Running setup.py egg_info for package batbelt Downloading/unpacking django-profiles == 0 .2 ( from django-th ) Downloading django-profiles-0.2.tar.gz Running setup.py egg_info for package django-profiles Downloading/unpacking django-registration == 0 .8 ( from django-th ) Downloading django-registration-0.8.tar.gz ( 284Kb ) : 284Kb downloaded Running setup.py egg_info for package django-registration Downloading/unpacking evernote == 1 .23.2 ( from django-th ) Downloading evernote-1.23.2.tar.gz ( 132Kb ) : 132Kb downloaded Running setup.py egg_info for package evernote Downloading/unpacking feedparser == 5 .1.3 ( from django-th ) Downloading feedparser-5.1.3.tar.gz ( 283Kb ) : 283Kb downloaded Running setup.py egg_info for package feedparser Downloading/unpacking httplib2 == 0 .8 ( from django-th ) Downloading httplib2-0.8.tar.gz ( 110Kb ) : 110Kb downloaded Running setup.py egg_info for package httplib2 Downloading/unpacking oauth2 == 1 .5.211 ( from django-th ) Downloading oauth2-1.5.211.tar.gz Running setup.py egg_info for package oauth2 Downloading/unpacking ordereddict == 1 .1 ( from django-th ) Downloading ordereddict-1.1.tar.gz Running setup.py egg_info for package ordereddict Downloading/unpacking South == 0 .7.6 ( from django-th ) Downloading South-0.7.6.tar.gz ( 91Kb ) : 91Kb downloaded Running setup.py egg_info for package South Downloading/unpacking pytidylib == 0 .2.1 ( from django-th ) Downloading pytidylib-0.2.1.tar.gz ( 157Kb ) : 157Kb downloaded Running setup.py egg_info for package pytidylib Installing collected packages: django-th, Django, batbelt, django-profiles, django-registration, evernote, feedparser, httplib2, oauth2, ordereddict, South, pytidylib Running setup.py install for django-th warning: no previously-included files found matching 'django_th/local_settings.py' Running setup.py install for Django changing mode of build/scripts-2.7/django-admin.py from 644 to 755 changing mode of /home/foxmask/Django-VirtualEnv/toto/bin/django-admin.py to 755 Running setup.py install for batbelt Running setup.py install for django-profiles Running setup.py install for django-registration Running setup.py install for evernote Running setup.py install for feedparser Running setup.py install for httplib2 Running setup.py install for oauth2 Running setup.py install for ordereddict Running setup.py install for South Running setup.py install for pytidylib Successfully installed django-th Django batbelt django-profiles django-registration evernote feedparser httplib2 oauth2 ordereddict South pytidylib Cleaning up...","tags":"Techno","url":"https://foxmask.net/post/2013/07/01/incontournables-pythonerie-le-fichier-requirements-txt/","loc":"https://foxmask.net/post/2013/07/01/incontournables-pythonerie-le-fichier-requirements-txt/"},{"title":"Django CBV qui donne mal au crane","text":"Toujours dans la veine des bouts de code qu'un n00b produit et se tend en piège, voici une erreur à laquelle j'ai eu droit et qui pour autant, sans attirer mon attention, en prod fini en erreur 500 ... et là arrive le mal de crane... parce que debugger sans le DEBUG à True en prod ... walou ... TypeError at / int () argument must be a string or a number , not 'SimpleLazyObject' le code incriminé est celui ci : class TriggerListView ( ListView ): def get_context_data ( self , ** kw ): context = super ( TriggerListView , self ) . get_context_data ( ** kw ) enabled = TriggerService . objects . filter ( user = self . request . user , status = 1 ) disabled = TriggerService . objects . filter ( user = self . request . user , status = 0 ) context [ 'nb_triggers' ] = { 'enabled' : len ( enabled ), 'disabled' : len ( disabled )} return context la ligne enable = ... ne convient pas quand on n'est pas connecté. j'imaginais que user= prendrait rien puisque la session n'existait pas. erreuuuuuuuuuuuuuuuuuur ! pour résoudre le problème de cette ListView il faut donc la faire proprement : def get_context_data ( self , ** kw ): enabled = disabled = () if self . request . user . is_authenticated (): enabled = TriggerService . objects . filter ( user = self . request . user , status = 1 ) disabled = TriggerService . objects . filter ( user = self . request . user , status = 0 ) context = super ( TriggerListView , self ) . get_context_data ( ** kw ) context [ 'nb_triggers' ] = { 'enabled' : len ( enabled ), 'disabled' : len ( disabled )} return context et là enfin quand on accède à la ListView sans être connecté tout sera en ordre","tags":"Techno","url":"https://foxmask.net/post/2013/06/30/django-cbv-qui-donne-mal-au-crane/","loc":"https://foxmask.net/post/2013/06/30/django-cbv-qui-donne-mal-au-crane/"},{"title":"Incontournables Pythonerie : publier son appli Django sans la config perso","text":"Voici donc une nouvel épisode des Incontournable Pythonerie que vous auriez pu oublier ;) Imaginons que vous ayez pondu la plus belle lib python jamais vu et que vous ayez besoin de la publier sur le dépôt Pypi . Vous me direz c'est \"QQ la praline (coucou christian;) ton histoire on fait juste python setup.py sdist et byebye\" Alors oui vous avez raison ! A présent imaginons un truc plus élaboré par exemple une application django. Avec django on dispose d'un fichier de configuration nommé comme chacun sait, settings.py Souvent, on y colle des trucs propres à notre environnement et au moment de commiter ce fichier sur github ou autre, on est fort bien emmerdouiller à retirer de ce fichier, notre config perso. Du coup, on en vient à une bidouille toute bête permettant de conserver le settings.py d'origine et d'y importer sa config perso comme ceci : # local settings management try : from .local_settings import * except ImportError : pass c'est 5 lignes vont en fin de votre settings.py et vous pouvez mettre tout ce que vous voulez dedans il aura la même fonction que l'original Ne vous reste plus ensuite qu'à mettre dans votre fichier .gitignore local_settings.py pour ne pas l'expédier dans les sources de votre projet Maintenant je suis tout joie, mon appli marche parfaitement et je vais de ce pas la publier sur pypi pour faire partager ma liesse : je tape python setup . py register réponds aux questions demandées et enchaine avec python setup . py sdist pour créer une archive suivi de python setup . py sdist upload pour envoyer ladite archive sur pypi. le résultat de sdist donne ceci : $ python setup.py sdist running sdist running egg_info creating django_th.egg-info writing requirements to django_th.egg-info/requires.txt writing django_th.egg-info/PKG-INFO writing top-level names to django_th.egg-info/top_level.txt writing dependency_links to django_th.egg-info/dependency_links.txt writing manifest file 'django_th.egg-info/SOURCES.txt' reading manifest file 'django_th.egg-info/SOURCES.txt' writing manifest file 'django_th.egg-info/SOURCES.txt' warning: sdist: standard file not found: should have one of README, README.txt ... creating django_th-0.3.0 creating django_th-0.3.0/django_th creating django_th-0.3.0/django_th.egg-info ... creating django_th-0.3.0/django_th/models creating django_th-0.3.0/django_th/services creating django_th-0.3.0/django_th/templatetags making hard links in django_th-0.3.0... hard linking setup.py -> django_th-0.3.0 hard linking django_th/__init__.py -> django_th-0.3.0/django_th hard linking django_th/admin.py -> django_th-0.3.0/django_th hard linking django_th/context_processors.py -> django_th-0.3.0/django_th hard linking django_th/service_provider.py -> django_th-0.3.0/django_th hard linking django_th/local_settings.py -> django_th-0.3.0/django_th hard linking django_th/settings.py -> django_th-0.3.0/django_th hard linking django_th/urls.py -> django_th-0.3.0/django_th hard linking django_th/views.py -> django_th-0.3.0/django_th hard linking django_th/wsgi.py -> django_th-0.3.0/django_th si vous avez prêté attention aux logs vous voyez un truc que je ne voulais pas mettre dans mon archive hard linking django_th/local_settings.py -> django_th-0.3.0/django_th là c'est la cata ... ma config part sur pypi avec potentiellement des clés de services tiers... vous voyez le basard :P Donc en retournant à la doc de distutils , on voit qu'on dispose d'une option permettant d'exclure du packaging une partie de son projet. On créé un fichier MANIFEST.in contenant : include *. txt recursive - include examples *. txt *. py prune examples / sample ? / build ici on n'a aucun cas d'exclusion explicite, elles sont plutôt implicites, puisqu'on dit ce qu'on veut mais pas ce qu'on ne veut pas. Dans mon cas je ne souhaite pas mettre local_settings.py c'est tout. Donc dans le MANIFEST.in on mettra exclude django_th / local_settings . py Je supprime le dossier \"dist\" créé via la commande python setup.py sdist précédente et relance pour obtenir : $ python setup.py sdist running sdist running egg_info creating django_th.egg-info writing requirements to django_th.egg-info/requires.txt writing django_th.egg-info/PKG-INFO writing top-level names to django_th.egg-info/top_level.txt writing dependency_links to django_th.egg-info/dependency_links.txt writing manifest file 'django_th.egg-info/SOURCES.txt' reading manifest file 'django_th.egg-info/SOURCES.txt' reading manifest template 'MANIFEST.in' writing manifest file 'django_th.egg-info/SOURCES.txt' warning: sdist: standard file not found: should have one of README, README.txt ... creating django_th-0.3.0 creating django_th-0.3.0/django_th creating django_th-0.3.0/django_th.egg-info creating django_th-0.3.0/django_th/forms creating django_th-0.3.0/django_th/lib creating django_th-0.3.0/django_th/lib/conditionchecker creating django_th-0.3.0/django_th/lib/feedsservice creating django_th-0.3.0/django_th/migrations creating django_th-0.3.0/django_th/models creating django_th-0.3.0/django_th/services creating django_th-0.3.0/django_th/templatetags making hard links in django_th-0.3.0... hard linking MANIFEST.in -> django_th-0.3.0 hard linking setup.py -> django_th-0.3.0 hard linking django_th/__init__.py -> django_th-0.3.0/django_th hard linking django_th/admin.py -> django_th-0.3.0/django_th hard linking django_th/context_processors.py -> django_th-0.3.0/django_th hard linking django_th/service_provider.py -> django_th-0.3.0/django_th hard linking django_th/settings.py -> django_th-0.3.0/django_th hard linking django_th/urls.py -> django_th-0.3.0/django_th hard linking django_th/views.py -> django_th-0.3.0/django_th hard linking django_th/wsgi.py -> django_th-0.3.0/django_th cette fois ci le fichier MANIFEST.in est pris en compte et on ne trouve plus le fichier de config perso. on pourra s'envoyer de nouveau un coup de python setup . py sdist upload pour envoyer les sources sur pypi. Si vous êtes arrivé au bout de ce billet bravo ;) pour la peine un cadeau \"bonusque\" (comme disait Coluche) : Je me suis pris la tête une bonne heure avec python setup . py build qui n'excluait pas le local_settings.py avec le fichier MANIFEST.in alors qu'en fait c'est sdist qui le gérait ...","tags":"Techno","url":"https://foxmask.net/post/2013/06/24/incontournables-pythonerie-publier-son-appli-django-sans-la-config-perso/","loc":"https://foxmask.net/post/2013/06/24/incontournables-pythonerie-publier-son-appli-django-sans-la-config-perso/"},{"title":"Django Trigger Happy 0.3","text":"Hey ! Hé oui déjà une version 0.3.0 après la 0.2.0 la méthode des petits pas ; ya rien de tel pour débuter :) Alors rien de transcendant mais du confort pour suivre ce qui se trame dans la récupération des informations de la toile. J'ai ajouté un compteur de news ainsi que le nom du compte de l'utilisateur à qui appartient le trigger qui tourne ce qui donne par exemple: user: foxmask - provider: ServiceRss - consummer: ServiceEvernote - News de Sam et Max = 12 new data j'ai mis un coup de bootstrap sur la liste des services activés et c'est tout. installation pour l'installer je vous recommande de ne pas utiliser pip ... pypi m'a pourri les urls du projet ; le md5 est faux l'url est fausse bref rien de mieux que \" la source \", décompressez et faire python setup install et basta. J'espère que ça s'arrangera pour les prochaines release; c'est la misère depuis que pypi veut héberger les archives. no Future ? A présent que cette version semble tenir la route, pour l'utiliser à titre perso en prod avec près de 30 flux RSS suivis, je vais pouvoir m'atteler à intégrer de nouveaux services. Pocket Readability Ceux ci seront assez facilement mis en place puisque peu d'information leur sont demandées de stocker et très similaires à gérer. Mais au delà de ces derniers, si vous êtes utilisateurs de IFTTT.com, quels services utilisez vous abondamment que vous voudriez bien me voir ajouter à \"DTH\" (Django Trigger Happy) ? Lâchez vous c'est openbar ;) rappel : si vous n'avez pas suivi ce qu'est DTH ; en revoici les contours , et en images edit concernant le process d'installation via pypi: aller, j'ai botté le train de pypi :) et tout est rentré dans l'ordre ( foobar ) foxmask@:~/Django-VirtualEnv/foobar$ pip install django_th Downloading/unpacking django-th Downloading django_th-0.3.0.tar.gz Running setup.py egg_info for package django-th warning: no previously-included files found matching 'django_th/local_settings.py' Downloading/unpacking Django == 1 .4.3 ( from django-th ) Downloading Django-1.4.3.tar.gz ( 7 .7Mb ) : 7 .7Mb downloaded Running setup.py egg_info for package Django Downloading/unpacking batbelt == 0 .4 ( from django-th ) Downloading batbelt-0.4.tar.gz Running setup.py egg_info for package batbelt Downloading/unpacking django-profiles == 0 .2 ( from django-th ) Downloading django-profiles-0.2.tar.gz Running setup.py egg_info for package django-profiles Downloading/unpacking django-registration == 0 .8 ( from django-th ) Downloading django-registration-0.8.tar.gz ( 284Kb ) : 284Kb downloaded Running setup.py egg_info for package django-registration Downloading/unpacking evernote == 1 .23.2 ( from django-th ) Downloading evernote-1.23.2.tar.gz ( 132Kb ) : 132Kb downloaded Running setup.py egg_info for package evernote Downloading/unpacking feedparser == 5 .1.3 ( from django-th ) Downloading feedparser-5.1.3.tar.gz ( 283Kb ) : 283Kb downloaded Running setup.py egg_info for package feedparser Downloading/unpacking httplib2 == 0 .8 ( from django-th ) Downloading httplib2-0.8.tar.gz ( 110Kb ) : 110Kb downloaded Running setup.py egg_info for package httplib2 Downloading/unpacking oauth2 == 1 .5.211 ( from django-th ) Downloading oauth2-1.5.211.tar.gz Running setup.py egg_info for package oauth2 Downloading/unpacking ordereddict == 1 .1 ( from django-th ) Downloading ordereddict-1.1.tar.gz Running setup.py egg_info for package ordereddict Downloading/unpacking South == 0 .7.6 ( from django-th ) Downloading South-0.7.6.tar.gz ( 91Kb ) : 91Kb downloaded Running setup.py egg_info for package South Downloading/unpacking pytidylib == 0 .2.1 ( from django-th ) Downloading pytidylib-0.2.1.tar.gz ( 157Kb ) : 157Kb downloaded Running setup.py egg_info for package pytidylib Installing collected packages: django-th, Django, batbelt, django-profiles, django-registration, evernote, feedparser, httplib2, oauth2, ordereddict, South, pytidylib Running setup.py install for django-th warning: no previously-included files found matching 'django_th/local_settings.py' Running setup.py install for Django changing mode of build/scripts-2.7/django-admin.py from 644 to 755 changing mode of /home/foxmask/Django-VirtualEnv/foobar/bin/django-admin.py to 755 Running setup.py install for batbelt Running setup.py install for django-profiles Running setup.py install for django-registration Running setup.py install for evernote Running setup.py install for feedparser Running setup.py install for httplib2 Running setup.py install for oauth2 Running setup.py install for ordereddict Running setup.py install for South Running setup.py install for pytidylib Successfully installed django-th Django batbelt django-profiles django-registration evernote feedparser httplib2 oauth2 ordereddict South pytidylib Cleaning up... Les dépendances sont enfin respectées.","tags":"Techno","url":"https://foxmask.net/post/2013/06/21/djngo-trigger-happy-0-3/","loc":"https://foxmask.net/post/2013/06/21/djngo-trigger-happy-0-3/"},{"title":"Django Trigger Happy 0.2","text":"Hop une nouvelle version ! Celle ci se voit agrémenter de : Nouveautés : Dans une note Evernote, ajout dans l'url de la note, du lien du flux, ceci permet d'utiliser la fonction \"aller à la source\" Ajout d'un \"statut\" à son trigger permettant de l'activer ou non sans pour autant avoir à le supprimer si on n'en veut plus, temporairement Ajout d'une pagination sur la liste des triggers Améliorations: Dans le pied de page de la note, remplacement du lien par l'affichage de l'origine de la source des données + le lien vers celle-ci Corrections de bugs: L'imposition d'un tag, pour la création des notes, n'est plus :) Le traitement des flux Atom faisait planter le traitement batch Edition d'un trigger rendu possible à présent Edit si vous vouliez l'installer via pip install django_th oubliez ! pour cette version ca ne fonctionne pas ; balot que je suis je n'ai pas mis le setup.py là où pypi l'attendait. Donc en attendant 2 solutions, l'archive est dispo depuis Pypi depuis GitHub (le lien zip :)","tags":"Techno","url":"https://foxmask.net/post/2013/06/17/django-trigger-happy-0-2/","loc":"https://foxmask.net/post/2013/06/17/django-trigger-happy-0-2/"},{"title":"Django Trigger Happy un IFTTT like , en images","text":"BonjEllo ! Je ne vous referai par \"l'article\" j'ai déjà longuement détaillé ce petit projet ici , je vous montre juste ce à quoi il ressemble pour le moment, dans une version très \"toute neuve\" ;) Les services activés par l'utilisateur : Wizard de création d'un trigger en 3 petites pages : Les des triggers prêts à l'emploi Une fois le wizard utilisé pour définir ce qu'on veut \"grabber\", on a notre jolie page d'accueil qui ressemble à ceci : fire ! à présent que tout est prêt il nous reste un petit script python qui va nous faire notre tambouille et qui au lancement vous donne le cours des opérations : ./fire.py INFO PROVIDER ServiceRss CONSUMMER ServiceEvernote : News Sam et Max INFO PROVIDER ServiceRss CONSUMMER ServiceEvernote : News de Numerama INFO PROVIDER ServiceRss CONSUMMER ServiceEvernote : Python Package Index INFO PROVIDER ServiceRss CONSUMMER ServiceEvernote : mes news pourries : ) ce script est collé dans une crontab toutes les 10minutes Résultat dans Evernote ah oui tout de même last but not least, il est bien beau avec son script mais zoukilé le résultat de toutes ces pages remplies ? c'est là ça vient ;) Comme on le voit, comme dans le wizard ci dessus, sur la partie droite de l'écran sous le titre \"Nexus 10\", on lit bien les noms des notebook et tags, correctement repris et bien créés à la volée. Si ce petit bidule vous plait, libre à vous de l'essayer, contribuer, critiquer. C'est open bar ! Le projet ne risque pas de pourrir dans un coin, j'en suis le premier client et est moult idées ;) ps: si vous l'essayer, utiliser plutot le dépôt github que pypi qui n'est pas uptodate avec les dernières corrections Edit : je vous rajoute 2 captures pour illustrer le rendu ... 1) reçu directement par mail en s'abonnant à la ml de sam&max 2) reçu via DTH","tags":"Techno","url":"https://foxmask.net/post/2013/06/04/django-trigger-happy-un-ifttt-like-en-images/","loc":"https://foxmask.net/post/2013/06/04/django-trigger-happy-un-ifttt-like-en-images/"},{"title":"Django Trigger Happy - une première","text":"Voici le premier projet un peu plus conséquent que les petits modules que je faisais de ci de là. Ce dernier est entièrement inspiré du service IFTTT \"If This Then That\" . Description Le principe du service est d'effectuer une action (souvent, stocker une information, ou la relayer) en fonction d'un événement se produisant n'importe où sur internet à partir d'une source donnée. Ce projet peut être utiliser pour vos besoins propres tout comme votre blog, ou vous pouvez également décider \" d'héberger \" les Triggers de vos potes. Pourquoi ce projet ? Comme à chaque fois que je me lance dans un truc c'est par réaction (épidermique ?) d'une solution qui ne fonctionne pas comme j'escomptais. Ici IFTTT marche bien, mais pas assez à mon goût, j'ai des problème d'encoding UTF8 très très désagréable ;) genre \"cet été je pars en lozère\" devient \"cet t je pars en lozre\"... Donc après avoir testé d'autres solutions genre CloudWork, j'étais ravi ... mais estomaqué du coût ! l'offre fonctionne au nombre de triggers à journée. Or un trigger chez CloudWork ce n'est pas un trigger chez IFTTT. Chez CloudWork un trigger c'est UN déclenchement, et non pas la programmation d'un déclenchement. Donc quand on est limité à 100 triggers à raison d'un toutes les 10min ... bonjour la facture :P Exemple d'utilisation Par exemple, vous suivez les billets d'un blog régulièrement, et vous ne souhaitez pas en perdre une miette. Vous définissez alors un \"trigger\" qui se déclenchera quand un nouveau billet paraîtra, alors ensuite vous déciderez d'en faire ce que bon vous semble, comme le publier sur vos réseaux sociaux favoris ou encore stocker le billet dans Evernote ou ailleurs. C'est très pratique car vous ne dépendez plus d'aucune solution de lecteur de flux RSS par exemple comme Feedly, GoogleReader ou autre. Aujourd'hui Actuellement \"Trigger Happy\" ne sait faire que 2 choses : lire des flux RSS, et stocker ceux ci dans Evernote, dans le carnet de votre choix avec le tag de votre choix. Mise en route 1) On procédera comme pour tout projet django avec un python manage . py syncdb suivi du lancement du serveur. On mettra dans une crontab (ou tout autre service de gestion de tâches préprogrammées) le script fire.py à intervalle régulier, genre tous les 10min (mais en prenant bien garde de ne pas avoir des \"pas\" trop courts afin de ne pas saturer le service cible qui pourrait vous bloquer l'accès à leur service). Le script se chargera de collecter les \"nouvelles fraîches\" pour vous les propager sur le(s) service(s) cible(s). 2) L'administrateur (ou même vous;) peut décider que tel ou tel service soit mis à disposition ou non. Ceci permettant une modularité la plus flexible possible. Donc, de là, la première chose à faire est d'aller dans l'admin et activer les 2 services RSS et Evernote, en cochant la case \"auth required\" pour d'Evernote. (cf explication ci dessous) 3) Ensuite l'utilisateur, disposera des services qu'il devra à son tour activer. Pour le service Evernote, comme l'administrateur aura coché la case \"auth required\", l'utilisateur sera envoyé vers le service Evernote pour demander que l'utilisateur autorise \"Django Trigger Happy\" de créer des notes dans ses carnets. 4) Une fois les services activés, il ne reste plus qu'à créer un trigger avec le wizard existant, d'où on indiquera depuis quel flux RSS on veut lire les sources de données, puis vers quel service on souhaite stocker/publier ces données. Si vous êtes accoutumé à IFTTT, vous vous rendez compte qu'il s'agit du même processus ;) Demain J'ai déjà prévu de pouvoir filtrer les flux RSS en fonction de contenu que je ne veux pas . Imaginons un blog ou tout n'est pas SFW :P ou plus simplement, sur le site de Pypi , énormément de projets publiés arrivent avec une description \"UNKNOWN\" qui à mon goût ne mérite pas qu'on s'y attarde puisque l'auteur n'a pas pris le temps de mettre une description, il semblerait. Donc ce filtre permet d'éviter d'avoir un contenu pollué vous l'aurez compris. Avenir L'architecture produite me permet d'ajouter facilement un service tels Twitter, Facebook, etc. et l'API Pour le moment le projet débute tout juste mais je publierai un prochain billet (et une page wiki) pour expliquer comment on s'ajoute un service de plus, le plus aisément possible. Quelques Liens Django Trigger Happy sur Pypi version 0.1 Django Trigger Happy sur GitHub Last but not Least : Greetings ! Un petit mot de remerciements pour Sam et Max que j'ai longuement sollicité et m'ont aidé sur l'archi à tenir ;)","tags":"Techno","url":"https://foxmask.net/post/2013/05/27/django-trigger-happy/","loc":"https://foxmask.net/post/2013/05/27/django-trigger-happy/"},{"title":"Debian 7 Wheezy - re mise à jour du driver NVIDIA","text":"Je viens de finir d'installer cette mise à jour de Debian Wheezy sur 2 PC ; un portable Dell Latitude sans aucun soucis mais comme d'hab mon PC de bureau n'en fait qu'à sa tête... Le soucis se situe à plusieurs endroits : GRUB Carte Graphique GRUB : La mise à jour m'a flingué grub sans que j'ai eu le temps de voir ce qu'il s'était passé... au reboot j'étais stuck \"dans le noir\"... condamné à refaire une installation from scratch ... NVIDIA GT 220 Comme on change de release Debian, on change de kernel, donc on est \"bon\" pour se retaper l'installation du driver NVIDIA en version 304.64 pour 64Bits ; toutes autres versions antiérieures ne compilera tout bonnement pas ne trouvant pas le fichier version.h dans les sources du kernel ; rien que ça. Donc pour ce faire : arreter X /etc/init.d/gdm3 stop lancer ./NVIDIA-Linux-x86_64-304.64.run A l'invite il peut vous dire que vous utilisez un GCC 4.7 et que le module était prévu pour compiler avec GCC 4.6 vous lui dites donc \"non\" et ça passera. La compilation se passe les doigts dans le nez ! L'installeur vous demandera s'il peut vous exectuer nvidia-xconfig pour pondre le fichier /etc/X11/xorg.conf Dites lui oui ! finissez par un petit modprobe nvidia lancer gdm3 /etc/init.d/gdm3 start Et là comme par enchantement on retrouve un environnement graphique paisible. Par paisible j'entends que l'installation précédente allait très bien mais que le ventilo du proc de la CG tournait comme un débile à qui l'on aurait demander de faire du traitement vidéo... Alors qu'une fois le module NVIDIA chargé tout redevenait clamos. Voilou ! Edit 16/5 : un truc qui m'a quand même fortement déplu avec ces mises à jour. (j'en ai fait 3), il n'y a aucun avertissement sur le non support du driver NVIDIA lors de l'upgrade. On est dans le noir si le driver ne passe pas ... et pour retrouver ses billes, \"interroger\" la \"toile\" ; si on n'est pas familier de links ou w3m c'est la mort","tags":"Techno","url":"https://foxmask.net/post/2013/05/06/debian-7-wheezy-re-mise-a-jour-du-driver-nvidia/","loc":"https://foxmask.net/post/2013/05/06/debian-7-wheezy-re-mise-a-jour-du-driver-nvidia/"},{"title":"HaveFnuBB les archives  sont de retour du futur","text":"Avec mon déménagement de site web du projet HaveFnuBB (le forum sous GPL produit avec Jelix ), j'ai malencontreusement oublié de récupérer mes archives du forum que je vous proposais au téléchargement. Rien que ça :-) Fort heureusement on me l'a fait remarquer et j'ai donc remis en ligne les versions 1.3.6 pour jelix 1.1 1.4.0 pour jelix 1.2 1.5.0 pour jelix 1.2 (aussi) Tout est là","tags":"Techno","url":"https://foxmask.net/post/2013/05/01/havefnubb-les-archives-sont-de-retour-du-futur/","loc":"https://foxmask.net/post/2013/05/01/havefnubb-les-archives-sont-de-retour-du-futur/"},{"title":"Django - think twice - another day for you and me in paradise","text":"thought one Intro : Parfois ça crêve les yeux mais quand on n'est pas réveillé ; on a justement les yeux crevés :) Voilà ce qui m'aveuglait : En tapant un python manage.py syncdb J'obtenais : django . core . exceptions . ImproperlyConfigured : '/home/foxmask/Django-Virtualenv/django-trigger-happy/django_th/django_th/sqlite3' isn 't an available database backend. Try using django . db . backends . XXX , where XXX is one of : 'dummy' , 'mysql' , 'oracle' , 'postgresql_psycopg2' , 'sqlite3' Error was : Import by filename is not supported . Evident non ? bon, mon settings.py contenait : DATABASES = { 'default' : { # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'. 'ENGINE' : '' , 'NAME' : '' , # Or path to database file if using sqlite3. 'USER' : '' , # Not used with sqlite3. 'PASSWORD' : '' , # Not used with sqlite3. 'HOST' : '' , # Set to empty string for localhost. Not used with sqlite3 'PORT' : '' , # Set to empty string for default. Not used with sqlite3. } } j'ai juste rajouté 'ENGINE':'sqlite3' et patatra alors que le message complet et explicite me dit \"using django.db.backends.XXX where XXX is one of ...\" c'etait tellement evident ;) Pourquoi comment ? dans mon process de dev j'utilise un settings.py que je ne modifie pas du tout pour la config DATABASES qui est elle dans un local_settings.py chargée à la fin de settings.py. Du coup en committant mon settings.py j'ai viré tout simplement \"django.db.backend.\" de 'ENGINE': de ce dernier puisque présent dans local_settings. Comment se faire des noeuds au cerveau ? Aahhhhhhhhhhhh je vous jure ;) thought two A coté de ce sujet qui a bien fait marrer #django-fr @ freenode ;)), un moins \"QQ La praline\" : splitter models.py en petit bout en fonction d'un périmètre fonctionnel Pour arriver à séparer les classes de son models.py on doit créer un dossier models contenant : myapp / models / __init__ . py truc . py muche . py et dans __init__.py mettre simplement : from truc import Machin from muche import Chose où Machin et Chose sont des classes dans truc.py et muche.py Seulement tout ceci ne suffit pas... quand on va lancer python manage.py syncdb, aucune des 2 tables ne sera créée. La solution, subtilité est cachée par là : class Machin ( models . Model ): class Meta : app_label = 'myapp' si app_label n'est pas défini django ne saura pas que la classe splitée appartient à cette application ( si j'ai tout bien suivi les explications ici )","tags":"Techno","url":"https://foxmask.net/post/2013/04/11/django-think-twice-another-day-for-you-and-me-in-paradise/","loc":"https://foxmask.net/post/2013/04/11/django-think-twice-another-day-for-you-and-me-in-paradise/"},{"title":"Pythonthon c'est réglé : Python et la PSF sont sauf !","text":"Comme relaté précédemment, la PSF demandait l'aide de tout à chacun pour son différend avec PO Box Hosting qui avait déposé la marque Python. Heureux dénouement Python Software Foundation Reaches Settlement, Ends Trademark Dispute , mais qui en doutait ? La marque Python n'étant déposée qu'aux \"States\" la PSF s'inquiétait de ce qui allait \"lui\" arriver en Europe où la marque n'était pas déposée. La société PO Box Hosting a donc retiré le dépot de la marque et tout est rentré dans l'ordre. La communauté s'en voit remercier pour sa mobilisation !","tags":"Techno","url":"https://foxmask.net/post/2013/03/20/pythonthon-cest-regle-python-la-psf-sont-sauf/","loc":"https://foxmask.net/post/2013/03/20/pythonthon-cest-regle-python-la-psf-sont-sauf/"},{"title":"Incontournables Pythonerie : un pack de 12 s'il te plait !","text":"Avec ce billet nous allons voir comment produire un package comme on en voit partout dans le monde opensource (ou pas d'ailleurs) pour livrer une \"suite logicielle\" à ses clients/utilisateurs. Brève présentation du script : Ce script extrait d'un fichier XML (produit par Autodeploy ) les targz installés sur les environnements. Puis lit un fichier de configuration pour décider dans quel répertoire va être télécharger chaque archive. C'est tout ! On mappe ni plus ni moins un targz dans un dossier. Prérequis à la lecture de cet article la lecture des mes billets précédents, vous donnera l'aperçu de ce qui va suivre, puisque chacun d'eux est une partie du script complet qui suivra comme je vous l'ai présenté depuis ces derniers jours, notamment : récupération des arguments entrés sur la ligne de commande soit avec optparse ou argpase la lecture de fichier XML la lecture de fichier de configuration la création de fichier de journalisation La nouveauté dans cet article est le pseudo \"wget\" à la sauce Python pour récupérer les archives Tout ceci ressemble bien au déroulement d'un script, ça tombe bien le voici :) Pour utiliser ce script, on tape python make_delivery -r RELEASE -e ENV -c CONFIG le script traitera ceux ci comme suit : usage = \"%prog -e environment name -r release name. \\n for example : \\n python make_delivery -e envname -r 20130101\" parser = OptionParser ( usage ) parser . add_option ( \"-e\" , \"--env\" , dest = \"environment\" , help = \"the environment name to use to build the delivery\" , metavar = \"ENV\" ) parser . add_option ( \"-r\" , \"--rel\" , dest = \"release\" , help = \"the release name of this delivery (used to name the final package like release-RELEASE-yyyymmdd)\" , metavar = \"RELEASE\" ) parser . add_option ( \"-c\" , \"--conf\" , dest = \"configfile\" , help = \"the path where the config file is located. This file should contain the name of the environment from which to download the archives. By Default the script will search in ./env_dirs.conf\" , default = \"./env_dirs.conf\" , metavar = \"CONFIG\" ) parser . add_option ( \"-l\" , \"--log\" , dest = \"configlogfile\" , help = \"the path where the config file for the loggging is located. By Default the script will search in ~/MakeDelivery/logging.conf\" , default = os . path . expanduser ( '~/MakeDelivery/logging.conf' ), metavar = \"LOGGING_CONFIG\" ) ( options , args ) = parser . parse_args () if options . environment == None or options . release == None : parser . error ( \"options -e and -r are mandatory\" ) else : [ ... ] #lets concat release name + date to have a name to use for logfile and directory release_name = release_name_main ( options . release ) [ ... ] release_name_dir = release_name_create ( release_name ) Dès lors make_delivery va faire quelques vérifications d'existence du dossier que j'escompte créé (pour ne pas écrire par dessus pusique je conserve un historique de toutes les livraisons faites au client). def release_name_create ( release_name_dir ): if os . path . isdir ( release_name_dir ): logger . critical ( \"Directory %s already exists. You should change the release number ( -r parameter ) \" , release_name_dir ) exit ( 1 ) else : logger . info ( \"Creation of %s \" , release_name_dir ) os . makedirs ( release_name_dir ) return release_name_dir Puis make_delivery appelle le module get_envs (parsant le fichier XML) pour obtenir les noms des applications de mon environnement et en extraire les URL de chacune from get_env import get_apps [ ... ] #get the data from the get_env class which read the ConfigWrapper file from autodeploy environment_datas = get_apps ( options . environment ) Puis make_delivery lit le fichier de configuration décrivant comment structurer ma livraison en indiquant le nom de chaque application avec son dossier de destination. [myenv] #directory to add to the delivery whatever happens doc: fake [myenv_apps] myapp1: app myapp2: app myapp3: null [myenv_software] myapp4: app myapp5: path/to/target myapp6: null #do nothing for this app myapp7: . Enfin le transfert de fichier se produit et affiche une progress bar histoire de voir où on en est. def make_release ( environment_name , release_name_dir , environment_datas , configfile ): for component in environment_datas : logger . info ( \"Environment =>>> %s Components: %s : Begin \" , environment_name , component ) archives = environment_datas [ component ] for archive_name in archives : os . chdir ( curdir ) destination_directory = config . get ( environment_name + \"_\" + component , archive_name ) if destination_directory == 'null' : continue #check if the directory exists #if yes ; chdir to it if os . path . isdir ( destination_directory ): os . chdir ( destination_directory ) #otherwise create it then chdir else : os . makedirs ( destination_directory ) os . chdir ( destination_directory ) #get the url of the archive url = archives [ archive_name ] #get the filename part of the URL to download file_name = url . split ( '/' )[ - 1 ] #download the file try : logger . info ( \"Download %s - url %s \" , archive_name , url ) u = urllib2 . urlopen ( url ) except : logger . error ( \"Error: %s %s %s \" , sys . exc_info ()[ 1 ], archive_name , url ) pass f = open ( file_name , 'wb' ) meta = u . info () #get the Content Length file_size = int ( meta . getheaders ( \"Content-Length\" )[ 0 ]) #display the prgress on the screen logger . info ( \"Downloading: %s Bytes: %s \" , file_name , file_size ) file_size_dl = 0 block_sz = 8192 while True : #read the file getting from the url buffer = u . read ( block_sz ) if not buffer : break file_size_dl += len ( buffer ) f . write ( buffer ) #progressbar status = r \" %10d [ %3.2f%% ]\" % ( file_size_dl , file_size_dl * 100. / file_size ) status = status + chr ( 8 ) * ( len ( status ) + 1 ) print status , f . close () logger . info ( \"Environment =>>> %s Components: %s : End \" , environment_name , component ) Evidement, dans la foulée tout est loggé, #set the name of the config logging file from the command line logging . config . fileConfig ( options . configlogfile , disable_existing_loggers = False ) [ ... ] FORMAT = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' #write the logfile in the current working dir fh = logging . FileHandler ( './' + release_name + '.log' ) fh . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( FORMAT ) fh . setFormatter ( formatter ) logger . addHandler ( fh ) ainsi je n'ai plus qu'à regarder dans mon fichier release-YYYYMMDD-name.log si j'ai des erreurs (genre une 404 au hasard ;) Une fois lancée, la commande affiche ceci : 2013-02-22 10:34:44,398 - make_delivery - INFO - Creation of release-201041.41-20130222 2013-02-22 10:34:44,439 - make_delivery - INFO - Creation of the release 201041.41 from the environment customer1-testing 2013-02-22 10:34:44,439 - make_delivery - INFO - Reading config file /root/MakeDelivery/env_dirs.conf 2013-02-22 10:34:44,470 - get_env - INFO - Read the Autodeploy config file http://autodeploy.domain.com/ConfigurationWrapper 2013-02-22 10:34:45,059 - make_delivery - INFO - Environment =>>> customer1-testing Components: apps : Begin 2013-02-22 10:34:45,104 - make_delivery - INFO - Download myapp1 - url http://maven.domain.com/deliveries/myapp1/releases/myapp_2.19.23_weblogic.tar.gz 2013-02-22 10:34:45,169 - make_delivery - INFO - Downloading: myapp1.19.23_weblogic.tar.gz Bytes: 29585377 2013-02-22 10:34:46,572 - make_delivery - INFO - Download myapp2 - url http://maven.domain.com/deliveries/myapp2/releases/myapp2_2010.r1.2.44.42_core_weblogic.tar.gz 2013-02-22 10:34:46,652 - make_delivery - INFO - Downloading: myapp2_2010.r1.2.44.42_core_weblogic.tar.gz Bytes: 67769731 2013-02-22 10:34:49,681 - make_delivery - INFO - Download framework - url http://maven.domain.com/maven2/internal/services/framework/framework/1.16.29/framework-1.16.29-delivery.tar.gz 2013-02-22 10:34:49,733 - make_delivery - INFO - Downloading: framework-1.16.29-delivery.tar.gz Bytes: 11556974 2013-02-22 10:34:50,231 - make_delivery - INFO - Download myapp3 - url http://maven.domain.com/deliveries/myapp3/releases/myapp3_2010.r1.1.37.42_weblogic.tar.gz 2013-02-22 10:34:50,303 - make_delivery - INFO - Downloading: myapp3_2010.r1.1.37.42_weblogic.tar.gz Bytes: 51400921 2013-02-22 10:34:52,224 - make_delivery - INFO - Download portal - url http://maven.domain.com/maven2/internal/services/portal/portal/1.10.18/portal-1.10.18-delivery.tar.gz 2013-02-22 10:34:52,276 - make_delivery - INFO - Downloading: portal-1.10.18-delivery.tar.gz Bytes: 16350871 2013-02-22 10:34:52,969 - make_delivery - INFO - Download myapp4 - url http://repo.domain.com/deliveries/myapp4/releases/myapp4_2010.r1.1.31.38_weblogic.tar.gz 2013-02-22 10:34:53,061 - make_delivery - INFO - Downloading: myapp4_2010.r1.1.31.38_weblogic.tar.gz Bytes: 60468113 2013-02-22 10:34:55,450 - make_delivery - INFO - Environment =>>> customer1-testing Components: apps : End 2013-02-22 10:34:55,450 - make_delivery - INFO - Environment =>>> customer1-testing Components: software : Begin 2013-02-22 10:34:56,677 - make_delivery - INFO - Download database - url http://maven.domain.com/maven2/internal/database/2010.r1.3.23.41/database-2010.r1.3.23.41.tar.gz 2013-02-22 10:34:56,727 - make_delivery - INFO - Downloading: database-2010.r1.3.23.41.tar.gz Bytes: 4902673 2013-02-22 10:35:11,832 - make_delivery - INFO - Download database2 - url http://maven.domain.com/maven2/internal/database2/database2/1.48.12/database2-1.48.12-delivery.tar.gz 2013-02-22 10:35:11,868 - make_delivery - INFO - Downloading: database2-1.48.12-delivery.tar.gz Bytes: 10995334 2013-02-22 10:35:12,321 - make_delivery - INFO - Environment =>>> customer1-testing Components: software : End 2013-02-22 10:35:12,321 - root - INFO - Environment =>>> customer1-testing download successfull completed On m'a cité il y a quelques temps l'usage de la lib \" requests \" plutôt que urllib2 car plus souple et facile de mise en oeuvre notamment pour gérer l'auth. Ici je n'ai nul besoin d'auth c'est un accès réseau sur l'intranet ;) Si vous voulez zieuter de plus près les sources de ce script, ou plus simplement en avoir une vue d'ensemble, il se trouve ici sur github Etant loin d'être un pro dans le domaine c'est modestement que j'ai tenté l'exercice de vous montrer étape par étape, comment \"ça marche\" chez moi. Ce script marche parfaitement en prod, et me permet d'économiser un temps monstreux : plus besoin de faire les mkdir plus besoin de faire les wget des presque 60 archives que constituent une \"livraison\" à mes clients le script ne prend que 3 minutes à produire ma livraison pour le plus \"gros\" client. Avant ce script il me fallait en passer par mkdir + wget avec risque d'erreurs potentielles ... => 1heure J'ai dans le pipe, la création d'une RELEASE NOTE listant au client les éléments fournis, dans un doc PDF, document note que je ponds manuellement aujourd'hui : un tableau des versions avec instructions d'installation => re une heure de perdue ... alors qu'apres un test d'une soluce python ça me prendrait 1min :) Voilou : N'hésitez pas à me faire un retour, si j'ai pu vous donner envie de vous mettre à python (si vous veniez de php comme bibi), si vous avez trouvé ça trop \"weak\"/trop juste, trop chiant, ou juste \"pas mal\", je suis prêt à tout entendre ;)","tags":"Techno","url":"https://foxmask.net/post/2013/03/18/incontournables-pythonerie-pack-de-12/","loc":"https://foxmask.net/post/2013/03/18/incontournables-pythonerie-pack-de-12/"},{"title":"RSS, RSS, quoi j'ai une gueule d'RSS ?","text":"Avec l' \"arrivée\" du \"départ annoncé\" de GoogleReader pour cet été, des mouvements de foule se produisent pour trouver une alternative, une vraie. Chacun tweetant la solution qu'il trouve la mieux avec de la valeur ajoutée qu'apporte cette dernière. Alors non je ne vous en donnerai pas une seule de ces solutions, je n'utilise aucun lecteur de flux RSS... J'utilise une doublette (un gestionnaire de triggers et un lecteur de carnets de notes \"pour lire plus tard\") qui n'est pas publique. Par contre tous ces mouvements, m'amènent à une réflexion que tout à chacun utilisant un blog a déjà eu, et qui pour moi importe plus que trouver une alternative à une solution qui meurt : Dois-je héberger moi même un \"lecteur\" de flux RSS ? Dois-je plutôt utiliser une plateforme lecteur de flux RSS ? On se posait la même question pour les blogs, utiliser une plateforme ou s'héberger son propre blog. Avec l'avènement de l'ère du mobile, il importe que chacune des 2 solutions permette l'accès via smartphone, que ça soit pour l'auto-hébergement, ou la plateforme qu'on aura choisi (qui fournirait un client pour smartphone of course) Ne négligez pas l'auto hébergement sous prétexte que vous préférez avoir un client natif iOS/Android (par exemple) pour lire les \"news\" plus facilement. Il suffit de \"pusher\" les nouveautés récupérer par votre lecteur RSS vers une solution cloud, style Evernote pour parvenir à avoir toutes vos news sans effort sur votre portable. Alors prenez votre temps pour trouver ce qui vous siéra le plus ;-)","tags":"Techno","url":"https://foxmask.net/post/2013/03/14/rss-rss-quoi-jai-une-gueule-drss/","loc":"https://foxmask.net/post/2013/03/14/rss-rss-quoi-jai-une-gueule-drss/"},{"title":"Oracle Migrer un schéma ISO en UTF8","text":"Vous avez sous la main un dump en ISO et vous souhaitez l'importer en UTF8, tout un programme ! La marche à suivre est la suivante : On importe le dump dans un schéma d'une base en ISO On exécutera un script byte_to_char.sql pour que le schéma prenne en compte l'UTF8 On exportera le schéma en UTF8 On importera le dump en UTF8 dans une base UTF8 Import du dump je vous laisse avec l'étape une, basique à souhait ;) Exécution du script byte_to_char.sql BEGIN FOR i IN ( SELECT utc . TABLE_NAME , utc . column_name , utc . data_length , utc . data_type FROM user_tab_cols utc , user_tables ut WHERE ut . TABLE_NAME = utc . TABLE_NAME AND utc . data_type IN ( 'VARCHAR2' , 'CHAR' ) AND char_used = 'B' AND column_name NOT LIKE 'SYS_%' ) loop EXECUTE immediate 'alter table ' || i . TABLE_NAME || ' modify ( ' || i . column_name || ' ' || i . DATA_TYPE || '(' || i . data_length || ' CHAR) )' ; END loop ; END ; / Ainsi, à l'issu de son exécution vos colonnes bytes seront en char ce qui permettra lors de l'import, de prendre en compte les caractères accentués sur plus d'un byte. Export du schéma en UTF8 mknode pipeExp p export NLS_LANG = AMERICAN_AMERICA.UTF8 gzip MY_UTF8.dmp.gz nohup exp userid = LOGIN/PASS file = pipeExp grants = n & l'important ici est la variable d'environnement NLS_LANG à positionner à AMERICAN_AMERICA.UTF8 Import du dump en UTF-8 idem ci dessus, c'est simple à souhait ;)","tags":"Techno","url":"https://foxmask.net/post/2013/03/14/oracle-migrer-un-schema-iso-en-utf8/","loc":"https://foxmask.net/post/2013/03/14/oracle-migrer-un-schema-iso-en-utf8/"},{"title":"Incontournables Pythonerie : parse parse parsera la dernière la dernière","text":"Le sujet du jour concernera le traitement des paramètres en lignes de commandes, mais au lieu de mon précédant billet sur le sujet , qui traitait de OptParse , celui ci traitera de argparse et des différences que j'ai constaté pour passer de l'un à l'autre, et qui ne sont pas dans la doc de \"migration\" Pour traiter un cas concret j'ai produit un lib python pour récupérer des infos d'un site en ligne telles que une timeline, la liste épisodes d'une série tv, la liste des séries tv ressemblant à une autre etc... ça c'est pour la lib, elle fonctionne ;) Pour la tester j'ai pondu un script qui fait l'appel à toutes les méthodes de la classe de ma lib. Il y a pas moins d'une quarantaine de méthodes. Pourquoi je vous dis ça ? parce que ça m'a fait faire une quarantaine de traitement d'arguments ;) Ah ba quand il faut il faut ;) Optparse Avec optparse, j'ai donc décidé de partir du postulat : faire des \"groupes\" : un par methode de ma classe avec les options qui matcheront les paramètres attendues par chaque méthode (classique quoi). Cela donnait ceci (je ne vous mets pas tout rassurez vous ;) : def main (): parser = OptionParser () group0 = OptionGroup ( parser , \"*** Search series\" ) group0 . add_option ( \"--title\" , dest = \"title\" , type = \"string\" , help = \"make a search by title\" ) parser . add_option_group ( group0 ) # group the options for handling Display of series parameters group1 = OptionGroup ( parser , \"*** Details of series\" ) group1 . add_option ( \"--display\" , dest = \"display\" , action = \"store\" , help = \"the name of the given serie\" ) parser . add_option_group ( group1 ) # group the options for handling Episodes parameters group2 = OptionGroup ( parser , \"*** Episodes\" , \"use --name (--season ) (--episode ) ( -- summary ) to filter episodes you want to search \") group2 . add_option ( \"--name\" , dest = \"name\" , action = \"store\" , help = \"the name of the given serie\" ) group2 . add_option ( \"--episode\" , dest = \"episode\" , action = \"store\" , help = \"the number of the episode (optional)\" ) group2 . add_option ( \"--season\" , dest = \"season\" , action = \"store\" , help = \"the number of the season (optional)\" ) group2 . add_option ( \"--summary\" , dest = \"summary\" , action = \"store_true\" , help = \"boolean set to false by default, to only get the summary of the episode ( optional ) \") parser . add_option_group ( group2 ) [ ... ] ( options , args ) = parser . parse_args () [ ... ] Ensuite pour avoir l'aide on tape python go . py - h qui affiche Usage: go.py [ options ] Options: -h, --help show this help message and exit *** Search series: --title = TITLE make a search by title *** Details of series: --display = DISPLAY the name of the given serie *** Episodes: use --name ( --season ) ( --episode ) ( --summary ) to filter episodes you want to search --name = NAME the name of the given serie --episode = EPISODE the number of the episode ( optional ) --season = SEASON the number of the season ( optional ) --summary boolean set to false by default, to only get the summary of the episode ( optional ) Ok c'est tout beau et cool et ça marche (en plus ;-) Bon à présent les limitations de optparse (outre le fait qu'il est déprécié car plus maintenu) : les paramètres obligatoires : optparse permet de rendre des paramètres obligatoires via un required=True, mais le soucis c'est que les groupes ne sont pas exclusifs et que dans le \"options\" qui est ici : ( options , args ) = parser . parse_args () \"options\" contient TOUS les paramètres de TOUS les groupes. Du coup si on fait un required=True sur group0 . add_option ( \"--title\" , dest = \"title\" , type = \"string\" , help = \"make a search by title\" , required = True ) quand je taperai python go . py -- display -- name dexter il me sortira que j'ai oublié de renseigner le paramètre TITLE.... que je n'ai pas besoin pour l'utilisation de --display ... Ça ça m'a dérangé car du coup j'ai dû déporter l'aspect \"obligation de renseigner un paramètre\" plus tard dans mon code. paramètres en conflit optparse ne permet pas d'utiliser 2 fois le même nom de paramètre pour deux groupes distincts, il pète une vraie exception et rien à faire pour contourner. Du coup la convention de nommage des variables (pour les rendre unique) devient vite pénible pour conserver un semblant d'homogénéité entre les noms des méthodes de la classe de ma lib et les noms des actions mises en place dans mon script. aide trop verbeuse Comme dit plus tôt j'ai pres de 40 groupes, du coup l'aide en devient carrément illisible au premier coup d'oeil. Si si je vous promets, imaginer ça Usage: go.py [ options ] Options: -h, --help show this help message and exit *** Search series: --title = TITLE make a search by title *** Details of series: --display = DISPLAY the name of the given serie *** Episodes: use --name ( --season ) ( --episode ) ( --summary ) to filter episodes you want to search --name = NAME the name of the given serie --episode = EPISODE the number of the episode ( optional ) --season = SEASON the number of the season ( optional ) --summary boolean set to false by default, to only get the summary of the episode ( optional ) ... multiplié par 10 ... Donc fort de ces constats je me suis dit \"bon hé ho ; si ça me saoule déjà rien qu'à moi personnellement moi même ; je ne serai pas le seul ; voyons argparse\" Argparse Pour démarrer sans trop perdre de temps j'ai donc suivi la doc d'upgrade mentionnée plus haut pour passer de optparse à argparse. Ça a vite fonctionné et j'étais plutôt content ;) paramètres en conflits : résolu Mais comme je suis toujours insatisfait, je suis reparti sur mon envie de mettre les mêmes noms de variables à mes actions qu'à celle des paramètres de mes méthodes de classe de ma lib. en clair je voulais pour ça : def shows_episodes ( self , url , season = None , episode = None , summary = False , hide_notes = False , token = None ): [ ... ] def shows_characters ( self , url , summary = False , the_id = None ): [ ... ] faire un truc du genre : python go . py shows_episodes -- url ... -- season ... -- episode ... python go . py shows_characters -- url ... -- summary ... Pour y parvenir argparse a une option de gestion des conflits conflict_handler qu'on passe à 'resolve' Ainsi pourvu, taper les paramètres sera beaucoup facile à retenir ou tout du moins plus simple à taper que python go . py -- shows_episodes -- shows_episodes_url ... -- shows_episodes_season ... -- shows_episodes_episode python go . py -- shows_characters -- shows_characters_url ... -- shows_characters_summary ... vous voyez le genre de balles dans la tete qu'on pouvait se tirer avec optparse pour avoir des var \"unique\" (sans conflit ;) aide trop verbeuse : résolu De même l'aide cette fois-ci s'est retrouvée raccourcie drastiquement. avant on avait Usage: go.py [ options ] Options: -h, --help show this help message and exit *** Search series: --title = TITLE make a search by title *** Details of series: --display = DISPLAY the name of the given serie *** Episodes: use --name ( --season ) ( --episode ) ( --summary ) to filter episodes you want to search --name = NAME the name of the given serie --episode = EPISODE the number of the episode ( optional ) --season = SEASON the number of the season ( optional ) --summary boolean set to false by default, to only get the summary of the episode ( optional ) à présent ca donne : Usage: go.py [ options ] Options: -h, --help show this help message and exit shows_search - Search series: use shows_series make a search by title shows_displays - Details of series: use display = DISPLAY the name of the given serie shows_episodes - Episodes: use shows_episode ( --season ) ( --episode ) ( --summary ) to filter episodes you want to search La différence entre les 2 ? On n'affiche plus ici les options de chaque commande ! Mais ensuite si on veut l'aide complète de l'action shows_search on tapera un : python go.py show_search --help qui donnera l'aide escomptée usage: go [ options ] shows_search [ -h ] --title TITLE positional arguments: shows_search Search series: use --shows_search --title optional arguments: -h, --help show this help message and exit --title TITLE make a search by title C'est plus clair plus concis et on est tout joie ;) Tout ceci est obtenu avec une particularité propre à argparse qui est la création d'un sub-parser. Oui un subparser. Avant on avait la totalité des actions (sous la main avec optparse) comme montré dans le premier snipset A présent avec subparser ca donne ceci : parser = argparse . ArgumentParser ( prog = \"go\" , usage = ' %(prog)s [options]' , description = 'BetaSeries API Management' , conflict_handler = 'resolve' , add_help = True ) subparsers = parser . add_subparsers ( help = 'sub-command help' ) group0 = subparsers . add_parser ( 'shows_search' , help = 'Search series: use -- shows_search -- title ') group0 . add_argument ( \"shows_search\" , action = \"store_true\" , help = 'Search series: use --shows_search --title ' ) group0 . add_argument ( \"--title\" , action = \"store\" , required = True , help = \"make a search by title\" ) group1 = subparsers . add_parser ( \"shows_display\" , help = \"Details of series: use --shows_display --url \" ) group1 . add_argument ( \"shows_display\" , action = \"store_true\" , help = \"Details of series : use --shows_display --url \" ) group1 . add_argument ( \"--url\" , action = \"store\" , required = True , help = \"the url/name of the given serie\" ) group2 = subparsers . add_parser ( \"shows_episodes\" , help = \"Show Episodes: use -- shows_episodes -- url ( -- season ) ( -- episode ) ( -- summary ) to filter episodes you want to search \") group2 . add_argument ( \"shows_episodes\" , action = \"store_true\" , help = \"Episodes: -- shows_episodes -- url ( -- season ) ( -- episode ) ( -- summary ) to filter episodes you want to search \") group2 . add_argument ( \"--url\" , action = \"store\" , required = True , help = \"the url of the given serie\" ) group2 . add_argument ( \"--episode\" , action = \"store\" , help = \"the number of the episode (optional)\" ) group2 . add_argument ( \"--season\" , action = \"store\" , help = \"the number of the season (optional)\" ) group2 . add_argument ( \"--summary\" , action = \"store_true\" , help = \"boolean set to false by default, to only get the summary of the episode ( optional ) \") [ ... ] args = parser . parse_args () if len ( sys . argv ) > 1 : do_action ( args ) else : parser . error ( \"enter -help to see the options you can use\" ) Une petite explication sur le parm help s'impose dans l'utilisation que j'en ai faite. Quand on tape python go.py --help l'aide affichée n'est autre que le texte (help=\"..\") qui se trouve sur ma ligne groupX = subparser.add_parser() Quand on tape python go.py shows_search --help l'aide affichée est celle sur la ligne add_argument(\"show_search\",help=\"..\") puisque cette fois ci je veux l'aide de la commande elle même. La différence c'est que la premiere sert pour afficher l'aide de toutes les commandes, la seconde pour l'aide de la commande elle seule. Donc, personnellement je mets la même chose sinon je n'ai pas d'aide affichée suffisement explicite. On pourrait se dire que argparse va réafficher quand même l'aide déjà fournie pour ici groupX = subparser.add_parser() , mais non : voici la difference sans le texte d'aide usage: go [ options ] shows_search [ -h ] --title TITLE positional arguments: shows_search optional arguments: -h, --help show this help message and exit avec le texte d'aide usage: go [ options ] shows_search [ -h ] --title TITLE positional arguments: shows_search Search series: use --shows_search --title optional arguments: -h, --help show this help message and exit --title TITLE make a search by title Autre différence majeure entre optparse et argparse : Récuperer les infos saisies avec optparse on faisait à la tout fin de la definition des groupes (pour mon cas) ( options , args ) = parser . parse_args () puis dans sa fonction on testait les paramètres : if options . title : #traitement elif options . name : #traitement Récuperer les infos saisies avec argparse args = parser . parse_args () Ici args est une liste nommée Namespace qui contient uniquement les paramètres disponibles via le subparser concerné, exemple un print de args donnerait ceci : python go_new . py shows_episodes -- url dexter Namespace ( episode = None , season = None , shows_episodes = True , summary = False , url = 'dexter' ) Ensuite donc dans sa fonction on teste si la commande est dispo dans options : f hasattr ( options , 'shows_characters' ): #traitement elif hasattr ( options , 'shows_episodes' ): #traitement On doit utiliser if hasattr(options, 'shows_episodes'): à la place de if options.shows_episodes sinon python produira systématiquement une erreur, puisque l'info sur options.shows_episodes n'est pas disponible quand on traitera de .... shows_characters... et inversement. voili voilo. Pour voir mes scripts complets avec optparse et argparse ; ils sont tous deux disponibles sur github au beau milieu d'un projet ;) Si vous avez des remarques &/ou corrections, lâchez vous ;) Si vous avez envie d'article sur un sujet particulier, \"il y a un\" billet \"pour cela\" (c) et je me fais fort de (tenter) d'y répondre ;)","tags":"Techno","url":"https://foxmask.net/post/2013/03/13/incontournables-pythonerie-parse-parse-parsera-la-derniere-la-derniere/","loc":"https://foxmask.net/post/2013/03/13/incontournables-pythonerie-parse-parse-parsera-la-derniere-la-derniere/"},{"title":"Incontournables Pythonerie : BigBrother is logging you","text":"Comme chacun sait, il est ûber bradiqueu de suivre ce que fait son programme, pour tenter de diagnostiquer tout problème protentiel dans son application. Pour cela on exploite tous plus ou moins la même chose avec nos langages respectifs, le plus répandu en JAVA par exemple, étant log4j . En Python donc, on dispose du module Logger . Comment marche un système de journalisation : on défini un nom de fichier dans lequel sera ajouté tous les évènements de son programme on défini un niveau de journalisation parmi INFO,DEBUG,WARN,ERROR,CRITICAL on défini un formatage des lignes de journalisation Voici comment tout cela se passe avec Logger : Où logger & les niveaux de log Un exemple le plus basique qui soit, consiste à afficher un message dans la console à l'exécution du script : import logging logging . warning ( 'Watch out!' ) # will print a message to the console logging . info ( 'I told you so' ) # will not print anything vous affichera un beau WARNING:root:Watch out! Cela est bien sympathique mais la plupart du temps on veut mettre tout cela dans un fichier de log, ceci s'effectue comme suit : import logging logging . basicConfig ( filename = 'example.log' , level = logging . DEBUG ) logging . debug ( 'This message should go to the log file' ) logging . info ( 'So should this' ) logging . warning ( 'And this, too' ) A présent plus rien ne s'affiche à la console mais votre fichier example.log contient bien comme attendu : DEBUG : root : This message should go to the log file INFO : root : So should this WARNING : root : And this , too Dans la foulée, on en a profité pour définir le niveau de journalisation à DEBUG (via level=logging.DEBUG), ce qui nous a permit d'avoir toutes les traces dans le fichier de log Il est par ailleurs tout à fait possible d'utiliser la journalisation au travers de plusieurs modules : # myapp.py import logging import mylib def main (): logging . basicConfig ( filename = 'myapp.log' , level = logging . INFO ) logging . info ( 'Started' ) mylib . do_something () logging . info ( 'Finished' ) if __name__ == '__main__' : main () # mylib.py import logging def do_something (): logging . info ( 'Doing something' ) ce qui affichera : INFO : root : Started INFO : root : Doing something INFO : root : Finished Formatage des logs Après avoir vu où logger et quel(le)s (niveaux d') infos, voici à présent le formatage de ses chers log: import logging logging . basicConfig ( filename = 'example.log' , level = logging . DEBUG ), \\ format = ' %(asctime)s %(message)s ' ) logging . debug ( 'This message should go to the log file' ) logging . info ( 'So should this' ) logging . warning ( 'And this, too' ) ceci vous produire une ligne affichage la date courant + le message : 2013-02-22 14:43:08,486 This message should go to the log file 2013-02-22 14:43:08,487 So should this 2013-02-22 14:43:08,487 And this, too Enfin voici un exemple complet avec en plus un fichier de configuration de log définissant par module, où mettre les logs (console / fichier) et quels niveaux de détail ces logs auront : logging.conf: [loggers] keys = root,simpleExample [handlers] keys = consoleHandler [formatters] keys = simpleFormatter [logger_root] level = DEBUG handlers = consoleHandler [logger_simpleExample] level = DEBUG handlers = consoleHandler qualname = simpleExample propagate = 0 [handler_consoleHandler] class = StreamHandler level = DEBUG formatter = simpleFormatter args = (sys.stdout,) [formatter_simpleFormatter] format = %(asctime)s - %(name)s - %(levelname)s - %(message)s datefmt = Ce fichier de configuration des log défini, on l'exploite ensuite dans son script comme ceci : import logging import logging.config logging . config . fileConfig ( 'logging.conf' ) # create logger logger = logging . getLogger ( 'simpleExample' ) # 'application' code logger . debug ( 'debug message' ) logger . info ( 'info message' ) logger . warn ( 'warn message' ) logger . error ( 'error message' ) logger . critical ( 'critical message' ) Ce qui produira comme log : 2013-02-22 14:57:30,744 - simpleExample - DEBUG - debug message 2013-02-22 14:57:30,745 - simpleExample - INFO - info message 2013-02-22 14:57:30,745 - simpleExample - WARNING - warn message 2013-02-22 14:57:30,745 - simpleExample - ERROR - error message 2013-02-22 14:57:30,745 - simpleExample - CRITICAL - critical message la première colonne contient la date, la seconde, le script utilisé/logger utilisé, la troisième le niveau d'info de log, et enfin la ligne de log elle même qu'on a écrit dans le code. Quelques subtilités , vous permettent : 1) d'avoir un fichier de log par exécution avec l'option filemode=\"w\" comme ceci : logging . basicConfig ( filename = 'example.log' , level = logging . DEBUG , filemode = \"w\" ) 2) de définir le niveau de log sur la ligne de commande si cela vous botte via --log=INFO par exemple 3) d'avoir des lignes de log utilisant des variables comme ceci logging . warning ( ' %s before you %s ' , 'Look' , 'leap!' ) qui affichera Look before your leap! 4) de jouer sur le format d'affichage de la date comme ceci : logging . basicConfig ( filename = 'example.log' , level = logging . DEBUG , format = ' %(asctime)s %(message)s ' , datefmt = '%m/ %d /%Y %I:%M:%S %p' ) Ce billet est très largement inspiré de la doc, très bien faite, et m'aura permit de vous présenter un nouveau volet indispensable à vos développements ;)","tags":"Techno","url":"https://foxmask.net/post/2013/03/11/incontournable-pythonerie-bigbrother-is-logging-you/","loc":"https://foxmask.net/post/2013/03/11/incontournable-pythonerie-bigbrother-is-logging-you/"},{"title":"Oracle et Opatch","text":"A une époque, patcher Oracle était un sinécure :P Depuis avec OPatch c'est on ne peut plus simple ! On récupère un patch chez Oracle, on le décompresse, on tape opatch apply et hop ! Donc après la décompression, ça donne : cd path_ou_on_a_decompresse_le_path application du patch oracle opatch apply Si vous rencontrez un erreur du genre : OPatch cannot find a valid oraInst.loc file to locate Central Inventory. Alors taper la commande suivante : opatch apply -invPtrLoc /u01/app/oracle/product/11.2.0.1.0/db/oraInst.loc le paramètre invPtrLoc indique à opatch, où se trouve le fameux \"Central Inventory\" ce qui donnera par exemple pour le patch 8795792 Invoking OPatch 11 .2.0.1.6 Oracle Interim Patch Installer version 11 .2.0.1.6 Copyright ( c ) 2011 , Oracle Corporation. All rights reserved. Oracle Home : /u01/app/oracle/product/11.2.0.1.0/db Central Inventory : /u01/app/oracle/product/11.2.0.1.0/inventory from : /u01/app/oracle/product/11.2.0.1.0/db/oraInst.loc OPatch version : 11 .2.0.1.6 OUI version : 11 .2.0.1.0 Log file location : /u01/app/oracle/product/11.2.0.1.0/db/cfgtoollogs/opatch/opatch2011-07-29_15-37-22PM.log Applying interim patch '8795792' to OH '/u01/app/oracle/product/11.2.0.1.0/db' Verifying environment and performing prerequisite checks... Voulez-vous continuer ? [ y | n ] User Responded with: Y All checks passed. Entrez votre adresse électronique pour être informé des problèmes de sécurité, installer et lancer Oracle Configuration Manager. Le processus est plus simple pour vous si vous utilisez votre adresse électronique/nom utilisateur My Oracle Support. Pour plus de détails, consultez la page http://www.oracle.com/support/policies.html. Adresse électronique/nom utilisateur : you@yourcompany.com Indiquez votre mot de passe My Oracle Support pour recevoir les mises à jour de sécurité via votre compte My Oracle Support. Mot de passe ( facultatif ) : Backing up files... Application d ' un patch au composant oracle.rdbms, 11 .2.0.1.0... Patch 8795792 successfully applied Log file location: /u01/app/oracle/product/11.2.0.1.0/db/cfgtoollogs/opatch/opatch2011-07-29_15-37-22PM.log OPatch succeeded. Une fois fait, relancer la base Oracle que vous aurez bien évidemment stoppée avant de commencer, je ne vous l'avais pas dit ? ba il faut tout lire avant de commencer ;)","tags":"Techno","url":"https://foxmask.net/post/2013/03/07/oracle-et-opatch/","loc":"https://foxmask.net/post/2013/03/07/oracle-et-opatch/"},{"title":"de PHP à Python : Minie petite souris","text":"Nouvel opus de la série de de PHP à Python . Aujourd'hui, Comment parcourir un fichier de configuration ini ? Alors en intro, un truc bien cool pour tout à chacun : les fichiers ini se lisent aussi bien avec Python que PHP. Zavez une appli avec un bon (gros) fichier de config au format ini ; pas de problème, vous allez voir ce que vous allez voir ;) Pour illustrer ce billet j'ai pris un fichier de config de mon forum HaveFnuBB écrit en PHP que voici Le bout de code PHP lisant mon fichier de config permet de connaitre les Réponses HTML possible, le code est le suivant : de PHP ... Ceci affichera les valeurs de mes variables définies dans ma section sus mentionnée array(3) { [\"minifyCSS\"]=> string(1) \"1\" [\"minifyJS\"]=> string(0) \"\" [\"minifyCheckCacheFiletime\"]=> string(0) \"\" } ... à Python Pour le script python cela donnera #!/usr/bin/python # -*- coding: utf-8 -*- import ConfigParser import os config = ConfigParser . ConfigParser () config . read ( os . getcwd () + '/havefnubb/var/config/defaultconfig.ini.php.dist' ) print config . items ( 'jResponseHtml' ) pour obtenir en résultat : [( 'minifyjs' , 'off' ), ( 'minifycss' , 'on' ), ( 'minifycheckcachefiletime' , 'off' )] Pour l'oeil aguerri, vous aurez remarqué que 1 est devenu on et \"\" est devenu off pour python. Le module ConfigParser ne se contente pas que de vous afficher le contenu d'une section (fort heureusement). Avec celui-ci vous avez un accès direct à tous les paramètres avec une instruction get comme ceci : #!/usr/bin/python # -*- coding: utf-8 -*- import ConfigParser import os config = ConfigParser . ConfigParser () config . read ( os . getcwd () + '/havefnubb/var/config/defaultconfig.ini.php.dist' ) print config . get ( 'jResponseHtml' , 'minifyjs' ) vous affichera le \"off\" vu plus haut. Outre l'accès direct, vous pouvez également pondre un fichier de config à la volée en commençant par ajouter la section puis les variables distinctes comme ceci : import ConfigParser config = ConfigParser . RawConfigParser () config . add_section ( 'Section1' ) config . set ( 'Section1' , 'an_int' , '15' ) config . set ( 'Section1' , 'a_bool' , 'true' ) config . set ( 'Section1' , 'a_float' , '3.1415' ) config . set ( 'Section1' , 'baz' , 'fun' ) config . set ( 'Section1' , 'bar' , 'Python' ) config . set ( 'Section1' , 'foo' , ' %(bar)s is %(baz)s !' ) # Writing our configuration file to 'example.cfg' with open ( 'example.cfg' , 'wb' ) as configfile : config . write ( configfile ) Produira un fichier example.cfg : [Section1] an_int = 15 a_bool = true a_float = 3.1415 baz = fun bar = Python foo = %(bar)s is %(baz)s! ici %(bar) et %(baz) seront remplacées par les valeurs de leur variable définie juste au dessus Coté PHP on n'a rien d'équivalent... sauf au sein de(s) framework(s) PHP comme Jelix et sa méthode jIniFileModifier('fichier')->getValue('variable','section') La prochaine partie traitera du traitement de la journalisation (les logs)","tags":"Techno","url":"https://foxmask.net/post/2013/03/04/de-php-a-python-minie-petite-souris/","loc":"https://foxmask.net/post/2013/03/04/de-php-a-python-minie-petite-souris/"},{"title":"Oracle Sauvegarder avec Impdp / Expdp et le Parfile","text":"Dans un billet précédent j'avais abordé ces 2 outils pour se faire des backups \"en réseau\" , voici à présent plus en détail, le fonctionnement des ces 2 outils. Oracle fournit donc bien évidement des outils pour produire des sauvegardes de ses données. Les plus basiques sont impdp et expdp. Le premier produit un import d'un dump. Le second export un/des schémas. impdp Exemple d'import d'un schéma customer , dont le nom de fichier se nomme foo.dmp dans mon schema nommé MYWORK et ma tablespace MYWORKD_TB impdp USER/pass schemas = customer dumpfile = foo.dmp directory = wkdir remap_schema = CUSTOMER:MYWORK remap_tablespace = CUSTOMER_TB:MYWOKRD_TB LOGFILE = foo.log le paramètre directory indique à oracle quel est l' objet directory dans lequel il doit utiliser le dumpfile et ecrire dans le fichier de log foo.log pour connaitre le nom du directory, on se connecte comme dba à la base oracle puis on tapera: select * from dba_directories ; on obtiendra donc le path vers le dossier où mettre foo.dmp pour qu'il soit localisé par Oracle. Quand on veut lancer ses commandes, on a bien souvent une bonne dizaine de paramètres à fournir et il devient assez laborieux de les retenir pour les besoins particuliers d'une base, ou de quelques schémas contenu dans notre base. C'est là qu'entre en jeu le \"parfile\". Celui ci contient ni plus ni moins que les paramètres que l'on saisirait sur la ligne de commandes: voici la tête qu'il aurait avec l'exemple précédent : customer.par schemas = customer dumpfile = mywork % u . dmp directory = wkdir remap_schema = CUSTOMER : MYWORK remap_tablespace = CUSTOMER_TB : MYWOKRD_TB LOGFILE = foo . log et on lancerait l'import comme ceci impdp USER / pass parfile = customer . par cette fois ci customer.par est cherché dans le dossier courant, et non dans le directory oracle expdp L'export cette fois ci fonctionne de la même façon : expdp user/pass directory = wkdir logfile = exp_mywork.log dumpfile = mywork.dmp et c'est tout. parallel Un paramètre intéressant lorsqu'on vous fournit des dumps est l'option parallel, utilisable lors des exports et des imports. L'intérêt est d'accélérer le traitement des sauvegardes Le dumps est en \"morceaux\" (de tailles non egales) ; le nombre de morceaux etant la valeur donnée à l'option parallel, exemple : expdp user/pass directory = wkdir logfile = exp_mywork.log dumpfile = mywork.dmp parallel = 4 produira les fichiers mywork1.dmp mywork2.dmp mywork3.dmp mywork4.dmp ensuite pour remonter ce dump on ecrira : impdp user / pass directory = wkdir logfile = exp_mywork . log dumpfile = mywork % U . dmp parallel = 4 That's all folk ;)","tags":"Techno","url":"https://foxmask.net/post/2013/02/28/oracle-sauvegarder-avec-impdp-expdp-et-le-parfile/","loc":"https://foxmask.net/post/2013/02/28/oracle-sauvegarder-avec-impdp-expdp-et-le-parfile/"},{"title":"Incontournables Pythonerie : Oh ho ho Géant Vert","text":"Le Géant Vert en question est libreoffice ;-) Ici un très simple billet pour vous montrer comment se faire un document OpenOffice depuis un script Python en 5 lignes :P Le script va se composer comme suit : Récupération des données à publier dans le document Récupération du document OpenOffice sur lequel va reposer notre génération finale \"Fusion\" des points 1 & 2 ;) Pour procéder à cette création, on aura besoin comme prérequis de Relatorio , un projet python pour créer des doc ooo/pdf/html, à partir d'objets python. Donc un petit pip install relatorio fera l'affaire pour disposer de l'armada ;) 1) Récupération des données à publier Ici je partirai d'un script Python renvoyant un dict, ni plus ni moins. Le voici , il se nomme \"maraicher.py\": potager = dict ( customer = 'Bonduelle' , version = '1.12.09' , previous_version = '1.12.08' , middleware = [ { 'composant' : 'Aluminium' , 'version' : '1.0.2' }, { 'composant' : 'Fer' , 'version' : '1.1.2' } , { 'composant' : 'Papier' , 'version' : '2.55.89' }, ], core = [ { 'composant' : 'Petits Pois' , 'version' : '1.12.13' }, { 'composant' : 'Carottes' , 'version' : '3.45.69' }, { 'composant' : 'Poireaux' , 'version' : '2.98.10' }, { 'composant' : 'Pommes de terre' , 'version' : '9.1' }, ], ) 2) le document OpenOffice : Ce dernier contient ce que vous avez ici . C'est en fait un tableau recensant des noms de produit avec leur versions associées. 3) Fusion La fusion va consister à lire les données du dict et les injecter dans le doc, tout cela se fait, like this (en 5lignes hein ;) : from relatorio.templates.opendocument import Template from maraicher import potager basic = Template ( source = None , filepath = 'Geant-Vert.odt' ) basic_generated = basic . generate ( o = potager ) . render () file ( 'Geant-Vert1.odt' , 'wb' ) . write ( basic_generated . getvalue ()) le résultat est ici Bon Appétit ;) edit du 25/03/2015 : mise à jour du lien vers le projet relatorio https://code.google.com/p/python-relatorio/","tags":"Techno","url":"https://foxmask.net/post/2013/02/25/incontournables-pythonerie-oh-ho-ho-geant-vert/","loc":"https://foxmask.net/post/2013/02/25/incontournables-pythonerie-oh-ho-ho-geant-vert/"},{"title":"Incontournables Pythonerie : Die Arg","text":"\"Die Arg\" , n'est pas le dernier opus de Bruce Willis ;) mais derrière ce titre j'ai caché le module argparse ;) En préambule à ce billet, j'ai entrepris de produire des billets sur un script j'exploite en prod dans mon quotidien. J'ai donc découpé ce script en petits bouts pour vous illustrer chaque partie que j'ai exploité, et la première d'entres elles est ce qui suit;) Ce dernier permet de traiter/interpreter les arguments entrés sur la ligne de commandes, à la suite du nom de votre script. Coté PHP, seuls les framework en propose(raie)nt, tels Jelix, ZF, SF, CakePHP, etceteri etcetara Avant de vous montrer le module argparse, je me dois de vous montrer à quoi ressemble un script python sans l'usage de celui ci. Le script ci dessous va afficher un texte succinct quand les paramètres attendus ne seront pas au rendez vous ou vous affichera une phrase avec ce qui a été saisi : test_arg.py from sys import argv def main (): #mettons ici un peu d'aide quand l'utilisateur ne s'en sortira pas usage = \"To run the script enter -e environment name -r release name. \\ nfor example : \\ npython test_arg - e foxenv - r v1 . 0 - FINAL \\ n \\ nYou can optionally use the switches : \\ n - c / path / to / configfile to use an alternative configfile \\ n - l / path / to / logging / config / file to use an alternative configfile for logging \" env = '' release = '' #pas mis assez d'arguments if len ( argv ) < 2 : #affichons donc l'aide print usage exit ( 0 ) #oops un pb ? on n'a pas encore le bon compte d'arguments if len ( argv ) < 5 and len ( argv ) > 2 : print \"the environment name ( specified with -e ) and release name ( specified with - r ) are mandatory \" exit ( 0 ) switch = argv [ 1 ] switch2 = argv [ 3 ] #où est quoi ? #vérifions l'order des switches if switch == '-e' and switch2 == '-r' : env = argv [ 2 ] release = argv [ 4 ] elif switch == '-r' and switch2 == '-e' : env = argv [ 4 ] release = argv [ 2 ] #j'ai tape portenawak - affichons ce qui ne va pas else : print \"the environment name ( specified with -e ) and release name ( specified with - r ) are mandatory \" #tout est ok : #affichage des valeurs entrées sur la ligne de commandes print \"environment {0} release {1} \" . format ( env , release ) if __name__ == '__main__' : main () Ainsi en tapant : python test_arg.py on obtient l'aide : To run the script enter -e environment name -r release name. for example : python test_arg -e foxenv -r v1.0-FINAL You can optionally use the switches : -c /path/to/configfile to use an alternative configfile -l /path/to/logging/config/file to use an alternative configfile for logging Ensuite python test_arg.py -r 123 .2 -e fox donne : environment fox release 123 .2 Ok parfait ca marche mais c'est tiré par les cheveux, fastidieux et quand on a plus complexe à traiter on n'est pas sorti de l'auberge. Voici à présent la même version avec le module \"optparse\" (la version fonctionnant avec ma debian et python 2.7, argparse étant pour la v3 de python) test_optparse.py from optparse import OptionParser import os def main (): #meme message d'aide en plus concis usage = \"%prog -e environment name -r release name. \\n for example : \\n python test_optparse -e envname -r 20130101\" parser = OptionParser ( usage ) #ajout d'option de paramètres utilisables parser . add_option ( \"-e\" , \"--env\" , dest = \"environment\" , help = \"the environment name to use to build the delivery\" , metavar = \"ENV\" ) parser . add_option ( \"-r\" , \"--rel\" , dest = \"release\" , help = \"the release name of this delivery (used to name the final package like release-RELEASE-yyyymmdd)\" , metavar = \"RELEASE\" ) parser . add_option ( \"-c\" , \"--conf\" , dest = \"configfile\" , help = \"the path where the config file is located. This file should contain the name of the environment from which to download the archives. By Default the script will search in ./env_dirs.conf\" , default = \"./env_dirs.conf\" , metavar = \"CONFIG\" ) parser . add_option ( \"-l\" , \"--log\" , dest = \"configlogfile\" , help = \"the path where the config file for the loggging is located.\" , metavar = \"LOGGING_CONFIG\" ) ( options , args ) = parser . parse_args () #je n'ai pas donné d'environement ni de release ... hophop error :) if options . environment == None or options . release == None : parser . error ( \"options -e and -r are mandatory\" ) else : print \"environment {0} release {1} \" . format ( options . environment , options . release ) if __name__ == '__main__' : main () Cours précis clair :) A présent en tapant la ligne précédente sans aucun argument on obtient : python test_optparse.py Usage: test_optparse.py -e environment name -r release name. for example : python make_delivery -e envname -r 20130101 test_optparse.py: error: options -e and -r are mandatory la différence est que cette fois ci une erreur est retournée. (c'est le message dans parse.error) A présent le petit plus, pas dégueux, en tapant python test_optparse.py -h on obtient : Usage: test_optparse.py -e environment name -r release name. for example : python test_optparse -e envname -r 20130101 Options: -h, --help show this help message and exit -e ENV, --env = ENV the environment name to use to build the delivery -r RELEASE, --rel = RELEASE the profinance release name of this delivery ( used to name the final package like release-RELEASE-yyyymmdd ) -c CONFIG, --conf = CONFIG the path where the config file is located. This file should contain the name of the environment from which to download the archives. By Default the script will search in ./env_dirs.conf -l LOGGING_CONFIG, --log = LOGGING_CONFIG the path where the config file for the loggging is located. By Default the script will search in ~/MakeDelivery/logging.conf Voilà, tout propre, nickel ! le reste fonctionne exactement pareil ! Merci (arg|opt)pargse ;) les sources de l'article sont disponibles sur github La prochaine partie traitera du traitement des fichiers de configuration","tags":"Techno","url":"https://foxmask.net/post/2013/02/25/incontournables-pythonerie-die-arg/","loc":"https://foxmask.net/post/2013/02/25/incontournables-pythonerie-die-arg/"},{"title":"de PHP à Python : MeetGeek !","text":"J'aurai pu monter un site \"QQ la praline\", style MeetGeek (merde il est déjà pris:P) pour coucher sur le papier ce qui suit ;) Samedi j'ai lancé une proposition en l'air sans aucun formalisme sur twitter . Cette proposition la voici : Vous êtes développeur PHP et avez envie de vous mettre au python ou vous vous êtes lancés il y a peu et souhaitez monter un projet opensource ? Je vous propose mes services ! Quel genre de projet m'intéresse ? Tous. Que ce soit une paire de libs pour télécharger des bidules de l'autre bout du web ou un projet web en django / bottle, ça sera avec plaisir que je tenterai d'apporter ma contribution. L'unique but à tout cela, comme un harlem shake : je bouge tout seul là, mais, bouger à plusieurs serait l'éclate ;) c'est sans autre contre partie que participer à un projet pour le plaisir. Pourquoi cibler des développeurs PHP ? Parce qu'il n'y aura pas de grand fossé entre nous :-) Si des projets python existant manquent de bras, je ne suis pas fermé pour contribuer également. Je vous dirai si je me sens à l'aise dans mes baskets pour votre projet. J'ai de la motivation à revendre alors chuis prêt à moudre de grain. Vous pourriez vous dire s'il est si motivé il n'a qu'à lancer ses propres projets. C'est déjà fait mais seul c'est beaucoup moins fun. Si vous voulez tout de même voir, ceux ci , sont ici : un gestionnaire de racourcisseurs d'URL et là un ifttt like (veryvery early stage) Pour revenir sur MeetGeek, l'idée que je m'en faisais, aurait été de mettre en relations des (idées de) projets et des geeks motivés pour contribuer. Voilà ! Donc Si vous êtes intéressés faites tomber votre commentaire à la suite ;)","tags":"Techno","url":"https://foxmask.net/post/2013/02/25/de-php-a-python-meetgeek/","loc":"https://foxmask.net/post/2013/02/25/de-php-a-python-meetgeek/"},{"title":"Jelix PHP5 Framework : version 1.5.0 out","text":"Le fameux Framework PHP5 Jelix sort sa version 1.5.0 Les contributeurs se sont mis en 4 pour sortir cette dernière version dont voici le menu des nouveautés : Support PHP 5.3 only à présent. Utilisation dans Jelix des fonctionnalités introduites par PHP 5.3, comme les fonctions anonymes, late static binding, les namespaces, la constante __DIR__ etc Support de HTML5 avec un nouvel objet Response HTML Pour jForms : Le générateur HTML est maintenant extensible. Vous pouvez fournir vos propres plugins pour générer tel ou tel contrôle de formulaire. Pour jDao : il est à présent possible d'importer la définition d'un autre DAO pour éviter de recopier une DAO similaire et ajouter prou de colonnes en plus Pour jAuth : nouveau système de hashage Pour jKVDb : nouveau plugin pour l'API dba Nouveaux modules : jSoap fusionnant jWSDL jTCPDF jACL2 : déplacement des ressources (classes, properties) dans un module dédié jacl2 jPref: déplacement de la classe jPref dans un module dédié outre tout cela, nombre de nouveautés concernent jZones, jEvents, jLocale, jelix-script et j'en passe. Ces nouveautés sont passées au crible sur le site même. Comme à l'accoutumé la doc complète pour cette version est disponible. Je vous invite donc à Télécharger cette dernière version 1.5.0 . Et la v 1.6.0 est déjà sur les rails, viendez forker ;) Have Fnu ! :D","tags":"Techno","url":"https://foxmask.net/post/2013/02/20/jelix-1-5-0-out/","loc":"https://foxmask.net/post/2013/02/20/jelix-1-5-0-out/"},{"title":"de PHP à Python : X aime L","text":"Nouvel opus de la série de de PHP à Python . Aujourd'hui, Comment parcourir un fichier XML ? Bon en plus je vais pousser le bouchon à utiliser XPath ;) tant qu'à lire un fichier, autant que ça soit \"intelligent\" et pas juste lire pour lire (quoi je me répète répète;) La matière première commune à ces 2 scripts sera ce fichier XML allégé pour l'occasion mais issu directement de autodeploy un outil ( opensource devrai-je le préciser ? ) que j'utilise au quotidien pour faire des déploiements d'applications les doigts dans le nez ;) En prod je n'ai que 200 environnements, avec en moyenne près de 10 targz chacun, je vous laisse imaginez le poids du trucs (pres de 10Mo grosso merdo;) Vivendi qui s'en sert en gere 10x plus ;) Ici je n'en extrairai que le nom de l'application/software et l'URL de PHP ... la version PHP donne ceci avec un coup de DOMXPath : Load('ConfigurationWrapper'); $xpath = new DOMXPath($doc); $app = find_archive($xpath,$envName,'/applicationservers/applicationserver/applications/application'); $soft = find_archive($xpath,$envName,'/softwares/software'); function find_archive(DOMXPath $xpath, $envName, $queryPath) { $query = '//environments/environment[@name=\"'.$envName.'\"]'.$queryPath; $entries = $xpath->query($query); foreach ($entries as $entry) { echo $entry->getAttribute('name') . ' ' . $entry->getAttribute('uri') .\"\\n\"; } } affichera : app1 http://maven.intranet.mycompany.com/repository/my-app1.tar.gz app2 http://maven.intranet.mycompany.com/repository/my-app2.tar.gz My Oracle Upgrade http://maven.intranet.mycompany.com/repository/my-oracle-upgrade.tar.gz ... à Python Le même process en Python avec cette fois ci le module LXML donne ceci : # -*- coding: utf-8 -*- from lxml import etree from lxml.etree import Element autodeploy_filename = 'ConfigurationWrapper' env = 'my-environmentA' def find_archive ( env_name , tree , xpath ): my_nodes = tree . xpath ( '//environments/environment[@name=\"' + env_name + '\"]' + xpath ); for node in my_nodes : if len ( node . get ( 'uri' , None )) > 0 : print \" {name} {uri} \" . format ( name = node . get ( 'name' , None ), uri = node . get ( 'uri' , None )) tree = etree . parse ( autodeploy_filename ) #get the name of the applications find_archive ( env , tree , '/applicationservers/applicationserver/applications/application' ) #get the name of the Software find_archive ( env , tree , '/softwares/software' ); affichera : app1 http://maven.intranet.mycompany.com/repository/my-app1.tar.gz app2 http://maven.intranet.mycompany.com/repository/my-app2.tar.gz My Oracle Upgrade http://maven.intranet.mycompany.com/repository/my-oracle-upgrade.tar.gz Actuellement sur l'intranet j'ai une appli en Jelix , qui parcourt le XML (dans un cache) et affiche tout le toutim aux end-user (consultants) pour leur permettre de trouver ce qui est installé sur les environnements jEE ainsi que les adresses des bases etc... Quand les consultants veulent que je livre un client avec une release donnée, alors, à l'autre bout de la chaine j'ai un script Python home made , qui parcourt le même flux et me pond un livrable à partir d'un fichier de configuration qui me décrit l'arbo cible et dépose chaque targz dans cette dernière. D'ailleurs un billet est à venir sur la lecture de fichier de config en PHP/Python, une histoire de petite souris mais je n'en dirais pas plus ;)","tags":"Techno","url":"https://foxmask.net/post/2013/02/18/de-php-a-python-x-aime-l/","loc":"https://foxmask.net/post/2013/02/18/de-php-a-python-x-aime-l/"},{"title":"Pythonthon","text":"L'association Python est sous le coup de perdre la marque Python L'Afpy est sur le qui vive, le branle bas va être lancé je pense au vu de ce qui s'est vite dit sur le mailing-list AMHA, vu le gigantesque microcosme gravitant autour du langage je suis optimiste : Il y a tant de ramifications, d'organisations commerciales, industrielles, scientifiques, associations autour de python, qu'il serait ahurissant et impensable que tous ne puissions pas en démontrer l'utilisation dans note quotidien. Par exemple toutes les distributions linux pourraient faire un geste, et notamment celles reposant sur une entreprise comme RedHat, Ubuntu, SuSe. De même l'existence de user group python semblent aussi peser dans la balance. Et là d'autres fondations existantes utilisant python sont nombreuses, pour n'en citer qu'une Django. Il suffirait que les instances en charge du dossier fasse une chose simple pour se rendre compte de l'étendue de l'utilisation de Python dans le monde informatique : aller sur le plus grand magasin en ligne au monde et taper python. Pour moi c'est quand même cousu de fil blanc. J'espère avoir raison ;)","tags":"Techno","url":"https://foxmask.net/post/2013/02/16/pythonthon/","loc":"https://foxmask.net/post/2013/02/16/pythonthon/"},{"title":"Incontournables Pythonerie","text":"Parallèlement à ma pitite série \" de PHP à Python \", je vais débuter ici une nouvelle série qui illustrera des modules facilement accessibles aux débutants et surtout leur rendront un énorme service. Dans le viseur, pour l'heure, j'ai : argparse (python v3) / optparse (python v2) : module permettant de récupérer les arguments entrés sur la ligne de commandes logger : module permettant de journaliser ce que bon vous semble à la manière du fameux log4j d'Apache urllib2 : un module permettant de manipuler des fichiers distants Si vous avez d'autres idées de modules python, \"rikiki\" mais \"maousse costaud\", mettez les moi en commentaires je verrai pour en faire un sujet ;)","tags":"Techno","url":"https://foxmask.net/post/2013/02/15/incontournables-pythonerie/","loc":"https://foxmask.net/post/2013/02/15/incontournables-pythonerie/"},{"title":"Oracle Trouver les tablespaces utilisés par un schéma","text":"Tout comme Roméo a sa Juliette, une base Oracle possède son tablespace ;) Donc pour trouver ce(s) dernier(s) tout se résume à interroger la table dba_segment dont le tablespace_name ne contient aucun de ceux des TB \"system\". SET linesize 300 SET pagesize 20 SET feedback off SELECT sysdate , a . owner username , a . tablespace_name , round ( b . total_space / 1024 / 1024 , 2 ) \"Total (MB)\" , round ( SUM ( a . bytes ) / 1024 / 1024 , 2 ) \"Used (MB)\" , round ( SUM ( a . bytes / b . total_space ) * 100 , 2 ) \"% Used\" FROM dba_segments a , ( SELECT tablespace_name , SUM ( bytes ) total_space FROM dba_data_files GROUP BY tablespace_name ) b WHERE a . tablespace_name NOT IN ( 'SYSAUX' , 'SYSTEM' , 'UNDOTBS1' , 'UNDOTBS2' ) AND a . tablespace_name = b . tablespace_name GROUP BY a . tablespace_name , a . owner , b . total_space / 1024 / 1024 ORDER BY a . tablespace_name , a . owner ; Ceci permet ensuite d'interroger dba_data_files pour trouver les noms de fichiers qui constituent le Tablespace cherché pour l'agrandir par exemple.","tags":"Techno","url":"https://foxmask.net/post/2013/02/14/oracle-trouver-les-tablespaces-utilises-par-un-schema/","loc":"https://foxmask.net/post/2013/02/14/oracle-trouver-les-tablespaces-utilises-par-un-schema/"},{"title":"de PHP à Python : les tests unitaires","text":"Nouvel épisode de la série de PHP à Python . Cette fois ci nous allons nous pencher sur l'incontournable outil du dev voulant s'assurer la qualité de son code et de sa non régression : le test unitaire. ](http://blog.pagesd.info/public/ContactManagement/unit-testing.png) J'aborderai succinctement comment on s'y prend en PHP pour ensuite vous montrer l'équivalent Python et finirai par d'autres moyens dont dispose Python dans son arsenal. de PHP ... Alors d'avance désolé, mais je ne vais pas aborder Atoum , quand bien même cela ne me déplairait pas :) Le but étant de montrer comment en PHP on fait un test unitaire et retrouver une smilitude voire carrement des ressemblances avec Python. Atoum n'existant pas dans une version Pythonesque, ca sera donc PHPUnit puisque de ce coté là, un pendant Python existe ;) Donc PHPUnit est bien connu des développeurs PHP et est donc un framework de tests unitaires dérivé de JUnit (issue du monde Java) Voici donc un script test.php contenant ce qui suit : cal = new Calc(); } public function testMoyenne() { $this->assertEquals($this->cal->moyenne(array(1,2,3)),2); $this->assertEquals($this->cal->moyenne(array(2,4,6)),4); } public function testDivision() { $cal = new Calc(); $this->assertEquals($this->cal->division(10,5),2); } } ?> calculs.php quant à lui contient : J'ai volontairement éluder la question de la division par zéro, on va le voir plus tard. le resultat du test donne ceci : phpunit test.php PHPUnit 3.4.14 by Sebastian Bergmann. .. Time: 0 seconds, Memory: 6.50Mb OK (2 tests, 3 assertions) Et \"voilà\" :) .... à Python partie 1 : unittest Coté du fameux reptile, il s'agit de \" unittest \" et est également un framework de tests unitaires, dérivé du même JUnit - donc on peut déjà se frotter les mains, l'acquis sur PHPUnit pour passer à unittest devrait être le plus smoothy possible :) Petite définition en passant d'unittest, toute droit sortie de la doc : The Python unit testing framework, sometimes referred to as \"PyUnit,\" is a Python language version of JUnit, by Kent Beck and Erich Gamma. JUnit is, in turn, a Java version of Kent's Smalltalk testing framework. Each is the de facto standard unit testing framework for its respective language. voici test.py import unittest from calculs import moyenne from calculs import division class CalculsTest ( unittest . TestCase ): def test_moyenne ( self ): self . assertEquals ( moyenne ( 1 , 2 , 3 ), 2 ) self . assertEquals ( moyenne ( 2 , 4 , 6 ), 4 ) def test_division ( self ): self . assertEquals ( division ( 10 , 5 ), 2 ) if __name__ == '__main__' : unittest . main () et calculs.py def moyenne ( * args ): length = len ( args ) sum = 0 for num in args : sum += num return float ( sum ) / float ( length ) def division ( a , b ): return a / b ce qui donne un fois lancé : python test.py .. ---------------------------------------------------------------------- Ran 2 tests in 0 .000s OK Quant à la division par Zéro volontairement omise voici en Python comment on la gère et la teste : [ ... ] class CalculsTest ( unittest . TestCase ): [ ... ] def test_division ( self ): self . assertEquals ( division ( 10 , 5 ), 2 ) self . assertRaises ( ZeroDivisionError , division , 10 , 0 ) Pour PHPUnit, on n'a semble-t-il toujours rien en magasin qui permette de tester quelle méthode lève bien l'exception attendue, alors qu' Atoum oui ;) une note finale, sur cette partie, pour rendre à césar le code Python ici est entièrement extrait du livre de Tarek Ziadé \"Python, Petit guide à l'usage du développeur agile\" Partie 2 : doctest A présent que nous avons vu les TU unittest , Python possède une seconde méthode de tests : doctest . Comment fonctionne-t-elle ? Le module doctest, au lancement de la commande python, va scruter votre code source à la recherche de texte ressemblant à un session python et une fois trouvé, exécutera justement la session python. Exemple : mynameis.py \"\"\" >>> my_name_is(\"foxmask\") 'foxmask' \"\"\" def my_name_is ( string ): return string if __name__ == '__main__' : import doctest doctest . testmod () pour tester on fera : $ python mynameis.py $ rien s'affiche, donc pas d'erreur - bon voyons le mode verbose quand même puisqu'on est curieux : python mynameis.py -v Trying: my_name_is ( \"foxmask\" ) Expecting: 'foxmask' ok 1 items had no tests: __main__.my_name_is 1 items passed all tests: 1 tests in __main__ 1 tests in 2 items. 1 passed and 0 failed. Test passed. Vous pourriez vous demander pourquoi faire 2 outils pour la même chose finalement ? Doctest permet également de produire de la doc et les docstring comme on les appelle permettent cela tout autant que de tester la non régression de votre code qui aurait pu varié. D'aucuns diront (l'un d'eux se reconnaitra ;) que l'on n'a pas écrit le code du test avant le code lui-même, mais l'inverse, et que donc doctest vérifie le résultat de son exécution. Ce n'est pas faux mais ça fait quand même son boulot ;) Il existe enfin une dernière méthode dite DDD : le Développement Dirigé par la Documentation. Je ne l'ai pas encore abordée et ne pourrait vous en dire plus en détails, juste qu'elle existe. nota : les sources de l'article sont sur github ;)","tags":"Techno","url":"https://foxmask.net/post/2013/02/12/de-php-a-python-les-tests-unitaires/","loc":"https://foxmask.net/post/2013/02/12/de-php-a-python-les-tests-unitaires/"},{"title":"HaveFnuBB sous le domaine exactement","text":"Amis lecteurs, vous savez que j'ai produit HaveFnuBB, ce projet de forum opensource en PHP5 avec Jelix , démarré en 2008, je vous en avais même parlé dans un billet de retour aux sources ;) Alors nan le projet n'est pas mort, loin s'en faut, il dort un peu parce que je consacre mon temps à d'autres occupations. Cependant devant le trop grand nombre de posts ouverts sur le site du projet, j'en ai déduit par ce dernier, qu'il est si stable et fiable que personne n'a besoin de support à son sujet, et c'est tant mieux (héhé). Donc par conséquent, j'ai décidé de déménager le projet sur le sous domaine http://havefnubb.foxmask.info . Tout s'y trouve à l'identique, site, forum, doc, fichiers de téléchargement. Et comme d'hab' si vous avez envie de contribuer, le projet havefnubb sur github se trouve. Viendez tous forker ensemble ;)","tags":"Techno","url":"https://foxmask.net/post/2013/02/07/havefnubb-sous-le-domaine-exactement/","loc":"https://foxmask.net/post/2013/02/07/havefnubb-sous-le-domaine-exactement/"},{"title":"de PHP à Python : Gangnam Style","text":"\"Gangnam Style\" kézako : Bon en fait j'étais parti vous parler sérieusement avec un titre de PSR-1 à PEP8 mais ca manquait de fun ;-) Donc le sujet du jour de la série \" de PHP à Python \", comme le laisse supposer (ceux qui connaissent) ces 2 'codes' PSR-1 & PEP8 : les règles de coding dans chacun des 2 langages. Je me suis dit qu'après tout, on ne pouvait pas aborder le passage à python sans en passer par les règles de \"rédaction\", donc voici. de PHP ... Pour PHP la définition de la PSR-1 est toute récente, et comme le rappelle mageekguy , celle ci est somme toute très subjective car cela n'empêchera personne de coder comme il a toujours fait, et cette règle peut déranger les habitudes de tout à chacun (moi le premier :) exemple de code : Pourquoi diable pour les fonctions les accolades sont à la ligne suivante et pas sur celle du if ? Perso, je les ai toujours TOUTES mises sur la même ligne, une habitude prise avec PERL :) ... à Python La PEP8 quant à elle n'a que 12 ans ... Les accolades n'existent pas on met des \":\" sur la même ligne que l'instruction et on indent de 4 espaces. Voici à quoi cela ressemble et on en discute après : #Use 4 spaces per indentation level. foo = long_function_name(var_one, var_two, var_three, var_four) def long_function_name(var_one, var_two, var_three, var_four): print(var_one) on remarquera qu'en php on défini une fonction par le mot clé \"function\", ici on utilisera \"def\" Le Python n'étant pas rigide en tout point, parfois \"la queue\" (ligne de code) est un peu longue donc on la \"retrousse\" comme suit : # Aligned with opening delimiter foo = long_function_name ( var_one , var_two , var_three , var_four ) # More indentation included to distinguish this from the rest. def long_function_name ( var_one , var_two , var_three , var_four ): print ( var_one ) C'est tout aussi lisible et l'indentation fait son office Alors certes cela peut perturber le développeur PHP au début, mais si comme moi vous aimez que ca soit au carré vous vous y ferez super vite. La PEP8 définie également le nombre de lignes blanches entre 2 \"def\" ou la taille maximum de caractères par ligne. Une autre règle importante concerne les imports. En PHP on n'ira pas écrire use Customer, Order mais bien use Customer use Order En Python on n'écrira pas non plus import customer , order mais bien 2 imports distincts import customer import order on retiendra que si la PEP8 n'est pas respectée, en exécutant votre script python vous aurez droit à une belle erreur ;) PHP a coté est largement plus laxiste ! Par exemple j'avais pris pour habitude d(e volontairement) écrire ce genre de tru,c pour bien le repérer visuellement plus tard, et le retirer quand j'avais fini de débogger : function bagnole_spec($voiture) { var_dump($voiture); if ( in_array($voiture,'nb_portes')) { echo $voiture['nb_portes']; } } En python, aucun atermoiement n'est possible, on ne peut pas faire du code crade même temporairement, ça doit pas dépasser. L'ordre de facto on s'y fait, après tout le deboggage a sa place, autant ne pas le négliger :) Conclusion : Bon évidement la PSR-1 ne se contente pas que de cela de même que la PEP8 ne se borne pas à ces limitations/impositions de style. Se conformer aux règles a du bon dès le départ, bon sauf quand ça fait plus de 10 piges qu'on crache du code et qu'on vous dit qu'il faut changer ;) De toute façon avec des IDE modernes vous aurez tout loisir que ce dernier vérifie que votre code colle aux exigences du langage. Par exemple avec Python et Aptana vous pouvez ajouter les lib qui permettent à l'IDE de valider que votre code n'est pas écrit de travers, l'une d'elle est pylint , ça donne cela : Comme on voit là sur la ligne return render , j'ai bien le droit de me faire plaisir par quelques retours à la ligne alignés sur la parenthèse ouvrante, quant à la ligne logger.debug on la voit beaucoup trop longue. Là python ne poussera pas le bouchon à nous invalider le code pour \"si peu\". Pour en revenir à \"Gangnam Style\": avec les accolades à tirelarigot on a vraiment l'impression d'avoir une posture de 'code' sur un canasson (quand on code pas comme un bourrin ;) et coté Python, d'aucuns diront qu'ils sont des poneys zailés ;)","tags":"Techno","url":"https://foxmask.net/post/2013/02/04/gangam-style/","loc":"https://foxmask.net/post/2013/02/04/gangam-style/"},{"title":"de PHP à Python : de composer à setuptools","text":"Comme introduit dans ce billet , voici donc le premier billet de la série de PHP à Python . Gérer des dépendances à son projet ? il y a aussi une application pour cela ! de PHP ... Cette application se nomme Composer Voici un extrait de la doc de Composer qui illustrera l'analogie qui suit pour la version python Le fichier composer.json est disponible en principe sur chacun des dépôts des projets qu'on peut trouver sur github et packagist. C'est le fichier sur lequel tout repose. composer.json : { \"repositories\": [ { \"type\": \"package\", \"package\": { \"name\": \"smarty/smarty\", \"version\": \"3.1.7\", \"dist\": { \"url\": \"http://www.smarty.net/files/Smarty-3.1.7.zip\", \"type\": \"zip\" }, \"source\": { \"url\": \"http://smarty-php.googlecode.com/svn/\", \"type\": \"svn\", \"reference\": \"tags/Smarty_3_1_7/distribution/\" }, \"autoload\": { \"classmap\": [\"libs/\"] } } } ], \"require\": { \"smarty/smarty\": \"3.1.*\" } } ici on comprendra qu'on s'est rajouté une dépendance sur smarty qui n'est pas sur packagist mais sur un repository \"privé\". ... à Python Cette application se nomme Setuptools L'homologue python sur lequel tout repose est le script setup.py dont voici un exemple : from setuptools import setup , find_packages setup ( name = 'fox_php_lib' , version = '0.1' , description = 'Fox dessine un mouton' , author = 'foxmask' , author_email = 'devnull@foxmask.info' , url = 'https://github.com/foxmask/fox_php_lib' , download_url = 'https://github.com/downloads/foxmask/fox_php_lib/fox_php_lib0.1.tar.gz' , packages = find_packages (), classifiers = [ 'Development Status :: 4 - Beta' , 'Environment :: Web Environment' , 'Intended Audience :: Developers' , 'License :: OSI Approved :: BSD License' , 'Operating System :: OS Independent' , 'Programming Language :: Python' , 'Framework :: Django' , 'Topic :: Software Development :: Libraries :: Python Modules' , 'Topic :: Utilities' ] ) Ce setup permet (quasiment comme son homologue) : De décrire un module, une lib De télécharger l'archive, décrite au paramètre download_url , via une commande particulière De télécharger les dépendances (lors de l'installation), décrite au paramètre packages via une fonction find_packages() qui parcourira mon application et identifiera les dépendances et les installera les meta données name , version , description , author , author_email , classifiers , sont assez explicites pour savoir ce qu'on y met ;) De définir des classifiers utiles pour retrouver et \"ranger\" ses contributions sur Pypi (cf plus bas) Comme on le voit sur la première ligne du script on a une ligne de code python, plus tard on a la méthode find_packages(), donc oui on a toute latitude d'utiliser du python pour \"peupler\" ses variables. A présent comment s'en servir ? on tape simplement : python setup.py install qui installera votre lib/module/app comme attendue pour composer, on taperait : php composer.phar install w00t :) Une fois installée, on peut vérifier que la lib est dispo en tapant : foxmask @foxmask : ~ $ python Python 2.6 . 6 ( r266 : 84292 , Dec 26 2010 , 22 : 31 : 48 ) [ GCC 4.4 . 5 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> from fox_php_lib import python >>> python () [ 2013 - 01 - 14 22 : 26 : 49.669036 ] Python ca rox ce que fait cette lib est juste ce qui suit ( test.py ): #!/usr/bin/env python # -*- coding: utf-8 -*- from datetime import datetime def python (): print \"[ %s ] Python ca rox\" % datetime . now () if __name__ == \"__main__\" : python () Dans la foulée de la création de notre setup.py, on peut très allégrement publier sa lib / son appli sur Python Package Index aka \"Pypi\" , mais cela pourra faire partie d'un autre billet sur son utilisation. presque le mot de la fin une dernière information quant à la publication de ce billet qui est très largement inspiré du très bon article de sam & max : Créer un setup et mettre sa lib python en ligne un dernier mot n'étant pas un utilisateur (même pas averti:P) de Composer, je me suis basé sur la doc et l'aide précieuse d'Artemiz pour son support à sa compréhension ;) nota : les sources de l'article sont disponibles sur github","tags":"Techno","url":"https://foxmask.net/post/2013/01/28/de-php-a-python-de-composer-a-setuptools/","loc":"https://foxmask.net/post/2013/01/28/de-php-a-python-de-composer-a-setuptools/"},{"title":"Django __unicode__ et form.as_p mes amis","text":"Toujours dans mes périgrinations de débutant, voici un coup qui m'a pris du temps à trouver seul, donc si ça peut servir à d'autres ;) Donc voici, Chapitre Un __unicode__ je veux produire une page pour afficher un formulaire composé de 2 listes déroulantes issues d'un autre modèle + un champ de texte j'ai composé le modèle suivant : models.py class TriggerType ( models . Model ): \"\"\" TriggerType \"\"\" code = models . CharField ( max_length = 80 ) name = models . CharField ( max_length = 140 ) class TriggerService ( models . Model ): \"\"\" TriggerService \"\"\" provider = models . ForeignKey ( TriggerType , related_name = '+' , blank = True ) consummer = models . ForeignKey ( TriggerType , related_name = '+' , blank = True ) description = models . CharField ( max_length = 200 ) user = models . ForeignKey ( User ) date_created = models . DateField () et voici mon formulaire forms.py class TriggerServiceForm ( forms . ModelForm ): \"\"\" Trigger Form \"\"\" class Meta : \"\"\" meta to add/override anything we need \"\"\" model = TriggerService widgets = { 'description' : TextInput ( attrs = { 'placeholder' : _ ( 'A description for your new service' )}), } provider = forms . ModelChoiceField ( queryset = TriggerType . objects . all ()) consummer = forms . ModelChoiceField ( queryset = TriggerType . objects . all ()) Voilà ! A présent voici ce qui se passait quand j'affichais mon formulaire : Dans mes 2 listes déroulantes j'obtenais \"TriggerType object\" en quise de valeur dans ces 2 listes. Comme j'ai eu du mal à trouver la solution finale, dans un premier temps je me suis résigné à faire un tuple dans mon form : TRIGGER_TYPE = [( data . code , data . name ) for data in TriggerType . objects . all ()] provider = forms . ChoiceField ( label = _ ( 'Provider' ), widget = forms . Select , choices = TRIGGER_TYPE , help_text = _ ( 'Select the service from which you want to grad your datas' )) consummer = forms . ChoiceField ( label = _ ( 'Consummer' ), widget = forms . Select , choices = TRIGGER_TYPE , help_text = _ ( 'Select the service to which you want to put your datas' )) ca marchait très bien mais comme je suis borné et avait repéré ModelChoiceField je me suis dit \"nan mais ho ModelChoiceField prend bien en argument le nom d'un modèle alors CA DOIT le faire!\" En retournant donc sur la doc j'entrevois la lumière : The __unicode__ method of the model will be called to generate string representations of the objects for use in the field's choices; to provide customized representations, Non de ieuD ! c'est aussi con que ça ? je retourne à mon modèle : models.py class TriggerType ( models . Model ): \"\"\" TriggerType \"\"\" code = models . CharField ( max_length = 80 ) name = models . CharField ( max_length = 140 ) def __unicode__ ( self ): \"\"\" required to build the drop down list otherwise will dislpay \"\"\" return \" %s \" % ( self . name ) et là de retour dans ma page web, Alélouya tout est tout propre. Content de moi, cette fois ci je pars pour saisir mes données et les enregistrer. Chapitre Deux form.as_p Confiant je ponds un formulaire à la main avec la vue qui va bien et avec le modèle ci dessus TriggerService et mon form TriggerServiceForm add_service.html { % trans 'Creation of a new service' % } { % csrf_token % } { % trans 'Provider' % } {{ form . provider }} { % trans 'Consummer' % } {{ form . consummer }} { % trans 'Description' % } {{ form . description }} { % trans \"Create it\" % } et voici ma vue views.py @login_required def save_service ( request ): \"\"\" save a service \"\"\" if request . method == 'POST' : # If the form has been submitted... service = TriggerService ( user_id = request . user . id ) form = TriggerServiceForm ( request . POST , instance = service ) if form . is_valid (): # All validation rules pass form . save () return HttpResponseRedirect ( '/trigger_added/' ) else : # unbound form (if any) form = TriggerServiceForm () # redirect to home of the existing enabled services return redirect ( 'home' ) Et là quand je soumets mon formulaire je repars d'où je viens, la page d'accueil (ici \"home\") sans que rien n'ai été ajouté dans ma base. Parti la fleur au fusil j'ai carrément été trop vite. Oui et le lecteur attentif dira \"ben oui tu gères pas le cas où le formulaire n'est pas valide du coup on n'a pas les raisons du pourquoi ca passe pas\" Exact ! So Let's go : @login_required def save_service ( request ): \"\"\" save a service \"\"\" if request . method == 'POST' : # If the form has been submitted... service = TriggerService ( user_id = request . user . id ) form = TriggerServiceForm ( request . POST , instance = service ) # 1) valid the form if form . is_valid (): # All validation rules pass form . save () # 2) redirect user return HttpResponseRedirect ( '/trigger_added/' ) # 3) if not valid else : template_name = 'add_service.html' # 4) keep the data put in the form context = { 'form' : form } context . update ( csrf ( request )) # 5) go back to the form and display the values + errors return render_to_response ( template_name , context , context_instance = RequestContext ( request )) # attempt to acces to save_service by another method than POST else : # unbound form (if any) form = TriggerServiceForm () # redirect to home of the existing enabled services return redirect ( 'home' ) Là, ma vue m'a l'air cool mais encore une fois je me retrouve sur le formulaire de saisie à tourner en boucle sans avoir les erreurs ... Après un peu de creusage de neuronne (oui j'en ai qu'un, surtout à cette heure là :P) je farfouille dans les sources de django-registration :D et retombe sur la propriété errors de chacun des champs. Je modifie tout ca et fini par produire le template suivant : add_service.html { % trans 'Creation of a new service' % } { % csrf_token % } {{ form . non_field_errors }} { % trans 'Provider' % } {{ form . provider . errors }} {{ form . provider }} { % trans 'Consummer' % } {{ form . consummer . errors }} {{ form . consummer }} { % trans 'Description' % } {{ form . description . errors }} {{ form . description }} { % trans \"Create it\" % } Là tranquillement j'enregistre mon formulaire confiant (oui toujours ;) mais déchante aussitôt ! Je boucle toujours !!! *Groumpf* Je retourne à la doc et je retrouve une balise form.as_p qui permet d'afficher son formulaire sans rien faire d'autre ;) je m'empresse de l'ajouter après {% csrf_token %} et recharge ma page et là Ô surprise, je m'aperçois que j'ai oublié tout bêtement 2 éléments de mon modèle qui sont à présent dans mon formulaire qui sont : user et date_create ce qui fait que mon form.is_valid() ne passait pas une seule fois. Donc j'ajoute à ma classe Meta de ma classe TriggerServiceForm forms.py exclude = ( 'user' , 'date_created' ) et une option à mon modèle pour ajouter une date automatiquement à chaque enregistrement des données du modèle models.py class TriggerService ( models . Model ): \"\"\" TriggerService \"\"\" ... date_created = models . DateField ( auto_now_add = True ) et hop hop hop tout roule comme sur des roulettes. C'est pas trop tôt !","tags":"Techno","url":"https://foxmask.net/post/2013/01/21/django-__unicode__-et-form_as_p-mes-amis/","loc":"https://foxmask.net/post/2013/01/21/django-__unicode__-et-form_as_p-mes-amis/"},{"title":"Oracle Identifications des Locks, Process, Sessions en cours","text":"Voici une requete SQL dans l'arsenal du DBA, nommons la locks.sql , permettant d'identifier les locks en cours sur un schema oracle : SELECT /*+ choose */ bs . username \"Blocking User\" , bs . username \"DB User\" , ws . username \"Waiting User\" , bs . sid \"SID\" , ws . sid \"WSID\" , bs . sql_address \"address\" , bs . sql_hash_value \"Sql hash\" , bs . program \"Blocking App\" , ws . program \"Waiting App\" , bs . machine \"Blocking Machine\" , ws . machine \"Waiting Machine\" , bs . osuser \"Blocking OS User\" , ws . osuser \"Waiting OS User\" , bs . serial # \"Serial#\" , DECODE ( wk . TYPE , 'MR' , 'Media Recovery' , 'RT' , 'Redo Thread' , 'UN' , 'USER Name' , 'TX' , 'Transaction' , 'TM' , 'DML' , 'UL' , 'PL/SQL USER LOCK' , 'DX' , 'Distributed Xaction' , 'CF' , 'Control FILE' , 'IS' , 'Instance State' , 'FS' , 'FILE SET' , 'IR' , 'Instance Recovery' , 'ST' , 'Disk SPACE Transaction' , 'TS' , 'Temp Segment' , 'IV' , 'Library Cache Invalidation' , 'LS' , 'LOG START OR Switch' , 'RW' , 'ROW Wait' , 'SQ' , 'Sequence Number' , 'TE' , 'Extend TABLE' , 'TT' , 'Temp TABLE' , wk . TYPE ) lock_type , DECODE ( hk . lmode , 0 , 'None' , 1 , 'NULL' , 2 , 'ROW-S (SS)' , 3 , 'ROW-X (SX)' , 4 , 'SHARE' , 5 , 'S/ROW-X (SSX)' , 6 , 'EXCLUSIVE' , TO_CHAR ( hk . lmode ) ) mode_held , DECODE ( wk . request , 0 , 'None' , 1 , 'NULL' , 2 , 'ROW-S (SS)' , 3 , 'ROW-X (SX)' , 4 , 'SHARE' , 5 , 'S/ROW-X (SSX)' , 6 , 'EXCLUSIVE' , TO_CHAR ( wk . request ) ) mode_requested , object_name , TO_CHAR ( hk . id1 ) lock_id1 , TO_CHAR ( hk . id2 ) lock_id2 FROM v$lock hk , v$session bs , v$lock wk , v$session ws , V$LOCKED_OBJECT a , dba_objects b WHERE hk . BLOCK = 1 AND hk . lmode != 0 AND hk . lmode != 1 AND wk . request != 0 AND wk . TYPE ( + ) = hk . TYPE AND wk . id1 ( + ) = hk . id1 AND wk . id2 ( + ) = hk . id2 AND hk . sid = bs . sid ( + ) AND wk . sid = ws . sid ( + ) AND a . object_id = b . object_id AND HK . sid = a . SESSION_ID ORDER BY 1 ; Oui hein ! ça s'invente pas ;) Une seconde current_request.sql vous donnant les requêtes en cours : Avec Oracle 9i : SELECT sesion . sid , sesion . username , optimizer_mode , hash_value , address , cpu_time , elapsed_time , sql_text FROM v$sqlarea sqlarea , v$session sesion WHERE sesion . sql_hash_value = sqlarea . hash_value AND sesion . sql_address = sqlarea . address AND sesion . username IS NOT NULL Mais ca c'était avant Ci dessous la même en \"encore plus longue\" pour les versions oracle > 9i SELECT sql_text , STATUS FROM v$session , v$sqlarea WHERE v$session . sql_id = v$sqlarea . sql_id ; Ah oui ça tranche hein ! Pour avoir la liste des sessions en cours un petit script session.sql : SET echo off ; SET termout ON ; SET linesize 180 ; SET pagesize 60 ; SET newpage 0 ; SELECT rpad ( c . name || ':' , 11 ) || rpad ( ' current logons=' || ( to_number ( b . sessions_current )), 20 ) || 'cumulative logons=' || rpad ( substr ( a . VALUE , 1 , 10 ), 10 ) || 'highwater mark=' || b . sessions_highwater Information FROM v$sysstat a , v$license b , v$database c WHERE a . name = 'logons cumulative' ; ttitle \"dbname Database|UNIX/Oracle Sessions\" ; SET heading off ; SELECT 'Sessions on database ' || substr ( name , 1 , 8 ) FROM v$database ; SET heading ON ; SELECT substr ( a . spid , 1 , 15 ) pid , substr ( b . sid , 1 , 15 ) sid , substr ( b . serial # , 1 , 15 ) ser # , substr ( b . machine , 1 , 16 ) box , substr ( b . username , 1 , 50 ) username , -- b.server, substr ( b . osuser , 1 , 28 ) os_user , substr ( b . program , 1 , 60 ) program FROM v$session b , v$process a WHERE b . paddr = a . addr AND TYPE = 'USER' ORDER BY spid ; ttitle off ; spool off ; Pour obtenir les requêtes en cours d'annulation (rollback), session_undo.sql : COLUMN username FORMAT A15 SELECT s . username , s . sid , s . serial # , t . used_ublk , t . used_urec , rs . segment_name , r . rssize , r . STATUS FROM v$transaction t , v$session s , v$rollstat r , dba_rollback_segs rs WHERE s . saddr = t . ses_addr AND t . xidusn = r . usn AND rs . segment_id = t . xidusn ORDER BY t . used_ublk DESC ; Voilà, ça sera tout pour aujourd'hui","tags":"Techno","url":"https://foxmask.net/post/2013/01/17/oracle-identifications-des-locks-process-sessions-en-cours/","loc":"https://foxmask.net/post/2013/01/17/oracle-identifications-des-locks-process-sessions-en-cours/"},{"title":"Oracle des expdp et impdp en réseau, si c'est possible","text":"A l'heure actuelle, la version d'Oracle est la 11G R2 (à tes souhaits pourrait rétorqué 6P0 à R2D2 :P), et avec cette version il est de coutume de ne plus employer pour ses sauvegardes/gestions de dumps, que le fameux duo expdp et impdp pour produire ce qu'on appelle des datapumps (à tes souhaits encore ;) Tout ceci rend la gestion des dumps beaucoup plus funs qu'avec les commandes exp/imp qu'on se trainaient depuis Oracle 8i&9i. Par contre un inconvénient majeur pour tout radin qui se respecte, ca bouffe l'espace disque ! Avant expdp / impdp avec \"exp\", on pouvait compresser ses dumps like that : mknod pipe p gzip < pipe > FOX_DUMP.gz & nohup exp LOGIN/PASS file = pipe grants = n & j'exporte mon dump dans le pipe qui le compresse à la volée, magie magie ;) Qui dit mieux ? ben pas oracle 11G :P je vous note la même chose à l'envers pour un import avec imp mknod pipe p gunzip < FOX_DUMP.gz > pipe & nohup imp LOGIN/PASS file = pipe grants = n ignore = y fromuser = OLDLOGIN touser = LOGIN & Bon ok on a pige la compression / décompression à la volée. A présent supposons que j'ai vraiment des soucis de place disque, même pour produire le dump compressé localement à mon serveur oracle. On procédait ainsi : depuis une autre machine disposant des outils oracle (il faut un minimum hein;) mknod pipe p gunzip < FOX_DUMP.gz > pipe & nohup imp LOGIN/PASS@FOX-SERV file = pipe grants = n ignore = y fromuser = OLDLOGIN touser = LOGIN & Ici la subtilité réside dans ce qui suit le @ ; ceci demande à oracle \"vas voir dans le fichier tnsnames.ora et trouves moi la référence FOX-SERV, puis quand tu l'as trouvé connectes-toi au serveur concerné\". Ceci marche aussi bien localement (pour invoquer une instance Oracle local, l'ORACLE_SID quoi) qu'en réseau car le tnsnames.ora contient tout ce qu'il faut pour trouver le serveur, le port d'écoute et l'instance exemple : FOX - SERVER = ( DESCRIPTION = ( ADDRESS_LIST = ( ADDRESS = ( PROTOCOL = TCP )( HOST = fox . somewhere . other . the . rainbow )( PORT = 1523 )) ) ( CONNECT_DATA = ( SID = V11UTF8 ) ) ) Ainsi, il m'etait facile de balancer un dump à trifouilli-les-oies avec exp :P Depuis expdp et impdp Ok, super on a vu comment on gérait avant , à présent comment fait-on ? Depuis l'arrivée du duo (ex|im)pdp, on ne peut plus : compresser les dumps à la volée exploiter le dernier cas précédemment évoqué, car la commande prend des arguments qui demandent à Oracle de vérifier \"localement\" ses ressources. bon ya aussi plein d'autres trucs qu'on peut plus faire mais OSEF :P c'est pas le sujet du jour ;) Le premier des paramètres oracle \"vérifié\" localement : DIRECTORY DIRECTORY est un objet oracle et pas un path qu'on peut indiquer à oracle à la volée pour lui dire \"mon dump est là / est à mettre là\", nan nan ça ça marche plus ! Le directory oracle est donc localisable comme suit : SQL > conn / AS sysdba Connected . SQL > select * from dba_directories ; OWNER DIRECTORY_NAME ------------------------------ ------------------------------ DIRECTORY_PATH -------------------------------------------------------------------------------- SYS ORACLE_OCM_CONFIG_DIR / app / oracle / product / 11 . 2 . 0 . 3 . 0 / db / ccr / state SYS DATA_PUMP_DIR / app / oracle / product / 11 . 2 . 0 . 3 . 0 / db / rdbms / log / Donc comme on le voit ici, ce sont des dossiers propre à Oracle créés lors de l'installation. Mais moi je veux importer / exporter des dumps sur un NAS (allez :P) Donc pour arriver à mes fins, j'ajoute une ligne dans mon /etc/fstab pointant sur un des dossiers (qu'on dit \"exportés\") de mon NAS en NFS, disons que le dossier sera dans /mnt/dump_oracle et je le \"mount\" (of course) Reste alors à ajouter ce dossier à oracle : SQL > conn / AS sysdba Connected . SQL > create directory fox_dump_oracle as '/mnt/dump_oracle' ; Directory created . SQL > A présent reste à finir ses affaires : expdp LOGIN/PASS dumpfile = exp_FOX_SCHEMA.dmp directory = fox_dump_oracle logfile = fox_dump_oracle.log & et là au joie, le dump partira clopin-clopant sur le NAS bouffer l'espace disque ailleurs ;)","tags":"Techno","url":"https://foxmask.net/post/2013/01/16/oracle-des-expdp-et-impdp-en-reseau-si-cest-possible/","loc":"https://foxmask.net/post/2013/01/16/oracle-des-expdp-et-impdp-en-reseau-si-cest-possible/"},{"title":"de PHP à Python : tous ensemble","text":"Voici (venu le temps des rires et des chants dans l'ile aux enfants du web c'est tous les jours le printemps, aheu 'scusez j'm'égare :) une nouvelle série d'articles \"de PHP à Python\" qui voit le jour. Elle n'aura rien à voir avec une certaine émission de construction de baraques en ruine pour familles en détresse... :P Celle ci va consister à illustrer et (tenter de) répondre à la question \"Comment fais tu ça en Python quand je fais ça en PHP ?\" Encore une fois, ma grandeur d'âme aura eu raison de moi (ça sera ma \"tournée\" pour mon cadeau d'anniversaire à chacun de vous en ce jour béni des dieux du net ;) et c'est un nouveau défi sur IRC (sur un channel PHP), qui est le résultat de ce billet et de ceux qui suivront. En voici les \"termes\" ;) dis moi, toi qui touche à python depuis un bon moment, ça te dis pas de nous pondre un petit article comparatif avec PHP ? genre les équivalents en python dans les grandes lignes une sorte de how to switch to python genre l'équivalent de composer, de phpunit, etc... niveau code, par exemple l'équivalent de pdo, de simpleXml, etc... donne nous envie de switcher ;) On retiendra tout de même une judicieuse remarque sur #python-fr à ma question \"connaissez-vous des articles pour montrer comment passer de php à python ?\" PHP et Python n'ont pas exactement la même utilité, donc à part dans le cadre d'un projet précis, je vois mal comment se dire \"tiens, je vais passer de PHP à Python\" \\&#94;\\&#94; Le niveau des billets risque de ne pas être pour le débutant en programmation (forcément ;) mais si d'aventure vous le seriez en Python vous pourriez me corriger ;) Je me lance bien que n'étant pas le moins du monde l'expert attitré dans ces 2 langages, je pratique python depuis août 2012 et plus assidûment depuis PyConFr 2012 (niveau noob++), et pour PHP je pratique depuis un tout petit poil plus longtemps (niveau dev++/l33tmoinsmoins ;) mais comme j'apprécie les 2 ( et ces temps ci plus l'un que l'autre parce que je m'éclate ) je prends le risque et relève le gant ;) Donc le sujet pour cette série sera de montrer comment telle \"opération\" en PHP se réalise en Python. Dans la bannette pour le moment il y aura : \"En Python comment fais tu \" : avec un Composer like ? PSR-1 like (update 28/1) Les String (update 29/1) les Tests unitaires ? le Passage de paramètres au nombre variable ? les Arrays ( demandé via twitter ) oui c'est cours (en fait c'était cours au départ mais depuis 1mois il s'en est passé des choses) mais... ... je vous propose de me coller en commentaire les sujets qu'ils vous plairaient de voir abordés et je verrai \"si je sais faire\", de même, je modifierai la liste ultérieurement en fonction de mes envies ;) et surtout pour maintenir le liens avec les articles publiés après coup. Déjà paru en \"kioske\" : \"de Composer à SetupTools\" PSR-1 like Les String Shrek passe à table les Tests Unitaires A paraître : Lecture de fichier de configuration Parser du XML","tags":"Techno","url":"https://foxmask.net/post/2013/01/14/de-php-a-python-tous-ensemble/","loc":"https://foxmask.net/post/2013/01/14/de-php-a-python-tous-ensemble/"},{"title":"Oracle Gestion des Tablespaces","text":"Quand on n'a pas demandé à Oracle de gérer l'espace disque avec l'option auto extend sur ses tablespaces, il arrive (souvant) qu'on se mange des erreurs parce que le tablespace est full. Donc ; Pour lister ses tablespace on tapera : SELECT file_name FROM dba_data_files WHERE tablespace_name = 'MONBEAUTBSPC' ; affichera FILE_NAME -------------------------------------------------------------------------------- / u02 / oradata / INSTANCE / data01 . dbf / u02 / oradata / INSTANCE / data02 . dbf ... Pour le tablespace temporaire on fait : SELECT tablespace_name , file_name , bytes FROM dba_temp_files WHERE tablespace_name = 'TEMP' ; Trouver la taille restant dans le tablespace SELECT SUM ( bytes ) / 1024 / 1024 AS Mo FROM dba_free_space WHERE tablespace_name = 'MONBEAUTBSPC' ; MO ---------- 5205 . 625 Si on constate que la taille n'est pas suffisante, pour les agrandir on fera, pour le tablespace MONBEAUTBSPC : alter tablespace MONBEAUTBSPC add datafile '/u02/oradata/INSTANCE/data03.dbf' size 30720 M ; pour le tablespace TEMP : ALTER TABLESPACE temp ADD TEMPFILE '/u02/oradata/INSTANCE/temp02.dbf' SIZE 512 m ;","tags":"Techno","url":"https://foxmask.net/post/2013/01/10/oracle-gestion-des-tablespace/","loc":"https://foxmask.net/post/2013/01/10/oracle-gestion-des-tablespace/"},{"title":"Sam et Max Le Papa Noël descendu du ciel","text":"A la suite d'un concours improvisé par Sam et Max , me voici l'heureux gagnant d'un joli cadeau, je vous laisse découvrir en images le paquet ;) ici le fond du colis : Les enfants ont déjà fait la razia ;) Ici on notera la touche \"perso\" de Sam et Max (c'est scotché sur la boiboite :) Les petits points blancs (si vous zoomez) ; c'est le sucre des bonbons secoués, dans l'emballage ! Etui, tapis et notice et la molette elle même est retro éclairée, du plus bel effet : Alors, \"petit bonhomme au chapeau pointu\" : un très grand merci ! j'allais en changer, mais Razer, c'était pas au budget ! Je pourrai tester cette fusée sur une partie de CODMW3 :)","tags":"General","url":"https://foxmask.net/post/2013/01/08/sam-max-le-papa-noel-descendu-du-ciel/","loc":"https://foxmask.net/post/2013/01/08/sam-max-le-papa-noel-descendu-du-ciel/"},{"title":"Oracle Exporter une Proc","text":"Le script suivant, nommons le export_pkg, vous permet d'exporter une proc de votre choix : SET heading off SET pagesize 0 SET linesize 255 SET trimspool ON SET feedback off SET verify off spool && 1 \\ . pkg SELECT 'CREATE OR REPLACE ' FROM dual ; SELECT TEXT FROM user_source WHERE TYPE = 'PACKAGE' AND NAME = '&&1' ORDER BY line ASC ; SELECT '/ ' FROM dual ; SELECT ' ' FROM dual ; SELECT 'CREATE OR REPLACE ' FROM dual ; SELECT TEXT FROM user_source WHERE TYPE = 'PACKAGE BODY' AND NAME = '&&1' ORDER BY line ASC ; SELECT '/ ' FROM dual ; SELECT 'exit; ' FROM dual ; spool off ; exit ; Il s'utilise ainsi : sqlplus login/pass @export_pgk.sql TOTO et vous créera TOTO.pkg avec le contenu de la proc contenu dans le schema \"login\"","tags":"Techno","url":"https://foxmask.net/post/2013/01/03/oracle-exporter-une-proc/","loc":"https://foxmask.net/post/2013/01/03/oracle-exporter-une-proc/"},{"title":"Oracle Changer l'éditeur SQL","text":"Maints fois on se retrouve à taper une connerie dans SQL*Plus mais on n'a pas la main sur l'histoirique comme on l'a avec votre shell BASH en pressant la touche flèché du haut. Donc pour tout de même se retrouver dans un \"univers\" familier on peut faire ceci define _editor=vi et en tapant la commande : SQL > ed on se retrouvera dans l'éditeur VI (dans un univers famillier :) et on pourra changer la dernière ligne qui nous posa problème ;)","tags":"Techno","url":"https://foxmask.net/post/2012/12/27/oracle-changer-lediteur-sql/","loc":"https://foxmask.net/post/2012/12/27/oracle-changer-lediteur-sql/"},{"title":"Jelix et un minify avant les fêtes","text":"Hey !!! Ca faisait une éternité que je ne m'étais pas arrêté sur Jelix et proposé un billet sur le sujet ;) Bon au menu de ce soir un truc très court mais utile si on ne veut pas se bourrer de dolipranes à en avance sur les fêtes ;) Donc voici : Minify Comme tout jelixien le sait, on peut \"minifier\" ses scripts CSS & JS. Tout se passe bien tant qu'on veut pas jouer au malin (comme moi ;p). J'installe une version 1.5-dev toute fraîche pour me farcir une évol' toute con mais avec maints ramifications . Du coup là après m'être farci jConfigCompiler, je suis sur jelix_minify.php et là, après avoir configuré jelix pour minifier mes CSS, je me mange un message d'erreur trop lourd pour une avant veille de réveillon : ERROR 400 BAD REQUEST Ma qué passa ? tout cassé la machina ? Je plonge dans le code de jelix_minify et ne vois rien qui cloche sur mon évol', donc commence à mettre dans un bateau firebug et firephp pour voir lequel des 2 sera le premier à ne pas tomber à l'eau. Manque de pot, ils se sont tous 2 noyés, j'ai dû me trouver la solution tout seul (comme d'hab' quand c'est trop compliqué, tout le monde se débine :P ). Alors !! T'accouche ? je vous vois trépigner \"mais qu'il est long pour la cracher sa valda\"... mais nan allez encore 2min de patience. Quand on se démarre un projet flambant neuf avec jelix, on obtient la belle page toute bleue ressemblant à celle ci . Mais avant d'être bleue, elle est toute blanche parce qu'on n'a pas ajouté dans son dossier www le path vers lib/jelix-www contenant les CSS de base de Jelix. Donc comme tout à chacun, devant ma page blanche (avec du texte mais sans couleur, s'entend) je me fais un pur lien symbolique : ln -s ../../lib/jelix-www jelix je recharge ma page et hop la voilà toute belle vêtue de bleu. Ensuite arrive ma config minify je recharge la page et hop ... toute blanche ... avec comme HTTP réponse l'indigeste 400... Comme on commence à l'entrevoir c'est mon lien symbolique qui le défrise l'ami Jelix_Minify. J'ai donc fini par sucrer le lien symbolique et le remplacer par une copie du dossier lui même. En rechargeant ma page ce coup ci tout était bleu (sans éléPHPant :D ) et sans plus l'erreur 400. Sur ce passez un bon réveillon à tous ;)","tags":"Techno","url":"https://foxmask.net/post/2012/12/21/jelix-et-un-minify-avant-les-fetes/","loc":"https://foxmask.net/post/2012/12/21/jelix-et-un-minify-avant-les-fetes/"},{"title":"Oracle Personnalisation du Prompt SQL*Plus","text":"Tout comme un admin système adore faire joujou en personnalisant son prompt de son shell chéri en rajoutant le nom du host et l'heure, on peut tout aussi bien le faire avec SQL*Plus afin de savoir sur quelle Instance Oracle on s'est connecté et avec quel Utilisateur et à quelle heure. Pour ce faire, on editera le fichier \\$ORACLE_HOME/sqlplus/admin/glogin.sql pour ajouter : set sqlprompt \"_user 'on' _date 'at' _connect_identifier > \" ce qui permettra d'afficher : - bash - 3 . 2 $ sqlplus / nolog SQL * Plus : Release 10 . 2 . 0 . 4 . 0 - Production on Tue Jan 19 14 : 42 : 05 2010 Copyright ( c ) 1982 , 2007 , Oracle . All Rights Reserved . on 19 - Jan - 11 at > conn / as sysdba Connected . SYS on 19 - Jan - 11 at INSTANCE >","tags":"Techno","url":"https://foxmask.net/post/2012/12/20/oracle-personnalisation-du-prompt-sqlplus/","loc":"https://foxmask.net/post/2012/12/20/oracle-personnalisation-du-prompt-sqlplus/"},{"title":"Quand Django reste muet à vous rendre dingue","text":"Voici un truc qui m'a usé les nerfs (de noob mais pas que :P) avec Django . je me fais un template principale, con à souhait, dans lequel je fais un include, \"QQ la praline\", comme suit : base.html { % load static % } { % load staticfiles % } { % load i18n % } { % load url from future % } [ ... ] { % include \"mark_forum.inc.html\" % } dans mark_forum.inc.html { % trans \"Mark all forum as read\" % } { % if current_id_forum > 0 % } { % trans 'Mark this forum as read' % } { % endif % } { % trans 'Show new posts' % } { % if current_id_forum > 0 % } { % if subscribed_to_this_forum % } { % trans 'Unsubscribe to this forum' % } { % else % } { % trans 'Subscribe to this forum' % } { % endif % } { % endif % } je lance mon appli et dans la console, me saute aux yeux lib/python2.6/site-packages/django/template/defaulttags.py:1235: DeprecationWarning: The syntax for the url template tag is changing. Load the ` url ` tag from the ` future ` tag library to start using the new behavior. donc j'ajoute à mark_forum.inc.html (malgré l'ajout existant dans l'index.html) { % load url from future % } et je relance ... et là plus d'erreur dans la console ! Yeah ! je me dis nickel ca marche, avancons ! ... ben non. Aucun message d'erreur et le template que je tente d'inclure affiche ... rien ! Donc je sors le scalpele et pars décortiquer tout le code du template. Je retire tout et colle un beau coucou à la place de tout le reste ; qui s'affiche. Bon déjà je me dis ok je suis pas débile c'est bien comme ça que l'inclusion marche (vin diou). Je rajoute mon premier noeud li et là re patatra plus rien ne s'affiche... Comme le problème de syntaxe sur url n'est plus en cause je me dis voyons trans et mettons une string pipo ~~alakon~~. Et là Bingo ! Il me manque donc { % load \"i18n\" % } J'avais imaginé que le template inclus récupérait le contexte parent et donc le load que j'avais fait dans le index.html.... que néni... comme pour le load url en fait. Mais là où j'ai mis le temps c'est, comme je l'ai dit plus tôt AUCUN message d'erreur me dit que je tente d'utiliser un \"template tag\" que je n'ai pas chargé avant ... Une bosse grosse engueulade comme pour le load url from future m'aurait largement aidé ;) Là franchement c'est très bas de la part de Django :P Si quelqu'un a une explication sur le mutisme de Django là dessus, je lui en serai gré ;)","tags":"Techno","url":"https://foxmask.net/post/2012/12/19/quand-django-reste-muet-a-vous-rendre-dingue/","loc":"https://foxmask.net/post/2012/12/19/quand-django-reste-muet-a-vous-rendre-dingue/"},{"title":"Debian, amour et désamour","text":"Utilisateur de Debian depuis Le toutou elastic aka \"Slink\" (version 2.1), je n'ai jamais eu à me plaindre. Je me suis toujours éclater avec cette distribution Linux, même si j'ai croisé sur le chemin SlackWare, Mandrake, RedHat, Suse etc... J'utilise actuellement Debian Squeeze (version 6 donc) que j'ai pu installer sur mon HP en passant par ... windows... parce que le BIOS des HP est inaccessible, oui ma bonne dame. Donc je n'ai aucun moyen de booter le PC depuis un média quelconque \"CD/DVD/Clé USB\". C'est donc depuis windows seven, je dis bien, que j'ai installé l'installeur debian pour ajouter ensuite au bootloader de windows un point d'entrée pour booter sur l'installeur debian (vous avez bien suivi ? c'est tordu hein ;) Donc à présent, quand je reboot j'ai 3 choix dans mon bootloader windows : windows 7 Debian GNU/Linux l'installeur Debian Tout ça marchait très bien, mais comme Wheezy va sortir dans peu de temps j'ai voulu mettre à jour ma version, et là, la catastrophe. Après un aptitude update un aptitude upgrade m'affichait Résolution des dépendances... ouverts: 378146 fermés: 658730 reportés: 344 en conflit: 1436 ca durait 2 heures, ça m'a gonflé, j'ai tout arrete et fait un simple apt-get dist-upgrade J'ai eu droit à un probleme de depedance sur default-java (un truc du genre) j'ai fait un dpkg -P default-java et relancer le dist-upgrade, et tout se finissa comme dans un rêve Hé bien Nan ! X était planté, j'attéri dans une bonne vieille console 80 colonnes, comme j'avais lancé l'upgrade depuis une session X, X a été mis à jour et je ne voyais plus où l'upgrade s'était arrété alors qu'un ps -auxf m'affichait toujours le process en cours... UN gros kill du PID + un apt-get --JENESAISPLUSQUOIPOURTOUTREMETTREDEQUERRE Et l'upgrade se finissa oui Comme par magie X se lança je pu accéder à gnome 3. Youpi ! Mais en fait non ! Avec Gnome2, ma Nvidia (une GT 220) était prise en charge, et mon processeur (le ventilateur) ne faisait AUCUN bruit. Par bruit j'entends (trop fort:P) comme quand un process lourd se lance vous voyez le genre, style j'extrais des pistes d'un CD avec abcde, ou encode une connerie en avi avec ffmpeg. C'est le même \"son\" que fait le PC au boot tant que l'OS (windows / linux) n'a pas fini de se charger et afficher l'interface graphique (de l'un des deux OS). A présent, avec Gnome3, ce bruit est constant, ce qui me fait dire que Debian Wheezy n'a pas chargé le module NVIDIA approprié sinon je n'entendrais plus un bruit. J'ai regardé la doc sur le wiki debian concernant NVIDIA, un beau bug de la commande nvidia-xconfig flingue la conf de X (créé un fichier xorg.conf vide). Si je tente d'en créer un avec juste la section Monitor pour ajouter le nom du driver, je ne peux plus accéder à X en rebootant. Obliger de faire un \"mode sans échec\" et virer le fichier pour que GDM redémarre. Comme le BRUIT m'insupporte, je suis reparti pour me réinstaller une debian squeeze, et là, pratiquement à la fin, grub veut pas s'installer, lilo non plus. Dès lors qu'on ne fait pas ce que l'installeur s'attend comme opération, grub se vrille ; obliger de tout recommencer, ce qui m'exaspère apres 3 installations _pour rien_ Donc une question à vous lecteur(s) : Avez vous une telle carte graphique et si oui pouvez vous me montrer votre /etc/X11/xorg.conf ? Si non, quelle distribution utilisez vous autre que Ubuntu, qui utilise Gnome3 ? Et par dessus le marché, cerise sur le cake, qui possède un installeur sous windows ? (saloperie de HP ... :P) Dans tout cela je ne vous ai pas dit pourquoi je voulais mettre une Debian Wheezy : Parce que j'ai besoin des nouvelles versions de Python > 2.6 (et accessoirement PHP :D) pour pouvoir fournir des applications fonctionnelles à souhait et parce que la squeeze, une fois la wheezy sortie, ne sera plus \"autant\" maintenu, comprendre que les nouveautés on devra tirer un trait dessus. Edit : 22h51 Bon je m'en suis enfin sorti apres 3 nouvelles installations et en me reprenant la tête avec le driver fourni par Debian qui est une vraie plaie sur mon 64bits... Analyse post-mortem : Quelque soit la config utilisée avec les pack deb l'écran blinkait indéfiniment. J'avais pris tous les pack deb nvidia ; taper nvidia-xconfig ; editer le fichier /etc/X11/xorg.conf mais rien y a fait. Hier pour anticiper le coup, j'avais download le driver chez nvidia.com directement. Ensuite j'ai lancé l'installeur après avoir installé gcc et kernel-header et choisi l'installation via DKMS Grosse erreur ! Il en avait rien à faire de mes kernel-headers. Solution : arreter X /etc/init.d/gdm3 stop lancer ./NVIDIA-.....run et à l'invite NE PAS UTILISER DKMS La compilation se passe les doigts dans le nez ! Ensuite un bon coup de nvidia-xconfig pour pondre le fichier /etc/X11/xorg.conf Puis un petit modprobe nvidia lancer gdm3 /etc/init.d/gdm3 start Et là comme par enchantement le processeur se tait et \"la lumière\" soit ! Me voilà de retour sur une Debian Squeeze ... et j'attendrai de pied ferme une Wheezy STABLE !","tags":"Techno","url":"https://foxmask.net/post/2012/12/17/debian-amour-et-desamour/","loc":"https://foxmask.net/post/2012/12/17/debian-amour-et-desamour/"},{"title":"Oracle bottant le train des réfractaires","text":"Voici le début d'une suite de billets dédiée à Oracle, One of my Job in Professional life :) Il était une fois un DBA qui en avait plein les bottes de demander aux utilisateurs de bien vouloir se déconnecter du schéma pour remonter un dump ... à leur demande de surcroit. Ainsi donc il dégaina une requête pour leur envoyer un bon coup de pompe dans le derrière. Pour trouver la/les connexions existantes sur un schéma oracle on fait : SELECT sid , serial # , osuser , status from v$session where username = '&1' ; et on indique le nom du schéma voulu. Quand on a une session on peut encore se faire \"à la main\" le fameux : alter system kill session ',' ; Mais quand vous avez une ribambelle de têtes de mule, il faut employer les grands moyens, et là, se faire la paire de kill à la main devient pénible et on maudit encore plus ces utilisateurs. Donc on produit un script SQL, nommons le generate-kill.sql , qui va produire un \"rapport\" redirigeant les fautifs dans un script SQL qu'on lancera ensuite. set head off spool kill_session . sql select 'alter system kill session ''' || sid || ',' || serial #|| ''';' from v$session where username = '&1' ; spool off exit On se connecte en tant que sysdba et on tape @ kill_session . sql bye bye : System altered . System altered . System altered . System altered . System altered . System altered . System altered . System altered . System altered . System altered . System altered . Et on peut enfin faire son drop user/create user pépère pour remonter le dump .","tags":"Techno","url":"https://foxmask.net/post/2012/12/13/oracle-bottant-le-train-des-refractaires/","loc":"https://foxmask.net/post/2012/12/13/oracle-bottant-le-train-des-refractaires/"},{"title":"Django blocktrans et l'indigestion de variables","text":"Dans mes tribulations (d'un Djangonaute en Poneyland) d'exploitation de modules tiers comme django registration , le template permettant l'envoi de mail est très \"free style\", comprendre qu'il est tellement explicite qu'on en perd son latin, du coup dans mes \"débuts\" je me suis carrément pris la tête à dépiauter tout la pile d'appels du \"schmilblick\". Ce qui ne marche pas Ainsi j'avais codé : { % url 'registration_activate' activation_key as registration_activate % } { % blocktrans % } Hello , You wish to subscribe to the service provided by {{ site . name }} . To confirm your subscription , go on the following page {{ site . domain }}{{ registration_activate }} to activate your account Your subscription request will expire in {{ expiration_days }} days if you dont confirm it . -- 2012 - {{ site . domain }} { % endblocktrans % } et le mail partant, arrivait comme ceci Hello, You wish to subscribe to the service provided by . To confirm your subscription, go on the following page /accounts/activate/1ccfbb6800bde1b8549f8384cd88fa3d1c931fa3/ to activate your account Your subscription request will expire in 7 days if you dont confirm it. -- 2012 - Un beau mail ... pourri à souhait, mais que se passe-t-il ? WTF ? je suis allé jusqu'à coller des print dans le fonction send_activation_email du module registration mais l'objet site était correctement peuplé. J'ai rechecké que mes propres context_processors n'interfèraient pas avec le template et que le framework Site était paré. Ce qu'il fallait faire Puis d'un coup (il était temps depuis 1h que je tournais en rond à tout vérifier) je me suis rappellé mais blocktrans faisait des indigestion de variables ... faut les lui faire bouffer tout cru en lui pinçant le pif avec le keyword with ! { % url 'registration_activate' activation_key as registration_activate % } { % blocktrans with site_name = site . name site_domain = site . domain % } Hello , You wish to subscribe to the service provided by {{ site_name }} . To confirm your subscription , go on the following page {{ site_domain }}{{ registration_activate }} to activate your account Your subscription request will expire in {{ expiration_days }} days if you dont confirm it . -- 2012 - {{ site_domain }} Et voilà ! Ceci m'avait sauté aux yeux avec la tentative d'utilisation de {% url .... %} puisque django hurlait ses grands Dieux que je n'avais pas le droit de le faire, donc j'avais trouvé à résoudre le problème de url, mais là Django s'est tu sans dire que site.xxx ne gênait.","tags":"Techno","url":"https://foxmask.net/post/2012/10/15/django-blocktrans-indigestion-variable/","loc":"https://foxmask.net/post/2012/10/15/django-blocktrans-indigestion-variable/"},{"title":"Django, un template tag, une boussole dans le template","text":"Dans un template utilisé tout au long de son site, tel un menu, je voulais trouver un moyen de savoir quelle vue l'avait appelé afin de positionner une classe CSS 'active' au bon endroit. Dans mes recherches j'ai trouvé A Django template tag for the current active page , où au début j'avais le me cheminement consistant à parser request.path, ensuite j'ai mis la main sur une amélioration de la proposition précédente sur ce blog ... que j'ai à mon tour modifié ;) Voici ce que ça donne dans un template : { % url 'base' as home % } { % url 'profiles_profile_detail' request . user . username as profiles_profile_detail % } { % url 'relationship_list' request . user . username 'friends' as relationship_list_friends % } { % trans \"Home\" % } {{ request . user . username }} { % trans \"My network\" % } { % trans \"log out\" % } Mon grin de sel se situe coté HTML où je ne veux pas laisser dans la page HTML des propriétés vides comme on le voit ici Du coup je retourne bêtement class=\"active\" plutôt que active @register . simple_tag def active ( request , pattern ): import re pattern = \"&#94; %s $\" % pattern if re . search ( pattern , request . path ): return ' class=\"active\" ' return ''","tags":"Techno","url":"https://foxmask.net/post/2012/10/13/django-template-tag-boussole-template/","loc":"https://foxmask.net/post/2012/10/13/django-template-tag-boussole-template/"},{"title":"Django Shortener clap une première","text":"Ce soir, me suis fendu d'un mini module django permettant de gérer son propre service de raccourcisseurs d'URL tel bitly et ses amis. Premier effet kisskool : un service \"con comme la lune\" On dépose une longue URL, le module vous en retourne une version courte. 2 petits paramètres sont nécessaire dans votre fichier settings.py: la taille de l'URL courte, l'hôte hébergeant le service (si ce n'est pas le domaine courant). Quand c'est fait vous faites votre copier coller de l'URL courte dans votre texte et \"voilà\" comme ils disent outre manche ;) Deuxième effet kisskool : intégration avec d'autres modules Comme le service ne pouvait s'en tenir là, j'ai aussi produit un template processor permettant de parser une string pour en extraire toutes les URLs et en ressortir une version courte pour chaque ! {{ my_nice_text | shrt }} on peut bien évidement ajouter une ribambelle de filtres à la suite {{ my_nice_text | shrt | safe | escape }} Où trouver le module ? Pour tester ce module vous avez la possibilité de le trouver sur PyPi , ou de taper pip install django_shortener ou de vous faire un fork sur github . Tout retour sur ce module sera apprécié comme c'est le premier, j'espère d'une longue série ;)","tags":"Techno","url":"https://foxmask.net/post/2012/09/30/django-shortener-clap-une-premiere/","loc":"https://foxmask.net/post/2012/09/30/django-shortener-clap-une-premiere/"},{"title":"Python pip freeze by noob","text":"Dans la série je découvre les joies de python, voici un très court billet sur l'utilisation de pip à la suite de ce très instructif billet de Sam et Max où on apprend qu'on peut produire son propre fichier requirements.txt recensant les modules utilisés pour son virtualenv . Le but de la manoeuvre étant d'éviter de se coltiner les réinstallations à l'identique sur des virtualenv / serveurs distincts. Donc tout content de mettre en pratique je me lance la commande comme suit : foxmask@foxmask:~$ cd Django-VirtualEnv/django-huanui foxmask@foxmask:~/Django-VirtualEnv/django-huanui$ pip freeze --local > requirements.txt ce qui me donne comme requirements.txt : Axiom==0.6.0 BeautifulSoup==3.1.0.1 Brlapi==0.5.5 ClientForm==0.2.10 Coherence==0.6.6.2 Epsilon==0.6.0 GnuPGInterface==0.3.2 Louie==1.1 Mako==0.3.4 MarkupSafe==0.9.2 MySQL-python==1.2.2 Nevow==0.10.0 PAM==0.4.2 PIL==1.1.7 PyOpenGL==3.0.1b2 Twisted-Conch==10.1.0 Twisted-Core==10.1.0 Twisted-Web==10.1.0 apt-xapian-index==0.41 chardet==2.0.1 configobj==4.7.2 cups==1.0 distribute==0.6.14 feedparser==4.1 gdata==2.0.8 gnome-app-install==0.4.7-nmu1 httplib2==0.6.0 louis==2.0.0 mechanize==0.1.11 mercurial==1.6.4 numpy==1.4.1 pep8==0.5.0 pexpect==2.3 pyOpenSSL==0.10 pyasn1==0.0.11a pycrypto==2.1.0 pyserial==2.3 pysqlite==2.6.0 python-apt==0.7.100.1-squeeze1 python-debian==0.1.18-squeeze1 pyxdg==0.19 rdflib==2.4.2 reportbug==4.12.6 rope==0.9.2 tagpy==0.94.7 uTidylib==0.2 unattended-upgrades==0.1 virtualenv==1.4.9 wsgiref==0.1.2 zope.interface==3.5.3 Oula mais ça me va pas cette ribambelle de modules, but wtf ? Bon ben comme les pros du pot l'ont déjà vu ; faute de débutant flagrante. Quand on se créé un virtualenv on n'utilise les scripts qui sont dans ce dernier, donc dans mon cas ici : ~/Django-VirtualEnv/django-huanui/bin/ donc on la refait : foxmask@foxmask:~/Django-VirtualEnv/django-huanui$ ./bin/pip freeze --local > requirements.txt et cette fois ci on respire, la \"pollution\" environnementale s'est dissipée : foxmask@foxmask:~/Django-VirtualEnv/django-huanui$ cat requirements.txt Django==1.4.1 distribute==0.6.10 django-debug-toolbar==0.9.4 django-profiles==0.2 django-registration==0.8 django-relationships==0.3.2 nota: si on ne colle pas le --local on \"ramasse\" tous les modules précédents edit : vu ce weekend à PyConFr grâce à @Dzen ;) pour ne plus se prendre le chou avec ./bin/pip ou autre ../bin/bin/pip blabla dans le dossier de son virtualenv on tape betement source bin/activate et les variables d'environnement feront leur boulot pour que tout soit relatif à votre environnement virtuel __avant__ celui de la machine courante","tags":"Techno","url":"https://foxmask.net/post/2012/09/10/python-pip-freeze/","loc":"https://foxmask.net/post/2012/09/10/python-pip-freeze/"},{"title":"Django reverse for xxx with arguments ... not found","text":"Nombreux sont ceux qui tombent dans le piège de cette erreur qui semble être une erreur de noobs ... Cette erreur la voici : Erreur : NoReverseMatch at / histo / Reverse for '' histo_save '' with arguments '()' and keyword arguments ' {} ' not found . Pour autant tout semble être correct, le fichier urls.py contient url ( r '&#94;histo/$' , 'tglr.views.histo' , name = 'histo' ), Tantdis que le template lui contient ``` {language=\"python\"} {% csrf_token %} Donc tout semble parfaitement tourner rond sauf que ... dans la console saute aux yeux : ```shell /usr/local/lib/python2.6/dist-packages/django/template/defaulttags.py:1235: DeprecationWarning: The syntax for the url template tag is changing. Load the ` url ` tag from the ` future ` tag library to start using the new behavior . category = DeprecationWarning ) Okkkayyyyy donc dans le template on rajoutera : { % load url from future % } et tout rentrera dans l'ordre Pour info j'aurai pu faire sauter les simples quotes autour de histo_save mais comme le dit le DeprecationWarning , utiliser {% url toto %} est déprécié","tags":"Techno","url":"https://foxmask.net/post/2012/08/31/django-reverse-for-xxx-with-arguments-not-found/","loc":"https://foxmask.net/post/2012/08/31/django-reverse-for-xxx-with-arguments-not-found/"},{"title":"HaveFnuBB 1.5.0","text":"cette nouvelle version, 1 an après la précédente quasiment jour pour jour, est marquée par de nombreuses corrections de bugs et d'importantes améliorations de performances. Corrections modules : havefnubb: moult petites corrections moduleinfos : affiche à présent les infos des modules correctement hfnurates : correction de la soumission des données ajax quand l'utilisateur n'est pas connecté fonctionnalités : le désabonnement n'était pas possible. après l'abonnement à une discussion, on ne retournait pas à la page d'où on venait suppression de la gestion des caches des zones des utilisateurs ne permettant pas l'affichage exact de leur information dans le mail avertissant qu'un nouveau message auquel on est abonné contenait un le lien erroné menant à une page 404 disparition des Tags sur les discussions corrigée Améliorations : installation l'installation du forum a été simplifiée pour fournir les informations \"à propos\", affichage des informations de la version existante et celle à installer l'installation de mise à jour / migration a été revue forum le recomptage des messages que l'on bouge d'un forum à l'autre a été améliorée gestion des flux Atom Ajout de meta keyword dans la configuration du forum Gestion de la messagerie privée Performances : le Thème principal ne repose à présent plus sur le système de grille 960gs dans un soucis de performance et d'amélioration de la rapidité du rendu, les autres thèmes restent inchangés et toujours basés sur 960gs le forum a été allégé dans la gestion de ses tables afin d'éliminer le maximum d'intermédiaires pour obtenir des informations simples et les restituer le plus vite possible. Ainsi la quantité de requêtes SQL a chuté drastiquement de même que les appels inutiles à des templates et zones. Par conséquent si vous aviez produit des thèmes, il faudra revoir le templae index.tpl pour y retirer l'appel d'une zone et la remplacer par une simple ligne HTML. (cf le index.tpl du thème par défaut pour plus d'infos). Installation de HaveFnuBB Télécharger la version 1.5.0 Une fois fait, au choix vous pouvez utilisez : le Wizard via l'interface web - pour une première installation rendez vous sur http://localhost/install.php - pour une mise à jour depuis une version 1.4.x, rendez vous sur http://localhost/update.php La ligne de commandes - Pour l'installation : `php lib/jelix-scripts/jelix.php --havefnubb installapp ` - Pour la mise à jour `php lib/jelix-scripts/jelix.php --havefnubb installapp ` Ah ba oui c'est bien la même commande ! ne vous avais je pas dit que Jelix était fun ? ;) Have Fnu !","tags":"Techno","url":"https://foxmask.net/post/2012/04/02/havefnubb-1-5-0/","loc":"https://foxmask.net/post/2012/04/02/havefnubb-1-5-0/"},{"title":"Jelix PHP 5 Bouillon de Cultures","text":"Je ne vous présente plus Jelix , le framework PHP 5, voici juste quelques news de ce dernier qui pourront vous intéresser ;-) Récemment ouverte chez googlegroups , la mailing-list Jelix-fr est le terrain de riches discussions sur le devenir du framework . Terrain fertile depuis que les statistiques de téléchargements ont décolé significativement ! Parmi les échanges on retrouve la réorganisation et réaménagement des roadmaps pour permettre aux contributeurs de mieux s'y retrouver. Ainsi sont abordés pour les versions : la v1.4, en particulier le rythme de publications des releases de la branche 1.x, le but est de tendre sur 3 mois. Sur son contenu , jAuth devrait s'enrichir avec oAuth, jForms devrait améliorer le stockage des instances, jApp::Config remplacerait les variables globales gJConfig et gJCoord. La simplification du moteur d'URL (à un seul moteur au lieu de 3 actuellement) pour améliorer les perf. Par ailleurs Laurent a débuté la rédaction de quelques RFC pour concrétiser ces réflexions. la v2 dans ses grandes lignes pour les tests unitaires : l'utilisation d' atoum a été évoquée Même si le framework est déjà performant sur des sites à forte charge, le but, comme on peut le voir, est d'améliorer les perfs encore et toujours. Ainsi donc, les échanges (sur la ML ) sont nombreux et chacun peut venir exposer ce qu'il ne veut plus ou souhaite absolument voir entre ses mains, débattre d'axes d'améliorations, etc... Enfin comme à son habitude l'équipe est ouverte à toutes propositions, alors venez nous en parler ! Quelques liens utiles : mailing-list Jelix-fr Comment contribuer au projet Jelix et travailler avec les sources Le dépôt GitHub de Jelix","tags":"Techno","url":"https://foxmask.net/post/2012/01/12/jelix-php-5-bouillon-de-cultures/","loc":"https://foxmask.net/post/2012/01/12/jelix-php-5-bouillon-de-cultures/"},{"title":"Oracle et taille de schema, tablespace","text":"Pour obtenir la taille d'un schéma, avec son script size_of_schema.sql SELECT SUM ( bytes ) / 1024 / 1024 FROM dba_segments WHERE owner = '&1' ; à l'invite, donnez lui le nom du schema et roule ;) Pour obtenir le paramètre de la base \"Undo Retention\", avec son script undo_retention.sql SELECT name , VALUE FROM v$parameter WHERE name = 'undo_retention' ; On tape sur la vue v\\$parameter qui contient une bonne partie des paramètres de votre base oracle que vous avez dans l'init.ora Taille du tablespace Undo avec son petit script undo_tablespace.sql SELECT ( bytes / 1024 / 1024 / 1024 ), FILE_NAME FROM dba_data_files WHERE tablespace_name = 'UNDO' ; le tablespace UNDO et le paramètre Undo retention permettent la gestion des rollback. l'undo retention est definie en minute, la duree pendant laquelle on conserve une requête de rollback et l'undo tablespace lui gère les données qu'on annule. Si l'un des 2 est trop petit, les rollback vont exploser.","tags":"Techno","url":"https://foxmask.net/post/2012/01/10/oracle-et-taille-de-schema-tablespace/","loc":"https://foxmask.net/post/2012/01/10/oracle-et-taille-de-schema-tablespace/"},{"title":"Linux Live USB Persistent : failed","text":"Suite à cette mésaventure , j'ai fait les frais d'une clé USB 32Go, et de 2 soirées d'affilées à la recherche d'images ISO qui soit exploitable sur le (vieux) portable recyclé pour ma fille. Mon \"cahier des charges\" étant : Hardware RAM utilisable 512Mo Wifi sur une clef USB à prendre en charge Linux en Français (avec Gnome 2 pas 3 tant qu'à faire puisque vieux PC) le mode persistant pour stocker quelques photos de copines :P Fort de tout cela j'ai pu essayer plusieurs outils : Linux Live USB fourni un outil permettant de choisir quel périphérique utiliser, de télécharger une ISO, l'installer et spécifier la taille max dédié au mode persistant. Mon premier choix s'était bien sûr porté sur Debian (utilisateur de debian depuis la potatoe ceci explique cela:), mais les ISO debian 6.0.1 requis par l'outil ne sont pas les bonnes... Ensuite j'ai pu m'orienter sur Ubuntu 11.10 , et tenter en vain de booter sur le portable... En vain car j'avais pas fait attention que j'avais déjà du mal à booter une 10.10 avant le crash du disque dur ... J'ai donc repris un 10.10 mais hélas mille fois hélas, à part le splash screen avec les petits points qui passent du blanc au marron, je n'arrive jamais à passer cet écran. Pourtant avec un live CD Ubuntu 10.10 , ca passe ... Devant cet échec j'ai installé les doigts dans le nez, une slackware : Slax de son nom! Slax fourni KDE mais est totalement en anglais (or je vous rappelle que c'est pour ma fille de 10ans, l'anglais technique c'est un poil trop tôt:) ... et ne prend pas en charge ma clef usb pour le wifi (à ba oui on cumule les obstacles) J'ai bien évidement tenté d'installer le projet Linux Live USB Debian mais à part une liste de packages à installer, rien n'explique comment le faire... Ensuite pour quand même tenter d'installer une Debian (oui têtu un jour têtu toujours) j'ai utilisé Cet outil qui m'a permis de mettre une 6.0.3 tout aussi facilement que la slackware, et booter, tout nickel jusqu'au deux derniers obstacles : la clé usb pour le wifi, non reconnue et Gnome full English ! Shit ! J'ai eu beau récupéré le driver (non-free:p) pour l'installer, de réseau wifi la Debian ne vit, ni ne voulu prendre en compte avec les infos que je lui avais fourni. Après tout cela je suis dépité, ok Ubuntu avant le crash disque reconnaissait ma clé usb et fonctionnait tant bien que mal, mais depuis toutes ces années je n'avais jamais vu de distribution ne pas me donner satisfaction. Sauf qu'à présent, l'idée de \"redonner un second souffle à un PC\" avec un linux salvateur/sauveur devient de plus en plus hypothétique. Si ubuntu avait envi de faire comme windows en devenant de plus en plus gourmand, c'est gagné ! Maintenant où j'en suis ? là : \"Papa quand est-ce que je peux utiliser mon ordinateur ?\" PS : maintenant je suis calé pour monter une distrib' en mode persistant et ... redimensionner la partie non allouée au delà des 4Go :D Si quelqu'un connait une distribution linux avec un gnome 2 en français qui soit exploitable avec moins de 512Mo ... et avec prise en charge du driver usb ZD1211 ca serait le grand pied !","tags":"Techno","url":"https://foxmask.net/post/2011/11/05/linux-live-usb-persistent-failed/","loc":"https://foxmask.net/post/2011/11/05/linux-live-usb-persistent-failed/"},{"title":"Linux sur laptop sans disque dur","text":"Hmm naaaaaaaaaaaaaaaaannnnnnnnnnnnn, le disque dur du portable qui me lâche :( Quelqu'un aurait une idée pour utiliser linux sur un (vieux) portable sans disque dur ? j'imaginerai juste pouvoir stocker le /home sur clé usb au moins, genre pour conserver les préférences de firefox et puis c'est tout. est-ce possible ? nota : le disque dur était un Seagate ST94019A (de 40Go) sur un hp pavillon zv5000.","tags":"Techno","url":"https://foxmask.net/post/2011/10/22/linux-sur-laptop-sans-disque-dur/","loc":"https://foxmask.net/post/2011/10/22/linux-sur-laptop-sans-disque-dur/"},{"title":"Jelix PHP5 Framework, version 1.3.0 dans les bacs","text":"Le framework PHP5, Jelix , sort une nouvelle version majeure (*) : 1.3 . Cette dernière a mis l'accent sur de nombreuses améliorations et nouveautés , dont une meilleure gestion des erreurs et des améliorations du système de log une nouvelle barre de debuggage, extensible ( que je vous avais présenté ) un objet réponse html amélioré, et extensible des simplifications dans la structure d'une application, et dans les scripts d'aide au développement une gestion des profils de connexion unifiée (jDb, jKvDb, Soap etc) des améliorations dans la gestion des droits jAcl2 et beaucoup beaucoup d'autres ... De même, tout est fait pour que vous puissiez migrer aisément depuis toute version antérieure à la 1.3. Dans la dynamique de cette version nous en avons profité pour produire le projet \" Booster \", le portail des ressources Jelix produites par tout à chacun. [*] La manuel complet [*] Télécharger la version 1.3.0","tags":"Techno","url":"https://foxmask.net/post/2011/10/19/jelix-php5-framework-version-1-3-dans-les-bacs/","loc":"https://foxmask.net/post/2011/10/19/jelix-php5-framework-version-1-3-dans-les-bacs/"},{"title":"HaveFnuBB Thème Twitter Bootstrap","text":"Après de longs mois sans trop faire avancer le projet (la faute aux vacances et à Booster.jelix.org ;) ) voici donc après quelques temps d'adaptation au toolkit tout en un qu'est Twitter BootStrap , je me suis lancé sur la création d'un thème pour HaveFnubb , voici ce que ça donne : La page d'accueil La liste des messages Encore quelques pages et ca devrait être pas mal au final. rg ;) ) voici donc après quelques temps d'adaptation au toolkit tout en un qu'est Twitter BootStrap , je me suis lancé sur la création d'un thème pour HaveFnubb , voici ce que ça donne : La page d'accueil La liste des messages Encore quelques pages et ca devrait être pas mal au final.","tags":"Techno","url":"https://foxmask.net/post/2011/10/02/havefnubb-theme-twitter-bootstrap/","loc":"https://foxmask.net/post/2011/10/02/havefnubb-theme-twitter-bootstrap/"},{"title":"Jelix PHP5 Framework tout neuf","text":"Bonjour, En ces temps de \"rentrée\", Jelix annonce, lui, plusieurs \"sorties\" ! Number one : nouveau serveur dédié pour le projet ! Le précédent avait été gracieusement offert par Bastnic mais avait fini par se sentir mal (et nous avait fait des frayeurs dernièrement;) Number Two : nouveau projet Booster . Ce site va permettre à la communauté Jelix de trouver un dépôt unique, central, de toutes les ressources Jelix existantes produites par la communauté, et surtout de se l'approprier pour ajouter ses propres créations. Ainsi donc, on y retrouvera tout aussi bien, des applications complètes, prêtes à l'emploi que des petits plugins ou gros modules. Ce projet aura été réalisé avec le concours de FlorianLB et votre serviteur. Number Three : Nouvelles releases 1.2.5 et 1.3RC2 . Voilà ! On aura été sur tous les fronts ;) et on a encore un paquet de choses en cours dont la mise à jour du forum, la 1.3 finale, un passage à OpenWorldForum , ParisWeb , etc... PS : un grand merci à Laurent pour m'avoir laissé prendre autant de place dans le projet depuis 3ans que je m'y suis interessé, franchement un bon grand \"kiffe\" que cette aventure, comme 'ils' disent ! Et une mention spéciale pour FlorianLB avec qui je prends beaucoup de plaisir à coder Booster.","tags":"Techno","url":"https://foxmask.net/post/2011/09/23/jelix-php5-framework-tout-neuf/","loc":"https://foxmask.net/post/2011/09/23/jelix-php5-framework-tout-neuf/"},{"title":"Erreur 403 pour Validator W3C et Google","text":"Un symptôme me casse bien les pieds depuis quelques temps et je ne trouve pas l'issue. Les Services \"Google centre pour webmasters\" et le \"Validator W3C\" me crachent une erreur 403 sur le présent site et un 2nd ; tous 2 hébergés chez OVH. Or à part le fichier robots.txt et le .htaccess je ne vois pas ce qui défriserait ces 2 services :/ J'ai questionné le support OVH, je mettrai les suites de ces mésaventures ici au fur et à mesure de l'avancement de ce patacaisse.","tags":"Techno","url":"https://foxmask.net/post/2011/07/16/erreur-403-pour-validator-w3c-et-google/","loc":"https://foxmask.net/post/2011/07/16/erreur-403-pour-validator-w3c-et-google/"},{"title":"HaveFnuBB! 1.4.0","text":"Bonjour, (ré)Intro : HaveFnuBB est un logiciel de gestion de Forums OpenSource (sous licence GPL 2.0) avec pour buts d'être : Rapide, Léger et Agréable ! Aujourd'hui : Après près d'un an de développement depuis la sortie de la version 1.3.6, voici la 1.4.0. Cette version se dote d'une nouvelle mouture du Framework PHP5 Jelix en 1.2 ( facilitant grandement les installations/mises à jour des modules et des applications ), et a subit énormément de corrections de bogues, d'améliorations des performances Je tire mon chapeau et adresse un très grand merci à Mr Jelix himself : Laurent Jouanneau , pour sa participation au code du forum, qui a grandement apporté sa pierre à l'édifice et remis plein de choses à leur place ;) L'annonce complète Télécharger HaveFnuBB! 1.4.0","tags":"Techno","url":"https://foxmask.net/post/2011/04/03/havefnubb-1-4-0/","loc":"https://foxmask.net/post/2011/04/03/havefnubb-1-4-0/"},{"title":"Django, la doc, en PDF c'est mieux","text":"Afin de rentabiliser le temps que je perds dans les transports parisiens, et ayant envi de découvrir Django , me suis dit qu'il me faudrait bien la documentation de Django sur mon p'tit iPodTouch. Donc je me suis farci le site http://www.djangoproject.com/ en long en large et en travers à la recherche d'un PDF, mais en vain. En faisant une petite recherche sur google , je me jette sur un vieux billet de 2008 , qui indique la marche à suivre pour ... se générer la documentation soi-même. Ok, je vais pas m'arrêter là, suis motivé :-) hop installation des outils easy_install et du support latex apt-get install python-setuptools apt-get install python-plastex apt-get install python-sphinx apt-get install texlive-full ( necessaire sinon erreur à la compilation latex = >pdf ) Mais au moment de la compilation sur ma debian (5.0) via : sphinx-build -b latex . build_latex l'avion crash en plein vol ... The process fails when it tries to do an import: from sphinx.directives.desc import option_desc_re Bon, pour ne pas passer pour un enquiquineur de première sur la ML django-fr, je m'en retourne voir google , et nouvelle trouvaille J'apprends là bas qu'il faut une mise à jour issue tout droit du trunk django, et qui consiste donc à copier le fichier djangodocs.py en lieu et place de celle de la 1.2.1.... et rebuilder Reste donc enfin à produire le pdf :) latex build_latex/django.tex django.pdf Et là miracle, ca repète :) tex capacity exceeded, sorry [ save = 5000 ] rebelote google tout çaaaaaaaaaaaa, on doit donc éditer /usr/share/texmf/web2c/texmf.cnf et changer la valeur de save_size retestage : latex build_latex/django.tex django.pdf Output written on django.pdf ( 1032 pages, 5261614 bytes ) Aaaaaaaaaaaaah enfin je peux le transférer dans iBooks sur l'iPodTouch Quel périple juste pour un PDF, gageons que la suite sera moins \"rigolote\" . Ben en fait c'est pas génial :/ dans le PDF, le code écrit en noir sur fond noir ça le fait pas du tout. EDIT : après un retour à la doc du projet lui même j'ai fini par faire plus simple et ça marche : Donc on se rend dans le dossier \"doc\" et on tape juste : make latex puis cd _build/latex make all-pdf pour obtenir Output written on django.pdf ( 1057 pages ... ) et cette fois ci on a le code illustrant la doc tout à fait lisible :) La Doc Django 1.2.1 en PDF est doc là :) EDIT2 : c'est bien lisible dans iBooks :D à moi le mille feuilles dans le RER :-)","tags":"Techno","url":"https://foxmask.net/post/2010/08/24/django-doc-en-pdf-cest-mieux/","loc":"https://foxmask.net/post/2010/08/24/django-doc-en-pdf-cest-mieux/"},{"title":"Jelix c'est Fini !","text":"Bon, hé bien finalement, pour moi l'aventure Jelix s'arrêtera là. Les sirènes de Symfony m'ont convaincues par le biais de Doctrine, que c'était le framework qu'il me fallait. Donc je vais arrêter également HaveFnuBB, le forum éponyme 100% Jelix. Je garderai juste un oeil sur le projet mais de loin quand même :-)","tags":"General","url":"https://foxmask.net/post/2010/04/01/jelix-cest-fini/","loc":"https://foxmask.net/post/2010/04/01/jelix-cest-fini/"},{"title":"Jelix versions 1.1.6 1.0.12 et Bitbucket","text":"Oyé Oyé Braves Gens Voici Quelques nouvelles de Jelix . Nouvelles versions : A commencer par 2 nouvelles versions de mises à jour de maintenance pour les branches 1.1 et 1.0 Où pour la version 1.1.6, une petite manipulation est nécessaire suite au support des champs binaires dans jDb et jDao Nouveaux dépots : Ensuite, il est à noter que les dépôts même de Jelix sont à présent sur Dans son sillage, auront bougé des projets de la forge tels : jCommunity : l'indispensable module pour gérer son site communautaire HaveFnuBB : le forum 100% Jelix powered , vrai composite des modules de la communauté jBlog : un blog 100% Jelix powered :) \"dépôt de miel\" : un ensemble de modules jelix indépendants, comme le download manager, etc.. Rarangi : Le Générateur de documentation PHP jButracker : Le système de gestion de tickets, wiki, sources Voilà de quoi satisfaire la majorité d'entre nous ;-)","tags":"Techno","url":"https://foxmask.net/post/2010/03/27/jelix-versions-1-1-6-1-0-12-et-bitbucket/","loc":"https://foxmask.net/post/2010/03/27/jelix-versions-1-1-6-1-0-12-et-bitbucket/"},{"title":"HaveFnuBB Hook System","text":"Parmi les forums bien pensés, existent des systèmes d'extension (ou hook) laissant libre cours à la créativité des développeurs de ces hooks. Dans HaveFnuBB (le forum que je réalise), il m'aura fallu un plugin de 10 lignes pour permettre à tout développeur, d'enrichir ses templates avec une souplesse déconcertante. L'idée derrière ce plugin est la suivante : Je me créé un portail, blog, forum, et me \"contente\" d'une petite batteries de fonctionnalités rendant la page à mon goût. Mais avec ce plugin, je permets à quiconque de rajouter ses propres fonctionnalités à l'endroit où le hook est appelé. Concrètement comment cela marche ? Je vous invite à poursuivre la lecture par ici => HaveFnuBB Hook System :)","tags":"Techno","url":"https://foxmask.net/post/2009/11/10/havefnubb-hook-system-2/","loc":"https://foxmask.net/post/2009/11/10/havefnubb-hook-system-2/"},{"title":"IRSSI dans tout sa splendeur","text":"Utilisateur du client IRC IRSSI depuis des siècles on en découvre toujours :) Sur Freenode il arrive qu'on soit redirigé sur #freenode-fr après s'y être connecté voici la config pour l'accès à freenode servers = ( { address = \"irc.freenode.org\"; chatnet = \"freenode\"; port = \"6667\"; autoconnect = \"yes\"; term_type = \"utf-8\"; }); Donc Pourquoi la redirection vers ce channel ? Parce qu'on ne s'est pas identifié au nickserv avant d'entrer sur le channel. Pourtant vous avez bien ajouté à irssi l'autosendcmd permettant de vous identifier au service nickserv chatnets = { freenode = { type = \"IRC\"; nick = \"votre_pseudo\"; autosendcmd = \"/&#94;msg NickServ IDENTIFY mot_de_passe\"; };}; Donc au démarrage de IRSSI, on lit bien 09 : 45 [ freenode ] - NickServ ( NickServ @services .) - Please identify via / msg NickServ identify < password > . 09 : 45 [ freenode ] - NickServ ( NickServ @services .) - You are now identified for FoxMaSk . 09 : 45 [ freenode ] - ! - FoxMaSk n = foxmask You are now logged in . ( id FoxMaSk , username n = foxmask , hostname foxmask . info ) Donc que se passe-t-il ? Hé bien la connexion de IRSSI est trop rapide , et irssi enchaine aussitôt l'accès au channel sitôt connecée sans avoir le temps de passer par la case nickserv. Donc l'option permettant de faire un pause après la connection et l'indetification avant de joindre un channel est un wait qui s'utilise comme suit : autosendcmd = \"/&#94;msg NickServ IDENTIFY mot_de_passe;wait 2000\" ce qui marquera une pause de 2secondes et là le tour est joué !","tags":"Techno","url":"https://foxmask.net/post/2009/10/25/irssi-dans-tout-sa-splendeur/","loc":"https://foxmask.net/post/2009/10/25/irssi-dans-tout-sa-splendeur/"},{"title":"ForumPHP 2009, avec Jelix et HaveFnuBB","text":"Dans ce précédant billet , j'évoquais une \"carence\" sur la présence des développeurs PHP contribuant à la communauté. Et ô surprise, quelque jours plus tard, je me vois contacter par un membre de l'Afup pour participer à un \"espace projets PHP opensource\". Je ne sais pas s'il y a une relation de cause à effet, mais en tout cas, l'esprit \"du libre\" m'animant depuis plus de 10ans, je n'ai pu m'empêcher de partager cela et contacter mes \"ex\" de CakePHP-Fr pour leur en toucher 2 mots, juste histoire que \"cette porte entrouverte\" profite au plus grand nombre afin que l'évènement soit une vraie réussite et perdure :) Et donc CakePHP-Fr y sera aussi :) Ainsi donc je pourrai vous présenter HaveFnuBB, le forum OpenSource produit avec Jelix le framework PHP5. Et cerise sur le gâteau, en la présence de Laurent Jouanneau, mr Jelix lui-même. :-)","tags":"Techno","url":"https://foxmask.net/post/2009/10/20/forumphp-2009-avec-jelix-et-havefnubb-2/","loc":"https://foxmask.net/post/2009/10/20/forumphp-2009-avec-jelix-et-havefnubb-2/"},{"title":"Jelix Dev Summit Episode 1, le 9 Octobre 2009","text":"Non ce n'est pas le titre d'une série Américaine \"choppée\" sur le reseaux peer-to-peer, mais tout simplement de la première rencontre des développeurs Jelix qui se tiendra donc ce 9 octobre 2009 prochain. Les lieu et sujets abordés sont disponibles ici","tags":"Techno","url":"https://foxmask.net/post/2009/10/01/jelix-dev-summit-episode-1-9-octobre-2009/","loc":"https://foxmask.net/post/2009/10/01/jelix-dev-summit-episode-1-9-octobre-2009/"},{"title":"Jelix et le module jCommunity","text":"En PHP, si une classe ou API existante vous plait mais que vous souhaitez y apporter votre touche personnelle vous inclurez cette dernière et la surchargerez pour éviter d'y toucher. Exactement le même principe s'applique avec les modules Jelix. On parlera alors \"d'overload\". Un module n'étant pas une simple classe (mais composé de contrôleurs, daos, forms, template, zones, classes, fichiers de traductions) tout ne peut-être surchargé. Qu'est ce qui peut faire l'objet d'overload ? Les templates : pour modifier le rendu les Dao : pour ajouter des propriétés (des colonnes/ tables ) et des factories (méthodes d'accès aux tables) les forms : pour ajouter des champs de formulaires et leurs règles de contrôle associées les locales : fichiers de traductions Avantages de l'\"Overload\" : Extension des fonctionnalités du module original Adaptabilité de tous les aspects du module (exception faite du contrôleur) Facilité de mise à jour du module originel , puisqu'on ne change aucune ligne de code de ce dernier Ainsi prenons comme exemple concret le module Jelix, jCommunity, et voyons ce que l'on peut en tirer. Présentation : jCommunity est le module de gestion des utilisateurs pour tout type de site web . Il inclut une gestion complète du workflow de : Inscription/désincsription Connexion/déconnexion 1) Overload des templates : Question : Ok c'est cool mais je veux adapter le rendu des templates de jCommunity aux thèmes de mon site. comment faire ? Réponse Par défaut le nom du thème se nomme default. Ainsi donc pour surcharger un template on placera tout simplement la version modifiée de ce dernier dans le dossier var/themes/default/jcommunity Ainsi quand Jelix appellera le template du module, il ira voir d'abord si une version existe dans le dossier des thèmes et l'utilisera. Comparer : le template de la page de consultation d'un membre : jcommunity (l'originale) havefnubb (l'overload) 2) Overload de Dao : Question La table jCommunity ne contient pas assez d'informations pour gérer mes utilisateurs, comment \"étendre\" celle-ci ? Réponse Copions la Dao jcommunity/dao/user.dao.xml dans le dossier var/overload/jcommunity/daos Changeons à l'intérieur tout ce dont on a besoin : nom de la table et des nouvelles colonnes. Nouvelles méthodes Dao etc. Comme précédemment Jelix ira voir d'abord si une Dao existe dans ce dossier. tout cela en respectant à minima le nom des méthodes existantes dans la DAO d'origine. Comparer : la Dao account.dao.xml : jcommunity (l'originale) havefnubb (l'overloaded) 3) Overload de forms : Question : Le formulaire gérant les données des utilisateurs est trop léger à votre goût ? Réponse : Qu'à cela ne tienne on crée une copie du forms jcommunity/forms/account.form.xml dans le dossier var/overload/jcommunity/forms Ensuite on y rajoute les champs qu'il nous sied de voir afficher (correspondant à notre DAO modifiée). Comme précédemment Jelix ira voir d'abord si un forms existe dans ce dossier. Comparer : le Forms account.forms.xml : jcommunity (l'originale) havefnubb (l'overloaded) 4) Extension fonctionnelle du module Ok tout fonctionne nickel On a pu tout surcharger comme souhaité Subsiste un dernier détail, après la surcharge de la couche de présentation, on a besoin d' étendre la couche fonctionnelle . Et cela est rendu possible grâce aux events Jelix . Par exemple, lors de l'inscription d'un membre, je souhaite vérifier que celui ci n'est pas banni du site. Pour cela jCommunity, génère un événement jcommunity_registration_prepare_save , envoyé juste avant l'enregistrement de l'e-mail du membre. En répondant à cet événement (via un listener) on est en mesure de procéder à cette vérification puis retourner au module jCommunty la réponse, positive ou non. ( petit rappel sur les Events Jelix dans un billet précédant ) 5) Conclusion : Ainsi, lors de l'appel à jCommunity, et grâce aux divers orverload, ce sont bien vos propres ressources qui sont utilisées, tout en exploitant pleinement le core/workflow de jCommunity.","tags":"Techno","url":"https://foxmask.net/post/2009/09/15/jelix-le-module-jcommunity-2/","loc":"https://foxmask.net/post/2009/09/15/jelix-le-module-jcommunity-2/"},{"title":"HaveFnuBB! et son installeur automatique aka wizard","text":"Dans un précédent billet vous m'aviez demandé de vous présenter l'installeur de mon forum HaveFnuBB! Voici donc le Système d'installation d'HaveFnuBB! : Le système d'installation se compose des étapes suivantes : ) Vérifications des pré-requis système ) Configuration du Forum ) Configuration de la Base de données ) Installation de la Base de données ) Création du compte Administrateur ) Fin INSTALLATION : Techniquement les étapes consistent en : Un seul contrôleur, 4 forms (*.forms.xml), 3 fichiers de config (*.ini.php) entre parenthèses, se trouve le nom de la méthode du contrôleur \"default\" du module \"hfnuinstall\" Etape 1 ( check ) - Vérifications : version_compare(phpversion(),'5.0','>=') function_exists('mysql_connect') Etape 2 ( config ) - Nom, description, langue et thème principaux, etc... lecture/écriture du fichier de config defaultconfig.ini.php lecture/écriture du fichier de config havefnu.ini.php Etape 3 ( dbconfig ) - Info sur les noms: du serveur, de la base, de l'utilisateur de la base et son mot de passe lecture/écriture du fichier de config dbprofils.ini.php Etape 4 ( installdb ) - exécution du script install.mysql.sql la structure de chaque module possède un dossier install/ , renfermant à son tour sql/install. driver .sql update/ version /install. driver .sql Etape 5 ( adminaccount ) - Infos admin : lecture/ecriture du fichier de config havefnu.ini.php appels des méthodes \"core\" de Jelix pour créer un utilisateur Etape 6 ( end ) - Message de fin ;) MISE A JOUR : La système de mise à jour, se charge : au minimum de mettre la version à jour dans havefnu.ini.php (le fichier de config principal du forum) sinon s'occupe de mettre à jour d'autres fichiers de configuration, et exécuter les scripts SQL. A chaque étape, l'installeur vérifie si la version actuelle est différente et enchaine les étapes sinon s'arrête. puis fini par vider le cache Bien évidement, on ne peut pas exécuter 2 fois de suite l'installation, puisque la vérification de la version est toujours faite. EPILOGUE : Si vous accédez à la racine de votre forum et qu'il n'est pas installé, Jelix routera l'utilisateur directement sur l'installeur. Ceci se fait grâce à l'intervention du plugin Coordinateur (qui pourra faire l'objet d'un autre article) vérifiant si le forum est installé. Pour plus d'infos je vous invite à consulter le code ici ou bien venir en parler sur le forum lui même ;) NOTA BENE : L'installeur n'a rien à voir avec la prochaine feature de Jelix, l'installeur touzazimut, de modules, plugins etc.. ;) Mais pour en avoir pas mal discuté en Janvier et codé appmgr, réadapter l'installeur d'HaveFnuBB avec Jelix 1.2 ne me posera aucun problème :P","tags":"Techno","url":"https://foxmask.net/post/2009/09/01/havefnubb-et-son-installeur-automatique-aka-wizard/","loc":"https://foxmask.net/post/2009/09/01/havefnubb-et-son-installeur-automatique-aka-wizard/"},{"title":"Jelix et la Communication inter modules","text":"Une pépite parmi tant d'autres que renferme Jelix, est la communication inter module. Mais qu'est-ce que cela ? Il arrive que des modules aient besoin de communiquer entre eux ou qu'ils aient besoin d'informations des uns et des autres. Imaginons un cas simple, une interface d'administration listant les modules (articles,wiki,news), présents sur son site favori. La solution \"Jelixienne\" consiste à faire communiquer le module d'administration avec tous les autres. Le module d'administration va émettre un message et récupérera les réponses des modules. Mise en place : je ferai apparaitre ces infos sur une pages dédiées \"Liste des modules\". Cette page sera constituée d'un template et les réponses des modules se feront à l'aide de zones ( rappel : les zones sont des portions de page) Donc pour cela je défini un contrôleur \"modules\" avec une action \"index\" par défaut : le contrôleur class modulesCtrl extends jController { function index() { $rep = $this->getResponse('html'); $tpl = new jTpl(); $tpl->assign('modules',jEvent::notify('HfnuAboutModule')->getResponse()); $rep->body->assign('MAIN',$tpl->fetch('modules')); return $rep; } } la ligne intéressante ici est : $tpl->assign('modules',jEvent::notify('HfnuAboutModule')->getResponse()); cette ligne fait 3 choses en même temps : elle émet un message nommé HfnuAboutModule elle récupère les données du message émis elle assigne ses données à la variable \"modules\" du template. la ligne suivante indique à Jelix, le nom du template \"module\", qui affichera les données $rep->body->assign('MAIN',$tpl->fetch('modules')); le template < h1 > Liste des modules </ h1 > {if count($modules)} {assign $count = count($modules)} {for $i=0; $i < $count;$i++} < div class = \"two-cols\" > < div class = \"col\" > {$modules[$i]} </ div > </ div > {/for} {/if} Bon ok on visualise un peu ce qui va se passer \"à la fin\" mais comment nos modules \"news\",\"wiki\",\"articles\" vont ils répondre à l'evenement HfnuAboutModule ? A tout jEvent::notify, un listener peut répondre, donc nous allons définir un listener comme suite en 2 temps : définition d'un fichier events.xml décrivant le nom de l'évènement et la classe y répondant, events.xml est donc le \"liant\" définition du listener lui-même. fichier events.xml <?xml version=\"1.0\" encoding=\"iso-8859-1\"?> <events xmlns= \"http://jelix.org/ns/events/1.0\" > <listener name= \"hfnuadmin\" > <event name= \"HfnuAboutModule\" /> </listener> </events> On retrouve bien ici le nom de l'évènement HfnuAboutModule auquel le listener hfnuadmin va se charger de répondre le listener class hfnuadminListener extends jEventListener{ function onHfnuAboutModule ($event) { $event->add( jZone::get('hfnuadmin~about',array('modulename'=>'hfnuadmin')) ); } } lorsque HfnuAboutModule est déclenché, alors onHfnuAboutModule entre en oeuvre et répond à l'event (via \\$event->add()) \\$event->add() peut recevoir tout type de données. Ici nous lui retournons une zone ( que nous avons précédemment abordés dans mes 2 précédants articles ) nommée \"about\" la zone class aboutZone extends jZone { protected $_tplname='zone.about'; protected function _prepareTpl(){ $moduleName = $this->param('modulename'); if ($moduleName == '') return; jClasses::inc('havefnubb~modulexml'); $moduleInfo = modulexml::parse($moduleName); $this->_tpl->assign('moduleInfo',$moduleInfo); } } notre zone ici récupère le paramètre du nom du module, puis parse le fichier module.xml et affecte le résultat au template zone.about le template < h1 > {$moduleInfo['name']} </ h1 > < dl > < dt > {@hfnuadmin~hfnuabout.about.version@} : </ dt >< dd > {$moduleInfo['version']} ({@hfnuadmin~hfnuabout.about.date.create@} {$moduleInfo['dateCreate']}) </ dd > < dt > {@hfnuadmin~hfnuabout.about.label@} : </ dt >< dd > {$moduleInfo['label']|escxml} </ dd > < dt > {@hfnuadmin~hfnuabout.about.desc@} : </ dt >< dd > {$moduleInfo['desc']} </ dd > < dt > {@hfnuadmin~hfnuabout.about.notes@} : </ dt >< dd > {$moduleInfo['notes']} </ dd > < dt > {@hfnuadmin~hfnuabout.about.licence@} : </ dt >< dd > {if $moduleInfo['licenceURL'] != ''} < a href = \"{$moduleInfo['licenceURL']}\" > {$moduleInfo['licence']} </ a > {else}{$moduleInfo['licence']}{/if} </ dd > < dt > {@hfnuadmin~hfnuabout.about.copyright@} : </ dt >< dd > {$moduleInfo['copyright']} </ dd > {foreach $moduleInfo['creators'] as $author} < dt > {@hfnuadmin~hfnuabout.about.authors@} : </ dt >< dd > {if $author['email'] != ''} < a href = \"mailto:{$author['email']}\" > {$author['name']|escxml}{else}{$author['name']|escxml}{/if} </ a ></ dd > {/foreach} < dt > {@hfnuadmin~hfnuabout.about.links@} </ dt >< dd >< a href = \"{$moduleInfo['homepageURL']}\" > {@hfnuadmin~hfnuabout.about.homepageURL@} </ a > - < a href = \"{$moduleInfo['updateURL']}\" > {@hfnuadmin~hfnuabout.about.updateURL@} </ a ></ dd > </ dl > Résultat : Liste des modules News! Version : stable 1.1.2 (du 2008-12-16) Libellé : Module de gestion de nouvelles Description : Ce module permet de gerer les nouvelles de son site web Notes : N/A License : GNU General Public Licence Copyright : 2008 FoxMaSk Auteurs : FoxMaSk Liens : Page d'accueil du module - Lien mise à jour Wiki Version : stable 1.0.2 (du 2009-01-25) Libellé : Wiki Description : Wiki maison pour la documentation du site web Notes : N/A License : GNU General Public Licence Copyright : 2008 FoxMaSk Auteurs : FoxMaSk Liens Page d'accueil du module - Lien mise à jour PS : ici je n'ai pas détaillé tous les events.xml des 3 modules ni les 3 listeners mais le code est le même ;) Conclusion : Voici donc la perle ; qui en quelques petites lignes ; a permis à tous les modules de se \"trouver\" et réunir des infos au même endroit. La même mécanique des jEvent::notify() permet par exemple d'enchainer des actions après l'inscription d'un membre (tels que lui envoyer un mail) jEvent::notify() permet également d'enrichir les fonctionnalités d'un module A via d'autres modules B,C,D sans avoir à modifier le module A, etc... en savoir plus sur la communication inter module","tags":"Techno","url":"https://foxmask.net/post/2009/08/18/jelix-et-la-communication-inter-modules-2/","loc":"https://foxmask.net/post/2009/08/18/jelix-et-la-communication-inter-modules-2/"},{"title":"Jelix est son nom","text":"j'évoquais ici ma découverte du framework PHP 5 Jelix. Depuis, 9mois se sont écoulés, mon implication autour du framework aura donc plu et me voici donc intronisé \" Member Core Team \" de Jelix, framework PHP ! Je ne reviendrai pas sur les qualités avérées du framework ( nan nan je ne suis pas parti pris ;) qui m'auront fait me lancer dans un projet d'envergure, le forum 100% en Jelix HaveFnuBB! . Ce projet a deux buts, pour moi : que les utilisateurs de forum trouvent une alternative aux mastodontes existants ;) avec une solution full PHP5 gage d'évolutivité que les développeurs PHP trouvent à leur tour une application des principes d'intégration de Jelix, car HaveFnuBB est un patchwork de modules Jelix indépendant les uns des autres, qui une fois réunis fournissent une application complète. Pour conclure : Merci à Jelix pour la confiance, je tâcherai de faire des beaux commits :D","tags":"Techno","url":"https://foxmask.net/post/2009/06/24/jelix-est-son-nom/","loc":"https://foxmask.net/post/2009/06/24/jelix-est-son-nom/"},{"title":"HaveFnuBB! fait son entrée sur FreshMeat","text":"HaveFnuBB! fait son entrée sur Freshmeat.net Le but est de faire connaître HaveFnuBB! aux bons endroits avec l'audience qui lui sied ;-)","tags":"Techno","url":"https://foxmask.net/post/2009/06/07/havefnubb-fait-son-entree-sur-freshmeat/","loc":"https://foxmask.net/post/2009/06/07/havefnubb-fait-son-entree-sur-freshmeat/"},{"title":"HaveFnuBB s'installe sur la toile","text":"Après avoir fait part du projet HaveFnuBB! ici même , voici que le projet s'installe sur la toile sur son propre domaine : HaveFnuBB.Org ! HaveFnuBB est le premier logiciel de forum produit à partir du framework PHP5 qu'est Jelix . Je vous invite à le tester , et si le projet vous intéresse, vous pouvez toujours participer en attendant une première version pour bientôt.","tags":"Techno","url":"https://foxmask.net/post/2009/04/13/havefnubb-s-installe-sur-la-toile/","loc":"https://foxmask.net/post/2009/04/13/havefnubb-s-installe-sur-la-toile/"},{"title":"Jelix et votre portail dans la langue de l'Utilisateur","text":"Je ne vais pas ici répéter la documentation sur le plugin coord \"autolocale\", plugin permettant de gérer automatiquement l'internationalisation de vos templates . Le propos de ce billet consiste à montrer comment avec Jelix, jAuth et jCommunity en particulier, on peut changer la langue de l'utilisateur et ainsi obtenir des pages dans pour sa langue. jCommunity donc, fourni des évènements, dont un permettant d'enregistrer toutes modifications sur le compte de l'utilisateur, cet evenement est jcommunity_save_account . Dans la class de mon listener authhavefnubb.listener.php j'aurai donc ce qui suit à chaque modification de mon compte : function onjcommunity_save_account ($event) { global $gJConfig; // recuperation des données saisies dans mon formulaire $form = $event->getParam('form'); if ( $form->getData('member_language') != '') { $_SESSION['JX_LANG'] = $form->getData('member_language'); $gJConfig->locale = $form->getData('member_language'); } // un petit message d'info signalant que le profil est mis à jour jMessage::add(jLocale::get('havefnubb~member.profile.updated'),'ok'); } l'astuce ici est de mettre $gJConfig->locale = $form->getData('member_language'); avant le jMessage::add pour que le message qui vient juste apres soit traduit immédiatement dans la langue choisie, sinon ce dernier apparaitrait \"encore\" dans la langue précédante ou celle du portail. ensuite on opérera de la même façon avec les évènement de jAuth que sont AuthLogin (lors de la connexion) et AuthLogout (lors de la déconnexion) function onAuthLogin ($event) { ... $_SESSION['JX_LANG'] = $user->member_language; $gJConfig->locale = $user->member_language; } function onAuthLogout ($event) { // suppression de la langue dans la session courante pour récupérer celle du portail $_SESSION['JX_LANG'] = ''; unset($_SESSION['JX_LANG']); } et l'on définira notre fichier events.xml comme suit : <listener name= \"authhavefnubb\" > <event name= \"AuthLogin\" /> <event name= \"AuthLogout\" /> <event name= \"jcommunity_save_account\" /> </listener> Ainsi donc avec une petite dizaine de lignes dans un listener et 3 noeuds XML on a permit à tout utilisateur de notre site d'avoir des pages dans sa langue favorite.","tags":"Techno","url":"https://foxmask.net/post/2009/03/08/jelix-et-votre-portail-dans-la-langue-de-lutilisateur/","loc":"https://foxmask.net/post/2009/03/08/jelix-et-votre-portail-dans-la-langue-de-lutilisateur/"},{"title":"HaveFnuBB, le forum \"made in\" Jelix","text":"Après avoir annoncé faire une pause sur les CMS , et démarré la découverte d'un framework nommé Jelix , ce dernier m'a amené à créer un petit blog perso (dé)pot de miel sur lequel je \"blog\" à la manière que fut la mienne sur CakePHP ici même ;) Avec bout de code, projets etc... Je me perds un peu vous me direz ;) Mais en fait non, c'est la trame qui m'amène à vous parler d'un projet écrit avec Jelix donc (forcément) nommé HaveFnuBB . Ce projet sans prétention, se veut être un simple forum (de plus, vu la palanquée qu'il existe sur la toile) mais qu'importe ! Le sien est toujours mieux que celui des autres ;) J'y apporte plusieurs choses et met en pratique les préceptes de la modularité, force, simplicité, vitesse de Jelix ! Ce petit joujou est déjà pourvu des fonctionnalités suivantes : Coté Forum Syntaxe Wiki (byebye le bbcode :P) Sous-Forum à foisons Tagger les Sujets Un bon moteur de recherche SEO Un fil d'ariane Coté Utilisateurs Moultes options pour gérer son profil peuvent faire parti de plusieurs groupes Coté Admin : Une configuration principale Gestion de Catégories de forums, de Forums, des droits au forums, notifications des sujets Cache Utilisateurs, Groupes, Ban (exclusions) Voilà donc le dernier passe-temps qui m'occupe quotidiennement ;) nb : Pour ceux qui seraient intéressés par un tel projet, il y a de la place, manifestez vous ! HaveFnu - !","tags":"Techno","url":"https://foxmask.net/post/2009/03/06/havefnubb-le-forum-made-in-jelix/","loc":"https://foxmask.net/post/2009/03/06/havefnubb-le-forum-made-in-jelix/"},{"title":"Jelix - Token le révélateur !","text":"Depuis la v 1.1RC1 de Jelix , l'apparition de la \"lutte\" contre les CSRF avec les tokens permettent non seulement de sécuriser vos formulaires ; tout à fait automatiquement et de façon transparente ; mais pas seulement ! Comment cela ? Supposons que nous éditions l'article 1 depuis l'url http://localhost/article/edit/1 le code de la methode \"edit\" serait le suivant : function edit { // recuperation de l'id passé sur l'url $id = (integer) $this->param('id'); // si le bouton validate n'est pas utilisé, nous inititions le formulaire if ($this->param('validate') == '') { $form = jForms::get('article~artdao',$id); } // le bouton validate a été pressé : else { // récuperation de l'instance du formulaire $form = jForms::fill('article~artdao'); $form->saveToDao('article~artdao',$id); } } Que ce passerait il avec ce code ? # L'acces à la page d'edition passerait impeccable. # La sauvegarde des données ne se passerait pas bien et nous aurions un message d'erreur : [exception 835] Le token du formulaire n'est pas valide, vous devez remplir le formulaire correctement à partir du site. ..lib/jelix/forms/jFormsBase.class.php 142 on aura beau vider le cache de son application rien n'y fera. Alors pourquoi ce message ? Simplement parceque lors de l'initialisation de l'instance \\$form (avec jForms::get() nous avons passé en paramètre l'id , mais qu'on ne l'a pas utiliser avec $form = jForms::fill('article~artdao'); donc, remplacez le code ci dessus, par ceci $form = jForms::fill('article~artdao',$id); et le message d'erreur sur le token ne se produira plus. Voici une façon détourner de vérifier que son formulaire est correctement géré avec la fonction anti CSRF ;-)","tags":"Techno","url":"https://foxmask.net/post/2008/12/14/jelix-token-le-revelateur/","loc":"https://foxmask.net/post/2008/12/14/jelix-token-le-revelateur/"},{"title":"Jelix - AppMgr - ou l'installeur de modules Jelix","text":"Voici un snapshot d'un module de gestion de ... modules Jelix Celui ci permet d'installer, activer, désactiver, supprimer, rechercher une mise à jour sur le site d'origine du module. Techniquement, Il s'occupe de parcourir l'arborescence des modules définis dans le fichier defaultconfig.ini.php dans la variable modulesPath et lit le fichier module.xml de chaque module. Si un tel module vous interesse faites vous connaître (un commentaire ?).","tags":"Techno","url":"https://foxmask.net/post/2008/11/03/jelix-appmgr-ou-linstalleur-de-modules-jelix/","loc":"https://foxmask.net/post/2008/11/03/jelix-appmgr-ou-linstalleur-de-modules-jelix/"},{"title":"J'ai l'X !","text":"Non le titre n'a rien à voir avec des films pour adultes ni avec l' école Polytechnique , découvrez ici ce qu'il en est réellement. Il y a presque un an de cela je faisais un comparatif des frameworks PHP(5) , et m'étais lancé avec CakePHP sur un projet libre à la clé que j'ai produit jusqu'à présent. Aujourd'hui, la bataille fait rage entre les frameworks par le biais de comparatifs de tout bord : PHP.net XML.net Comme je suis un éternel insatisfait et surtout toujours curieux de tout, je me suis tourné cette fois ci vers JELIX , un vrai Framework MVC en PHP5 . Rapide Prés' de la bête : Jelix est un framework open-source pour PHP5 qui permet de développer tout type d'application : * Performant : conçu pour les sites à forte charge. * Entièrement objet, fortement modulaire et extensible. * Basé sur des modèles de conception connus dont MVC, DAO.. * Prend en charge de nombreux formats de sortie : XHTML, XUL, RSS, ATOM, RDF, ZIP, XML, PDF, etc. * Facilite le développement des services web de type XML-RPC, JSON, et autres contenus pour Ajax. * Intègre un puissant système de formulaires, notamment pour faire des formulaires de type CRUD. * Inclus bien d'autres fonctionnalités et facilités... dès que j'aurai le temps je publierai quelques petits bouts de code de ci de là ;)","tags":"Techno","url":"https://foxmask.net/post/2008/09/10/j-ai-l-x/","loc":"https://foxmask.net/post/2008/09/10/j-ai-l-x/"}]};